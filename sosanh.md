# âš”ï¸ So SÃ¡nh Chi Tiáº¿t: DQN vs PPO trong Tá»‘i Æ¯u HÃ³a NÄƒng LÆ°á»£ng Microgrid

TÃ i liá»‡u nÃ y cung cáº¥p má»™t báº£n Ä‘Ã¡nh giÃ¡ sÃ¢u sáº¯c vá» hai thuáº­t toÃ¡n Reinforcement Learning Ä‘Æ°á»£c sá»­ dá»¥ng trong dá»± Ã¡n: **Deep Q-Network (DQN)** vÃ  **Proximal Policy Optimization (PPO)**.

---

## 1. Tá»•ng Quan Vá» Hai Thuáº­t ToÃ¡n

| Äáº·c Ä‘iá»ƒm | ğŸ”µ DQN (Deep Q-Network) | ğŸŸ£ PPO (Proximal Policy Optimization) |
| :--- | :--- | :--- |
| **Loáº¡i thuáº­t toÃ¡n** | **Value-based**: Há»c hÃ m giÃ¡ trá»‹ $Q(s, a)$ Ä‘á»ƒ Æ°á»›c lÆ°á»£ng pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y. | **Policy-based (Actor-Critic)**: Há»c trá»±c tiáº¿p chiáº¿n thuáº­t $\pi(a\|s)$ vÃ  hÃ m giÃ¡ trá»‹ $V(s)$. |
| **CÆ¡ cháº¿ há»c** | **Off-Policy**: CÃ³ thá»ƒ há»c tá»« dá»¯ liá»‡u cÅ© (Experience Replay). | **On-Policy**: Chá»‰ há»c tá»« dá»¯ liá»‡u má»›i nháº¥t do chÃ­nh nÃ³ táº¡o ra. |
| **KhÃ´ng gian hÃ nh Ä‘á»™ng** | **Rá»i ráº¡c (Discrete)**: Chá»‰ chá»n 1 trong $N$ hÃ nh Ä‘á»™ng (Vd: Báº­t/Táº¯t). | **Cáº£ hai**: Há»— trá»£ tá»‘t cáº£ Rá»i ráº¡c vÃ  LiÃªn tá»¥c (Continuous - Vd: Chá»‰nh van 50%). |
| **Äá»™ phá»©c táº¡p cÃ i Ä‘áº·t** | Trung bÃ¬nh (cáº§n Replay Buffer, Target Network). | Cao (cáº§n GAE, Clipping, 2 máº¡ng Actor-Critic). |
| **Sá»± á»•n Ä‘á»‹nh** | Tháº¥p hÆ¡n (dá»… bá»‹ dao Ä‘á»™ng, khÃ³ há»™i tá»¥ náº¿u hyperparams sai). | Ráº¥t cao (nhá» cÆ¡ cháº¿ Clipping giá»›i háº¡n update). |

---

## 2. PhÃ¢n TÃ­ch CÆ¡ Cháº¿ Hoáº¡t Äá»™ng (Deep Dive)

### ğŸ”µ DQN: Há»c Qua KÃ½ á»¨c (Flashcards Analogy)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

1. **Replay Buffer (Bá»™ nhá»› há»“i tÆ°á»Ÿng):** DQN lÆ°u trá»¯ má»i tráº£i nghiá»‡m $(S, A, R, S')$ vÃ o bá»™ nhá»›. Khi há»c, nÃ³ bá»‘c ngáº«u nhiÃªn má»™t lÃ´ dá»¯ liá»‡u (batch) Ä‘á»ƒ training.
    * *Æ¯u Ä‘iá»ƒm:* **Sample Efficiency** cao. Má»™t tráº£i nghiá»‡m cÃ³ thá»ƒ Ä‘Æ°á»£c há»c Ä‘i há»c láº¡i nhiá»u láº§n. PhÃ¡ vá»¡ sá»± tÆ°Æ¡ng quan thá»i gian giá»¯a cÃ¡c máº«u.
2. **Target Network (Máº¡ng má»¥c tiÃªu):** Äá»ƒ trÃ¡nh viá»‡c "vá»«a há»c vá»«a sá»­a Ä‘Ã¡p Ã¡n", DQN dÃ¹ng má»™t máº¡ng riÃªng (Target) Ä‘á»ƒ tÃ­nh nhÃ£n $y$, máº¡ng nÃ y chá»‰ Ä‘Æ°á»£c cáº­p nháº­t sau má»—i vÃ i nghÃ¬n bÆ°á»›c.

> **GÃ³c nhÃ¬n Non-IT:**  
> DQN giá»‘ng nhÆ° **há»c Ã´n thi báº±ng Flashcards**. Báº¡n trá»™n láº«n cÃ¡c cÃ¢u há»i tá»« quÃ¡ khá»© Ä‘á»ƒ há»c (Replay), vÃ  báº¡n giá»¯ nguyÃªn Ä‘Ã¡p Ã¡n trong má»™t khoáº£ng thá»i gian Ä‘á»ƒ khÃ´ng bá»‹ rá»‘i (Target Network).

### ğŸŸ£ PPO: Há»c Vá»›i Huáº¥n Luyá»‡n ViÃªn (Coach Analogy)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

1. **Actor-Critic:** Máº¡ng Actor quyáº¿t Ä‘á»‹nh hÃ nh Ä‘á»™ng, máº¡ng Critic Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng Ä‘Ã³ tá»‘t hay xáº¥u ($V(s)$).
2. **Clipped Surrogate Objective:** PPO giá»›i háº¡n má»©c Ä‘á»™ thay Ä‘á»•i cá»§a policy trong má»—i bÆ°á»›c update (thÆ°á»ng lÃ  $\epsilon = 0.2$, tá»©c khÃ´ng Ä‘á»•i quÃ¡ 20%).
    * *Æ¯u Ä‘iá»ƒm:* **Äá»™ á»•n Ä‘á»‹nh cá»±c cao**. TrÃ¡nh viá»‡c policy bá»‹ "sáº­p" (collapse) do update quÃ¡ Ä‘Ã , Ä‘iá»u thÆ°á»ng tháº¥y á»Ÿ cÃ¡c thuáº­t toÃ¡n Policy Gradient cÅ©.

> **GÃ³c nhÃ¬n Non-IT:**  
> PPO giá»‘ng nhÆ° **Huáº¥n luyá»‡n viÃªn thá»ƒ thao**. HLV chá»‰ chá»‰nh sá»­a tÆ° tháº¿ cá»§a báº¡n tá»«ng chÃºt má»™t ("Tháº¥p tay xuá»‘ng má»™t chÃºt"), khÃ´ng báº¯t báº¡n Ä‘á»•i hoÃ n toÃ n cÃ¡ch chÆ¡i ngay láº­p tá»©c Ä‘á»ƒ trÃ¡nh cháº¥n thÆ°Æ¡ng hoáº·c máº¥t phong Ä‘á»™.

---

## 3. Hiá»‡u Suáº¥t TrÃªn BÃ i ToÃ¡n Microgrid

Dá»±a trÃªn thá»±c nghiá»‡m, dÆ°á»›i Ä‘Ã¢y lÃ  so sÃ¡nh hiá»‡u quáº£ cá»§a hai thuáº­t toÃ¡n Ä‘á»‘i vá»›i bÃ i toÃ¡n nÄƒng lÆ°á»£ng:

### ğŸš€ Tá»‘c Ä‘á»™ há»™i tá»¥ (Convergence Speed)

* **DQN:** ThÆ°á»ng há»™i tá»¥ nhanh hÆ¡n á»Ÿ giai Ä‘oáº¡n Ä‘áº§u nhá» tÃ¡i sá»­ dá»¥ng dá»¯ liá»‡u (Replay Buffer). Tuy nhiÃªn, Ä‘Æ°á»ng cong loss cÃ³ thá»ƒ dao Ä‘á»™ng máº¡nh.
* **PPO:** Há»™i tá»¥ cháº­m hÆ¡n vÃ  mÆ°á»£t mÃ  hÆ¡n (monotonic improvement). Cáº§n nhiá»u dá»¯ liá»‡u (mÃ´i trÆ°á»ng tÆ°Æ¡ng tÃ¡c) hÆ¡n Ä‘á»ƒ Ä‘áº¡t cÃ¹ng má»©c hiá»‡u suáº¥t.

### ğŸ¯ Cháº¥t lÆ°á»£ng chÃ­nh sÃ¡ch (Policy Quality)

* **DQN:** CÃ³ xu hÆ°á»›ng tÃ¬m ra chiáº¿n thuáº­t "cá»±c Ä‘oan" (Bang-bang control) do báº£n cháº¥t `argmax` cá»§a Q-learning (Vd: Xáº£ háº¿t pin hoáº·c Sáº¡c Ä‘áº§y pin).
* **PPO:** CÃ³ thá»ƒ há»c Ä‘Æ°á»£c chiáº¿n thuáº­t má»m dáº»o hÆ¡n (Stochastic policy), Ä‘áº·c biá»‡t náº¿u chuyá»ƒn sang action liÃªn tá»¥c (Vd: Xáº£ 40% pin).

### ğŸ› ï¸ Äá»™ nháº¡y vá»›i SiÃªu tham sá»‘ (Hyperparameters Sensitivity)

* **DQN:** Ráº¥t nháº¡y cáº£m. Cáº§n tinh chá»‰nh ká»¹ `learning_rate`, `epsilon_decay`, `buffer_size`, `target_update_freq`. Náº¿u sai, máº¡ng cÃ³ thá»ƒ khÃ´ng há»™i tá»¥ (Q-value phÃ¢n ká»³).
* **PPO:** KhÃ¡ "trÃ¢u bÃ²" (Robust). CÃ¡c tham sá»‘ máº·c Ä‘á»‹nh (clip=0.2, gamma=0.99) thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t trÃªn nhiá»u bÃ i toÃ¡n khÃ¡c nhau mÃ  khÃ´ng cáº§n chá»‰nh sá»­a nhiá»u.

---

## 4. Káº¿t Luáº­n & Khuyáº¿n Nghá»‹

### Khi nÃ o nÃªn chá»n DQN?

1. **NgÆ°á»i má»›i báº¯t Ä‘áº§u:** DQN dá»… hiá»ƒu, dá»… debug hÆ¡n.
2. **HÃ nh Ä‘á»™ng rá»i ráº¡c:** BÃ i toÃ¡n chá»‰ cáº§n Báº­t/Táº¯t thiáº¿t bá»‹.
3. **TÃ i nguyÃªn tÃ­nh toÃ¡n tháº¥p:** DQN thÆ°á»ng nháº¹ hÆ¡n PPO má»™t chÃºt.
4. **Muá»‘n tiáº¿t kiá»‡m máº«u (Sample efficient):** Khi viá»‡c tÆ°Æ¡ng tÃ¡c vá»›i mÃ´i trÆ°á»ng tá»‘n kÃ©m thá»i gian.

### Khi nÃ o nÃªn chá»n PPO?

1. **Cáº§n sá»± á»•n Ä‘á»‹nh cao:** KhÃ´ng muá»‘n Ä‘au Ä‘áº§u chá»‰nh hyperparams.
2. **HÃ nh Ä‘á»™ng liÃªn tá»¥c:** Cáº§n Ä‘iá»u khiá»ƒn cÃ´ng suáº¥t má»‹n (Vd: Äiá»u khiá»ƒn dÃ²ng sáº¡c chÃ­nh xÃ¡c tá»«ng Ampe).
3. **Muá»‘n káº¿t quáº£ SOTA (State-of-the-Art):** PPO hiá»‡n lÃ  chuáº©n má»±c cho nhiá»u bÃ i toÃ¡n phá»©c táº¡p (bao gá»“m cáº£ ChatGPT).
4. **MÃ´i trÆ°á»ng ngáº«u nhiÃªn (Stochastic):** PPO xá»­ lÃ½ nhiá»…u tá»‘t hÆ¡n DQN.

### ğŸ† Lá»±a chá»n cho Äá»“ Ã¡n Microgrid

Vá»›i bÃ i toÃ¡n nÃ y, **DQN lÃ  lá»±a chá»n khá»Ÿi Ä‘áº§u tá»‘t nháº¥t** vÃ¬ tÃ­nh trá»±c quan vÃ  phÃ¹ há»£p vá»›i action space rá»i ráº¡c (5 hÃ nh Ä‘á»™ng). Tuy nhiÃªn, **PPO lÃ  bÆ°á»›c nÃ¢ng cao Ä‘Ã¡ng giÃ¡** Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ á»•n Ä‘á»‹nh vÃ  Ä‘iá»ƒm sá»‘ trong pháº§n bÃ¡o cÃ¡o.
