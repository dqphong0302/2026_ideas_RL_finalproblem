{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ”‹ Tá»‘i Æ¯u HÃ³a NÄƒng LÆ°á»£ng Microgrid - PPO (Policy Gradient)\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ“Œ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng (CHá»ˆ Cáº¦N 3 BÆ¯á»šC)\n",
                "\n",
                "| BÆ°á»›c | HÃ nh Äá»™ng | Thá»i Gian |\n",
                "|------|-----------|------------|\n",
                "| **1** | Cháº¡y Ã´ \"ğŸ“¦ CÃ i Äáº·t\" | ~10 giÃ¢y |\n",
                "| **2** | Cháº¡y Ã´ \"ğŸš€ Huáº¥n Luyá»‡n\" | ~60 giÃ¢y |\n",
                "| **3** | Cháº¡y Ã´ \"ğŸ“Š Káº¿t Quáº£\" | ~5 giÃ¢y |\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ¯ BÃ i ToÃ¡n\n",
                "\n",
                "**Má»¥c tiÃªu:** Dáº¡y AI quáº£n lÃ½ nÄƒng lÆ°á»£ng thÃ´ng minh cho há»‡ thá»‘ng microgrid\n",
                "\n",
                "```\n",
                "â˜€ï¸ NÄƒng lÆ°á»£ng máº·t trá»i  â”€â”\n",
                "ğŸŒ¬ï¸ NÄƒng lÆ°á»£ng giÃ³      â”€â”¼â”€â–º ğŸ¤– PPO Agent â”€â–º âš¡ Cung cáº¥p Ä‘iá»‡n cho há»™ gia Ä‘Ã¬nh\n",
                "ğŸ”‹ Pin lÆ°u trá»¯         â”€â”¤\n",
                "âš¡ LÆ°á»›i Ä‘iá»‡n           â”€â”˜\n",
                "```\n",
                "\n",
                "## ğŸ§  PPO vs DQN - KhÃ¡c biá»‡t chÃ­nh\n",
                "\n",
                "| Äáº·c Ä‘iá»ƒm | DQN | PPO |\n",
                "|-----------|-----|-----|\n",
                "| Loáº¡i | Value-based | Policy-based |\n",
                "| Máº¡ng | Q-Network | Actor-Critic |\n",
                "| Output | Q-values â†’ chá»n action cÃ³ Q cao nháº¥t | XÃ¡c suáº¥t trá»±c tiáº¿p Ï€(a|s) |\n",
                "| Training | Off-policy (replay buffer) | On-policy (rollout buffer) |\n",
                "| Æ¯u Ä‘iá»ƒm | ÄÆ¡n giáº£n, á»•n Ä‘á»‹nh | Linh hoáº¡t, policy smooth |\n",
                "\n",
                "**PPO = Proximal Policy Optimization (Schulman et al., 2017)**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“¦ BÆ¯á»šC 1: CÃ i Äáº·t (Cháº¡y 1 láº§n)\n",
                "\n",
                "Ã” nÃ y cÃ i Ä‘áº·t thÆ° viá»‡n vÃ  táº¡o mÃ´i trÆ°á»ng microgrid + PPO Agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸ“¦ BÆ¯á»šC 1: CÃ€I Äáº¶T - Cháº¡y Ã´ nÃ y trÆ°á»›c\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Œ CÃ€I Äáº¶T THÆ¯ VIá»†N\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "!pip install gymnasium torch numpy matplotlib -q\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.distributions import Categorical\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"ğŸ–¥ï¸ Thiáº¿t bá»‹: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ”§ Cáº¤U HÃŒNH - Thay Ä‘á»•i cÃ¡c giÃ¡ trá»‹ nÃ y cho bÃ i lÃ m riÃªng!\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "SEED = 42              # ğŸ”§ Thay Ä‘á»•i: 42, 123, 456, 789, 999\n",
                "EPISODES = 200         # ğŸ”§ Thay Ä‘á»•i: 100-500\n",
                "LR_ACTOR = 3e-4        # ğŸ”§ Thay Ä‘á»•i: 1e-4 - 1e-3\n",
                "LR_CRITIC = 1e-3       # ğŸ”§ Thay Ä‘á»•i: 5e-4 - 3e-3\n",
                "CLIP_EPSILON = 0.2     # ğŸ”§ PPO clip range: 0.1-0.3\n",
                "PPO_EPOCHS = 10        # ğŸ”§ Update epochs: 5-15\n",
                "GAE_LAMBDA = 0.95      # ğŸ”§ GAE lambda: 0.9-0.99\n",
                "GAMMA = 0.99           # ğŸ”§ Discount factor: 0.95-0.99\n",
                "\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "random.seed(SEED)\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸŒ MÃ”I TRÆ¯á»œNG MICROGRID (giá»‘ng báº£n DQN Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class MicrogridEnv:\n",
                "    \"\"\"MÃ´i trÆ°á»ng microgrid: Solar + Wind + Battery + Grid\"\"\"\n",
                "    def __init__(self):\n",
                "        self.battery_capacity = 100.0\n",
                "        self.battery_level = 50.0\n",
                "        self.current_hour = 0\n",
                "\n",
                "    def reset(self):\n",
                "        self.battery_level = 50.0\n",
                "        self.current_hour = 0\n",
                "        return self._get_state()\n",
                "\n",
                "    def _get_state(self):\n",
                "        hour = self.current_hour % 24\n",
                "        solar = max(0, np.sin((hour - 6) * np.pi / 12)) * 50 * (0.8 + 0.4 * np.random.random())\n",
                "        wind = 30 * np.random.random()\n",
                "        demand = 40 + 20 * np.sin((hour - 6) * np.pi / 12) + 10 * np.random.random()\n",
                "        price = 0.15 + 0.1 * (1 if 17 <= hour <= 21 else 0)\n",
                "        return np.array([\n",
                "            self.battery_level / self.battery_capacity,\n",
                "            demand / 100, solar / 50, wind / 30, price / 0.25,\n",
                "            np.sin(2 * np.pi * hour / 24),\n",
                "            np.cos(2 * np.pi * hour / 24), 0.5\n",
                "        ], dtype=np.float32)\n",
                "\n",
                "    def step(self, action):\n",
                "        state = self._get_state()\n",
                "        demand = state[1] * 100\n",
                "        solar = state[2] * 50\n",
                "        wind = state[3] * 30\n",
                "        price = state[4] * 0.25\n",
                "        renewable = solar + wind\n",
                "        grid_used = renewable_used = battery_change = 0\n",
                "\n",
                "        if action == 0:  # Xáº£ pin\n",
                "            discharge = min(20, self.battery_level, demand)\n",
                "            battery_change = -discharge\n",
                "            renewable_used = min(renewable, demand - discharge)\n",
                "            grid_used = max(0, demand - discharge - renewable_used)\n",
                "        elif action == 1:  # Sáº¡c tá»« renewable\n",
                "            renewable_used = min(renewable, demand)\n",
                "            grid_used = max(0, demand - renewable_used)\n",
                "            excess = renewable - renewable_used\n",
                "            battery_change = min(20, self.battery_capacity - self.battery_level, excess)\n",
                "        elif action == 2:  # Mua tá»« lÆ°á»›i\n",
                "            grid_used = demand\n",
                "        elif action == 3:  # Renewable + xáº£ pin\n",
                "            renewable_used = min(renewable, demand)\n",
                "            remaining = demand - renewable_used\n",
                "            discharge = min(20, self.battery_level, remaining)\n",
                "            battery_change = -discharge\n",
                "            grid_used = max(0, remaining - discharge)\n",
                "        else:  # Renewable + lÆ°á»›i\n",
                "            renewable_used = min(renewable, demand)\n",
                "            grid_used = max(0, demand - renewable_used)\n",
                "\n",
                "        self.battery_level = np.clip(self.battery_level + battery_change, 0, self.battery_capacity)\n",
                "        reward = renewable_used / 40 - 2 * grid_used / 40 * price - 0.1 * abs(battery_change) / 20\n",
                "        if grid_used == 0 and 17 <= self.current_hour % 24 <= 21:\n",
                "            reward += 0.5\n",
                "        self.current_hour += 1\n",
                "        done = self.current_hour >= 24\n",
                "        return self._get_state(), reward, done, {\n",
                "            'renewable_used': renewable_used, 'grid_used': grid_used, 'cost': grid_used * price\n",
                "        }\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ§  ACTOR-CRITIC NETWORK (khÃ¡c DQN: cÃ³ 2 heads)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class ActorCritic(nn.Module):\n",
                "    \"\"\"\n",
                "    Actor-Critic Network:\n",
                "    - Actor: Output xÃ¡c suáº¥t chá»n má»—i action â†’ Ï€(a|s)\n",
                "    - Critic: Output giÃ¡ trá»‹ tráº¡ng thÃ¡i â†’ V(s)\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        # Shared layers\n",
                "        self.shared = nn.Sequential(\n",
                "            nn.Linear(8, 128), nn.Tanh(),   # ğŸ”§ Thay Ä‘á»•i: 64, 128, 256\n",
                "            nn.Linear(128, 128), nn.Tanh(),\n",
                "        )\n",
                "        # Actor head â†’ probabilities\n",
                "        self.actor = nn.Sequential(nn.Linear(128, 5), nn.Softmax(dim=-1))\n",
                "        # Critic head â†’ state value\n",
                "        self.critic = nn.Linear(128, 1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        features = self.shared(x)\n",
                "        return self.actor(features), self.critic(features)\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ¤– PPO AGENT\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class PPOAgent:\n",
                "    \"\"\"\n",
                "    PPO Agent - Proximal Policy Optimization\n",
                "    KhÃ¡c DQN:\n",
                "    - KhÃ´ng dÃ¹ng replay buffer â†’ dÃ¹ng rollout buffer (on-policy)\n",
                "    - KhÃ´ng dÃ¹ng epsilon-greedy â†’ sample tá»« policy distribution\n",
                "    - Clipped objective â†’ ngÄƒn policy thay Ä‘á»•i quÃ¡ lá»›n\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        self.network = ActorCritic().to(device)\n",
                "        self.optimizer = optim.Adam([\n",
                "            {'params': self.network.shared.parameters(), 'lr': LR_ACTOR},\n",
                "            {'params': self.network.actor.parameters(), 'lr': LR_ACTOR},\n",
                "            {'params': self.network.critic.parameters(), 'lr': LR_CRITIC},\n",
                "        ])\n",
                "        # Rollout storage\n",
                "        self.states = []\n",
                "        self.actions = []\n",
                "        self.log_probs = []\n",
                "        self.rewards = []\n",
                "        self.values = []\n",
                "        self.dones = []\n",
                "\n",
                "    def select_action(self, state, eval_mode=False):\n",
                "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        with torch.no_grad():\n",
                "            probs, value = self.network(state_t)\n",
                "        if eval_mode:\n",
                "            return probs.argmax().item()\n",
                "        dist = Categorical(probs)\n",
                "        action = dist.sample()\n",
                "        # Store for PPO update\n",
                "        self.states.append(state)\n",
                "        self.actions.append(action.item())\n",
                "        self.log_probs.append(dist.log_prob(action).item())\n",
                "        self.values.append(value.item())\n",
                "        return action.item()\n",
                "\n",
                "    def store_reward(self, reward, done):\n",
                "        self.rewards.append(reward)\n",
                "        self.dones.append(float(done))\n",
                "\n",
                "    def update(self):\n",
                "        \"\"\"PPO Update: GAE â†’ Clipped Surrogate â†’ Multiple epochs\"\"\"\n",
                "        if len(self.states) == 0:\n",
                "            return\n",
                "\n",
                "        # 1. Compute GAE advantages\n",
                "        advantages = []\n",
                "        gae = 0\n",
                "        values = self.values + [0]  # Bootstrap = 0 for terminal\n",
                "        for t in reversed(range(len(self.rewards))):\n",
                "            delta = self.rewards[t] + GAMMA * values[t+1] * (1 - self.dones[t]) - values[t]\n",
                "            gae = delta + GAMMA * GAE_LAMBDA * (1 - self.dones[t]) * gae\n",
                "            advantages.insert(0, gae)\n",
                "\n",
                "        advantages = torch.FloatTensor(advantages).to(device)\n",
                "        returns = advantages + torch.FloatTensor(self.values).to(device)\n",
                "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
                "\n",
                "        # Prepare tensors\n",
                "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
                "        actions = torch.LongTensor(self.actions).to(device)\n",
                "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
                "\n",
                "        # 2. PPO epochs\n",
                "        for _ in range(PPO_EPOCHS):\n",
                "            probs, values_pred = self.network(states)\n",
                "            dist = Categorical(probs)\n",
                "            new_log_probs = dist.log_prob(actions)\n",
                "            entropy = dist.entropy()\n",
                "\n",
                "            # Clipped surrogate objective\n",
                "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
                "            surr1 = ratio * advantages\n",
                "            surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * advantages\n",
                "            policy_loss = -torch.min(surr1, surr2).mean()\n",
                "\n",
                "            value_loss = nn.MSELoss()(values_pred.squeeze(), returns)\n",
                "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy.mean()\n",
                "\n",
                "            self.optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
                "            self.optimizer.step()\n",
                "\n",
                "        # Clear rollout buffer\n",
                "        self.states = []\n",
                "        self.actions = []\n",
                "        self.log_probs = []\n",
                "        self.rewards = []\n",
                "        self.values = []\n",
                "        self.dones = []\n",
                "\n",
                "# Khá»Ÿi táº¡o\n",
                "env = MicrogridEnv()\n",
                "agent = PPOAgent()\n",
                "print(\"\\nâœ… CÃ i Ä‘áº·t hoÃ n táº¥t! Chuyá»ƒn sang BÆ¯á»šC 2.\")\n",
                "print(f\"   ğŸ§  Thuáº­t toÃ¡n: PPO (Proximal Policy Optimization)\")\n",
                "print(f\"   ğŸ“Š Clip Îµ: {CLIP_EPSILON}, PPO Epochs: {PPO_EPOCHS}\")\n",
                "print(f\"   ğŸ“ˆ LR Actor: {LR_ACTOR}, LR Critic: {LR_CRITIC}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸš€ BÆ¯á»šC 2: Huáº¥n Luyá»‡n AI (PPO)\n",
                "\n",
                "Ã” nÃ y huáº¥n luyá»‡n PPO Agent.\n",
                "\n",
                "**KhÃ¡c biá»‡t training PPO vs DQN:**\n",
                "- DQN: Má»—i step â†’ lÆ°u replay buffer â†’ sample batch â†’ update\n",
                "- PPO: Thu tháº­p rollout (nhiá»u episodes) â†’ tÃ­nh GAE advantage â†’ update nhiá»u epochs â†’ clear buffer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸš€ BÆ¯á»šC 2: HUáº¤N LUYá»†N PPO AGENT\n",
                "\n",
                "print(\"â•\" * 60)\n",
                "print(\"ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N PPO AGENT\")\n",
                "print(\"â•\" * 60)\n",
                "print(f\"ğŸ“Š Sá»‘ episodes: {EPISODES}\")\n",
                "print(f\"ğŸ² Random seed: {SEED}\")\n",
                "print(f\"ğŸ“ˆ Clip Îµ: {CLIP_EPSILON}, GAE Î»: {GAE_LAMBDA}\")\n",
                "print(\"â•\" * 60)\n",
                "\n",
                "rewards_history = []\n",
                "renewable_history = []\n",
                "UPDATE_EVERY = 4  # Update sau má»—i 4 episodes\n",
                "\n",
                "for episode in range(1, EPISODES + 1):\n",
                "    state = env.reset()\n",
                "    total_reward = 0\n",
                "    total_renewable = 0\n",
                "    total_grid = 0\n",
                "\n",
                "    for step in range(24):  # 24 giá»/ngÃ y\n",
                "        action = agent.select_action(state)\n",
                "        next_state, reward, done, info = env.step(action)\n",
                "        agent.store_reward(reward, done)\n",
                "\n",
                "        state = next_state\n",
                "        total_reward += reward\n",
                "        total_renewable += info['renewable_used']\n",
                "        total_grid += info['grid_used']\n",
                "\n",
                "        if done:\n",
                "            break\n",
                "\n",
                "    # PPO update sau má»—i UPDATE_EVERY episodes\n",
                "    if episode % UPDATE_EVERY == 0:\n",
                "        agent.update()\n",
                "\n",
                "    # LÆ°u history\n",
                "    rewards_history.append(total_reward)\n",
                "    renewable_ratio = total_renewable / (total_renewable + total_grid + 1e-6) * 100\n",
                "    renewable_history.append(renewable_ratio)\n",
                "\n",
                "    if episode % 10 == 0:\n",
                "        avg_reward = np.mean(rewards_history[-10:])\n",
                "        print(f\"Episode {episode:3d} | Reward: {avg_reward:+7.2f} | Renewable: {renewable_ratio:.1f}%\")\n",
                "\n",
                "print(\"\\n\" + \"â•\" * 60)\n",
                "print(\"âœ… HUáº¤N LUYá»†N HOÃ€N Táº¤T!\")\n",
                "print(f\"ğŸ“ˆ Best Reward: {max(rewards_history):.2f}\")\n",
                "print(f\"ğŸ“ˆ Final Avg Reward: {np.mean(rewards_history[-10:]):.2f}\")\n",
                "print(\"â•\" * 60)\n",
                "print(\"\\nğŸ‘‰ Chuyá»ƒn sang BÆ¯á»šC 3 Ä‘á»ƒ xem káº¿t quáº£!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“Š BÆ¯á»šC 3: Xem Káº¿t Quáº£\n",
                "\n",
                "Ã” nÃ y hiá»ƒn thá»‹:\n",
                "- ğŸ“ˆ Äá»“ thá»‹ quÃ¡ trÃ¬nh há»c PPO\n",
                "- ğŸ“Š So sÃ¡nh PPO Agent vs Random\n",
                "- ğŸ“‹ Báº£ng sá»‘ liá»‡u káº¿t quáº£"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸ“Š BÆ¯á»šC 3: XEM Káº¾T QUáº¢ - Cháº¡y Ã´ nÃ y Ä‘á»ƒ xem Ä‘á»“ thá»‹\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Š Váº¼ Äá»’ THá»Š TRAINING\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Äá»“ thá»‹ 1: Reward\n",
                "axes[0].plot(rewards_history, alpha=0.3, color='purple')\n",
                "window = min(10, len(rewards_history))\n",
                "if len(rewards_history) >= window:\n",
                "    smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
                "    axes[0].plot(range(window-1, len(rewards_history)), smoothed, color='purple', linewidth=2)\n",
                "axes[0].set_title('ğŸ“ˆ Reward Theo Episode (PPO)', fontsize=12)\n",
                "axes[0].set_xlabel('Episode')\n",
                "axes[0].set_ylabel('Total Reward')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Äá»“ thá»‹ 2: Renewable usage\n",
                "axes[1].plot(renewable_history, color='green', alpha=0.3)\n",
                "if len(renewable_history) >= window:\n",
                "    smoothed = np.convolve(renewable_history, np.ones(window)/window, mode='valid')\n",
                "    axes[1].plot(range(window-1, len(renewable_history)), smoothed, color='green', linewidth=2)\n",
                "axes[1].set_title('â˜€ï¸ Tá»· Lá»‡ NÄƒng LÆ°á»£ng TÃ¡i Táº¡o', fontsize=12)\n",
                "axes[1].set_xlabel('Episode')\n",
                "axes[1].set_ylabel('Renewable %')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('ppo_training_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Š SO SÃNH Vá»šI RANDOM BASELINE\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"\\n\" + \"â•\" * 60)\n",
                "print(\"ğŸ“Š ÄÃNH GIÃ: PPO AGENT vs RANDOM BASELINE\")\n",
                "print(\"â•\" * 60)\n",
                "\n",
                "def evaluate(agent, use_random=False, num_episodes=10):\n",
                "    total_rewards, total_costs, total_renewable = [], [], []\n",
                "    for _ in range(num_episodes):\n",
                "        state = env.reset()\n",
                "        ep_reward = ep_cost = ep_renewable = ep_total = 0\n",
                "        for step in range(24):\n",
                "            if use_random:\n",
                "                action = random.randint(0, 4)\n",
                "            else:\n",
                "                action = agent.select_action(state, eval_mode=True)\n",
                "            next_state, reward, done, info = env.step(action)\n",
                "            ep_reward += reward\n",
                "            ep_cost += info['cost']\n",
                "            ep_renewable += info['renewable_used']\n",
                "            ep_total += info['renewable_used'] + info['grid_used']\n",
                "            state = next_state\n",
                "            if done: break\n",
                "        total_rewards.append(ep_reward)\n",
                "        total_costs.append(ep_cost)\n",
                "        total_renewable.append(ep_renewable / ep_total * 100 if ep_total > 0 else 0)\n",
                "    return {'reward': np.mean(total_rewards), 'cost': np.mean(total_costs), 'renewable': np.mean(total_renewable)}\n",
                "\n",
                "trained_results = evaluate(agent, use_random=False)\n",
                "random_results = evaluate(agent, use_random=True)\n",
                "\n",
                "reward_imp = ((trained_results['reward'] - random_results['reward']) / abs(random_results['reward'])) * 100 if random_results['reward'] != 0 else 0\n",
                "cost_imp = ((random_results['cost'] - trained_results['cost']) / random_results['cost']) * 100 if random_results['cost'] != 0 else 0\n",
                "\n",
                "print(f\"\\n{'Metric':<25} {'PPO Agent':>15} {'Random':>15} {'Improvement':>15}\")\n",
                "print(\"-\" * 70)\n",
                "print(f\"{'Mean Episode Reward':<25} {trained_results['reward']:>+15.2f} {random_results['reward']:>+15.2f} {reward_imp:>+14.1f}%\")\n",
                "print(f\"{'Daily Cost ($)':<25} {trained_results['cost']:>15.2f} {random_results['cost']:>15.2f} {cost_imp:>+14.1f}%\")\n",
                "print(f\"{'Renewable Usage (%)':<25} {trained_results['renewable']:>14.1f}% {random_results['renewable']:>14.1f}%\")\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Š Váº¼ BIá»‚U Äá»’ SO SÃNH\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
                "metrics = ['Reward', 'Cost ($)', 'Renewable %']\n",
                "trained_vals = [trained_results['reward'], trained_results['cost'], trained_results['renewable']]\n",
                "random_vals = [random_results['reward'], random_results['cost'], random_results['renewable']]\n",
                "colors = ['#9b59b6', '#e74c3c']  # Purple for PPO\n",
                "\n",
                "for i, (metric, t_val, r_val) in enumerate(zip(metrics, trained_vals, random_vals)):\n",
                "    bars = axes[i].bar(['PPO', 'Random'], [t_val, r_val], color=colors)\n",
                "    axes[i].set_title(metric, fontsize=12, fontweight='bold')\n",
                "    axes[i].grid(True, alpha=0.3)\n",
                "    for bar, val in zip(bars, [t_val, r_val]):\n",
                "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
                "                     f'{val:.1f}', ha='center', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('ppo_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\" + \"â•\" * 60)\n",
                "print(\"âœ… Káº¾T QUáº¢ ÄÃƒ ÄÆ¯á»¢C LÆ¯U!\")\n",
                "print(\"   ğŸ“ ppo_training_curves.png\")\n",
                "print(\"   ğŸ“ ppo_comparison.png\")\n",
                "print(\"â•\" * 60)\n",
                "\n",
                "print(\"\\nğŸ“ Káº¾T LUáº¬N:\")\n",
                "print(f\"   âœ… PPO Agent Ä‘Ã£ há»c Ä‘Æ°á»£c chÃ­nh sÃ¡ch quáº£n lÃ½ nÄƒng lÆ°á»£ng tá»‘i Æ°u\")\n",
                "print(f\"   âœ… Cáº£i thiá»‡n reward: {reward_imp:+.1f}% so vá»›i random\")\n",
                "print(f\"   âœ… Giáº£m chi phÃ­ Ä‘iá»‡n: {cost_imp:+.1f}%\")\n",
                "print(f\"   âœ… Tá»· lá»‡ nÄƒng lÆ°á»£ng tÃ¡i táº¡o: {trained_results['renewable']:.1f}%\")\n",
                "print(f\"   ğŸ§  Thuáº­t toÃ¡n: PPO (Schulman et al., 2017)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“š Giáº£i ThÃ­ch PPO Chi Tiáº¿t\n",
                "\n",
                "### ğŸ¯ 5 HÃ nh Äá»™ng Cá»§a Agent (giá»‘ng DQN)\n",
                "\n",
                "| Action | TÃªn | Ã NghÄ©a |\n",
                "|--------|-----|--------|\n",
                "| 0 | Xáº£ pin | DÃ¹ng nÄƒng lÆ°á»£ng trong pin Ä‘á»ƒ cáº¥p Ä‘iá»‡n |\n",
                "| 1 | Sáº¡c tá»« renewable | LÆ°u nÄƒng lÆ°á»£ng máº·t trá»i/giÃ³ vÃ o pin |\n",
                "| 2 | Mua tá»« lÆ°á»›i | Mua toÃ n bá»™ Ä‘iá»‡n tá»« lÆ°á»›i Ä‘iá»‡n |\n",
                "| 3 | Renewable + Xáº£ pin | Æ¯u tiÃªn dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, thiáº¿u thÃ¬ xáº£ pin |\n",
                "| 4 | Renewable + LÆ°á»›i | Æ¯u tiÃªn dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, thiáº¿u thÃ¬ mua lÆ°á»›i |\n",
                "\n",
                "### ğŸ§  PPO Há»c NhÆ° Tháº¿ NÃ o?\n",
                "\n",
                "```\n",
                "1. ğŸ² Thu tháº­p rollout: Cháº¡y policy hiá»‡n táº¡i, lÆ°u (s, a, r, log_prob)\n",
                "   â†“\n",
                "2. ğŸ“Š TÃ­nh GAE Advantage: A_t = Î£ (Î³Î»)^l Â· Î´_{t+l}\n",
                "   â†“\n",
                "3. ğŸ”„ PPO Update (nhiá»u epochs):\n",
                "   L = min(ratio Ã— A, clip(ratio, 1-Îµ, 1+Îµ) Ã— A)\n",
                "   ratio = Ï€_new(a|s) / Ï€_old(a|s)\n",
                "   â†“\n",
                "4. ğŸ—‘ï¸ Clear buffer â†’ quay láº¡i bÆ°á»›c 1\n",
                "```\n",
                "\n",
                "### ğŸ”‘ CÃ´ng Thá»©c ChÃ­nh\n",
                "\n",
                "**Clipped Surrogate Objective:**\n",
                "```\n",
                "L_CLIP = E[min(r(Î¸)Â·A, clip(r(Î¸), 1-Îµ, 1+Îµ)Â·A)]\n",
                "```\n",
                "- `r(Î¸) = Ï€_new / Ï€_old` : tá»· sá»‘ xÃ¡c suáº¥t má»›i/cÅ©\n",
                "- `A` : advantage (GAE)\n",
                "- `Îµ` : clip range (thÆ°á»ng 0.2)\n",
                "- Clipping ngÄƒn policy thay Ä‘á»•i quÃ¡ lá»›n â†’ training á»•n Ä‘á»‹nh\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ”§ TÃ¹y Chá»‰nh Cho BÃ i LÃ m RiÃªng\n",
                "\n",
                "| Sinh viÃªn | SEED | EPISODES | LR_ACTOR | CLIP_Îµ | PPO_EPOCHS |\n",
                "|-----------|------|----------|----------|--------|------------|\n",
                "| SV1 | 42 | 200 | 3e-4 | 0.2 | 10 |\n",
                "| SV2 | 123 | 300 | 1e-4 | 0.1 | 8 |\n",
                "| SV3 | 456 | 150 | 5e-4 | 0.3 | 15 |\n",
                "| SV4 | 789 | 250 | 2e-4 | 0.2 | 5 |\n",
                "| SV5 | 999 | 400 | 4e-4 | 0.15 | 12 |\n",
                "\n",
                "### ğŸ“š TÃ i Liá»‡u Tham Kháº£o\n",
                "- PPO: Schulman et al., 2017 - \"Proximal Policy Optimization Algorithms\"\n",
                "- GAE: Schulman et al., 2015 - \"High-Dimensional Continuous Control Using GAE\"\n",
                "- Actor-Critic: Konda & Tsitsiklis, 2000 - \"Actor-Critic Algorithms\""
            ]
        }
    ]
}