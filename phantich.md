# ğŸ“‹ PHÃ‚N TÃCH YÃŠU Cáº¦U Äá»€ BÃ€I

## Microgrid Energy Optimization using Deep Reinforcement Learning

---

## 1. TÃ“M Táº®T Äá»€ BÃ€I

**Má»¥c tiÃªu**: XÃ¢y dá»±ng má»™t agent Deep Reinforcement Learning (DQN hoáº·c Policy Gradient) Ä‘á»ƒ **tá»‘i Æ°u hÃ³a phÃ¢n phá»‘i nÄƒng lÆ°á»£ng** trong há»‡ thá»‘ng microgrid (lÆ°á»›i Ä‘iá»‡n siÃªu nhá»).

**BÃ i toÃ¡n cá»‘t lÃµi**: Táº¡i má»—i bÆ°á»›c thá»i gian (má»—i giá» trong ngÃ y), agent pháº£i quyáº¿t Ä‘á»‹nh:

- PhÃ¢n bá»• nÄƒng lÆ°á»£ng Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u tiÃªu thá»¥
- LÆ°u trá»¯ nÄƒng lÆ°á»£ng dÆ° vÃ o pin
- Mua nÄƒng lÆ°á»£ng tá»« lÆ°á»›i Ä‘iá»‡n chÃ­nh khi cáº§n

**Ba má»¥c tiÃªu chÃ­nh**:

1. ğŸŒ¿ Tá»‘i Ä‘a hÃ³a sá»­ dá»¥ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o (solar + wind)
2. ğŸ’° Tá»‘i thiá»ƒu hÃ³a chi phÃ­ mua Ä‘iá»‡n tá»« lÆ°á»›i
3. âš¡ TrÃ¡nh thiáº¿u Ä‘iá»‡n (unmet demand)

---

## 2. PHÃ‚N TÃCH Cáº¤U TRÃšC BÃ€I LÃ€M (6 pháº§n)

### ğŸ“Œ Pháº§n 1: Problem Description (15%)

| YÃªu cáº§u | Chi tiáº¿t | CÃ¡ch giáº£i |
|----------|----------|-----------|
| MÃ´ táº£ há»‡ thá»‘ng microgrid | CÃ¡c nguá»“n nÄƒng lÆ°á»£ng, há»‡ thá»‘ng lÆ°u trá»¯, nhu cáº§u tiÃªu thá»¥ | Váº½ sÆ¡ Ä‘á»“ há»‡ thá»‘ng microgrid gá»“m: Solar Panel, Wind Turbine, Battery, Main Grid, Consumer Load |
| Táº¡i sao RL phÃ¹ há»£p hÆ¡n rule-based | So sÃ¡nh RL vs cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng | PhÃ¢n tÃ­ch: (1) BÃ i toÃ¡n quyáº¿t Ä‘á»‹nh tuáº§n tá»± â†’ phÃ¹ há»£p MDP; (2) TÃ­nh ngáº«u nhiÃªn cá»§a renewable/demand; (3) Rule-based khÃ´ng thÃ­ch á»©ng real-time |
| Háº¡n cháº¿ phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng | Linear programming, heuristic scheduling | LP cáº§n mÃ´ hÃ¬nh chÃ­nh xÃ¡c, khÃ´ng xá»­ lÃ½ tá»‘t uncertainty; Rule-based cá»©ng nháº¯c, khÃ´ng tá»‘i Æ°u toÃ n cá»¥c |
| á»¨ng dá»¥ng thá»±c táº¿ | Real-world relevance | Giáº£m carbon footprint, giáº£m chi phÃ­ nÄƒng lÆ°á»£ng, tÄƒng Ä‘á»™ tin cáº­y cho microgrid |

### ğŸ“Œ Pháº§n 2: MDP Modelling (20%) â­ TRá»ŒNG Sá» CAO

| ThÃ nh pháº§n MDP | Äáº·c táº£ trong bÃ i | Chi tiáº¿t triá»ƒn khai |
|----------------|-------------------|---------------------|
| **State Space** (8D) | Battery level, Demand, Renewable gen, Previous actions | `[battery_level, demand, solar, wind, grid_price, hour_sin, hour_cos, prev_action]` â€” táº¥t cáº£ normalized vá» [0,1] |
| **Action Space** (5 discrete) | Grid draw, Charge/discharge, Allocate to loads | Action 0: Xáº£ pin; Action 1: Sáº¡c tá»« renewable; Action 2: Mua tá»« grid; Action 3: Renewable + Xáº£ pin; Action 4: Renewable + Grid |
| **Reward Function** | Positive: renewable use; Negative: grid purchase, unmet demand | `R = 1.0Ã—(renewable_used) - 2.0Ã—(grid_cost) - 5.0Ã—(unmet) - 0.1Ã—(battery_wear) + 0.5Ã—(peak_bonus)` |
| **Transition Dynamics** | Battery updates, stochastic renewable, probabilistic demand | Battery cáº­p nháº­t theo efficiency (95%), solar/wind ngáº«u nhiÃªn, demand cÃ³ peak sÃ¡ng/tá»‘i |
| **Episode Termination** | End of day, critical battery, unmet threshold | 24 steps (1 ngÃ y), pin < 5% capacity, hoáº·c unmet ratio > 20% |

**Cáº§n lÃ m**: Váº½ **MDP diagram** thá»ƒ hiá»‡n rÃµ rÃ ng states â†’ actions â†’ rewards â†’ transitions.

### ğŸ“Œ Pháº§n 3: RL Algorithm (25%) â­â­ TRá»ŒNG Sá» CAO NHáº¤T

| YÃªu cáº§u | CÃ¡ch giáº£i |
|----------|-----------|
| **Thuáº­t toÃ¡n**: DQN (Deep Q-Network) | Sá»­ dá»¥ng **Double DQN** vá»›i target network riÃªng biá»‡t Ä‘á»ƒ giáº£m overestimation bias |
| **Exploration vs Exploitation** | Îµ-greedy: Îµ báº¯t Ä‘áº§u = 1.0, giáº£m dáº§n â†’ 0.01 vá»›i decay rate 0.995 |
| **Network Architecture** | MLP 3 layers: 256â†’256â†’128 neurons, ReLU activation, Dropout 0.1, Xavier init |
| **Hyperparameters** | LR=1e-4, Î³=0.99, batch_size=64, buffer=100K, target_update=1000 steps |
| **Training Process** | 500 episodes Ã— 24 steps/episode, Experience Replay, convergence monitoring |
| **Code** | Python + PyTorch, well-commented, Gymnasium-compatible |

**Kiáº¿n trÃºc máº¡ng neural** (cáº§n váº½ diagram):

```
Input (8D) â†’ Linear(8, 256) â†’ ReLU â†’ Dropout(0.1)
           â†’ Linear(256, 256) â†’ ReLU â†’ Dropout(0.1)
           â†’ Linear(256, 128) â†’ ReLU â†’ Dropout(0.1)
           â†’ Linear(128, 5) â†’ Q-values
```

### ğŸ“Œ Pháº§n 4: AI Optimization Analysis (15%)

| YÃªu cáº§u | CÃ¡ch giáº£i |
|----------|-----------|
| PhÃ¢n tÃ­ch cÃ¡ch RL tá»‘i Æ°u energy dispatch | MÃ´ táº£ policy há»c Ä‘Æ°á»£c: agent Æ°u tiÃªn renewable â†’ battery â†’ grid |
| Reward trends | Váº½ biá»ƒu Ä‘á»“ reward theo episode, cho tháº¥y convergence |
| Learning convergence | PhÃ¢n tÃ­ch loss curve, epsilon decay, Q-value trends |
| Policy efficiency | So sÃ¡nh agent vs random baseline: reward improvement, cost savings |

### ğŸ“Œ Pháº§n 5: Results & Evaluation (15%)

| Metric cáº§n trÃ¬nh bÃ y | Ã nghÄ©a |
|----------------------|----------|
| **Cumulative Reward** | Biá»ƒu Ä‘á»“ reward tÃ­ch lÅ©y qua cÃ¡c episodes |
| **Daily Cost Savings** | So sÃ¡nh chi phÃ­ agent vs baseline ($ savings) |
| **Renewable Usage Ratio** | Tá»· lá»‡ nÄƒng lÆ°á»£ng tÃ¡i táº¡o Ä‘Æ°á»£c sá»­ dá»¥ng (target > 60%) |
| **Unmet Demand Frequency** | Táº§n suáº¥t thiáº¿u Ä‘iá»‡n (target < 10%) |
| **Agent vs Random** | So sÃ¡nh hiá»‡u suáº¥t trained agent vs random policy |

**Graphs/Charts cáº§n táº¡o**:

1. Training reward curve (raw + smoothed)
2. Renewable ratio over episodes
3. 24-hour energy dispatch profile (1 ngÃ y máº«u)
4. Agent vs Random comparison bar chart

### ğŸ“Œ Pháº§n 6: Ethics & Future (10%)

| Chá»§ Ä‘á» | Ná»™i dung cáº§n tháº£o luáº­n |
|--------|----------------------|
| **Ethical concerns** | Bias trong data, transparency cá»§a AI decisions, accountability khi máº¥t Ä‘iá»‡n |
| **Practical issues** | Sim-to-real gap, computational requirements, safety constraints |
| **Future enhancements** | Multi-agent RL, continuous action space (DDPG/SAC), transfer learning, integration vá»›i IoT sensors |

---

## 3. ÄÃNH GIÃ TIáº¾N Äá»˜ HIá»†N Táº I

### âœ… ÄÃ£ hoÃ n thÃ nh

| Pháº§n | Tráº¡ng thÃ¡i | File |
|------|-----------|------|
| MicrogridEnv (Gymnasium-compatible) | âœ… HoÃ n thÃ nh | `Microgrid_DQN_Colab.py` |
| DQN Agent (Double DQN) | âœ… HoÃ n thÃ nh | `Microgrid_DQN_Colab.py` |
| Training pipeline | âœ… HoÃ n thÃ nh | `run_training.py` |
| Evaluation & Visualization | âœ… HoÃ n thÃ nh | `Microgrid_DQN_Colab.py` |
| Report (REPORT.md) | âœ… HoÃ n thÃ nh | `REPORT.md`, `REPORT.html` |
| Colab notebook | âœ… HoÃ n thÃ nh | `Microgrid_DQN_Colab.ipynb` |

### ğŸ“ LÆ°u Ã½ quan trá»ng

- Code Ä‘Æ°á»£c thiáº¿t káº¿ cho **5-7 sinh viÃªn** sá»­ dá»¥ng chung, má»—i ngÆ°á»i thay Ä‘á»•i hyperparameters (seed, lr, hidden_dims, episodes) Ä‘á»ƒ táº¡o bÃ i riÃªng biá»‡t
- CÃ¡c tham sá»‘ Ä‘Ã¡nh dáº¥u `ğŸ”§ [CUSTOMIZABLE]` cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y Ã½
- CÃ¡c tham sá»‘ Ä‘Ã¡nh dáº¥u `âš ï¸ [REQUIRED CHANGE]` **báº¯t buá»™c** pháº£i thay Ä‘á»•i

---

## 4. CHIáº¾N LÆ¯á»¢C LÃ€M BÃ€I CHO Tá»ªNG SINH VIÃŠN

### BÆ°á»›c 1: CÃ¡ nhÃ¢n hÃ³a Config

```python
# VÃ­ dá»¥ cho Sinh viÃªn 1:
CONFIG = {
    "seed": 42,
    "learning_rate": 1e-4,
    "hidden_dims": [256, 256, 128],
    "num_episodes": 500,
    # ... cÃ¡c tham sá»‘ khÃ¡c giá»¯ nguyÃªn hoáº·c thay Ä‘á»•i nháº¹
}
```

### BÆ°á»›c 2: Cháº¡y training trÃªn Google Colab

1. Upload `Microgrid_DQN_Colab.ipynb` lÃªn Google Colab
2. Thay Ä‘á»•i CONFIG theo gá»£i Ã½ cÃ¡ nhÃ¢n
3. Run All Cells â†’ Thu Ä‘Æ°á»£c káº¿t quáº£ training + evaluation

### BÆ°á»›c 3: Viáº¿t report theo 6 pháº§n

1. DÃ¹ng `REPORT.md` lÃ m template
2. Thay sá»‘ liá»‡u báº±ng káº¿t quáº£ thu Ä‘Æ°á»£c tá»« training cá»§a mÃ¬nh
3. Äáº£m báº£o cÃ³ Ä‘á»§: MDP diagram, code snippets, graphs, analysis

### BÆ°á»›c 4: Review & Submit

- Kiá»ƒm tra plagiarism
- Äáº£m báº£o referencing APA/IEEE
- Äáº£m báº£o bÃ i viáº¿t pháº£n Ã¡nh sá»± hiá»ƒu biáº¿t cÃ¡ nhÃ¢n

---

## 5. CÃC ÄIá»‚M Cáº¦N LÆ¯U Ã Äáº¶C BIá»†T

> âš ï¸ **Academic Integrity**: Táº¥t cáº£ nguá»“n pháº£i Ä‘Æ°á»£c trÃ­ch dáº«n Ä‘Ãºng APA/IEEE. KhÃ´ng Ä‘Æ°á»£c copy nguyÃªn vÄƒn tá»« AI tools. BÃ i pháº£i thá»ƒ hiá»‡n sá»± hiá»ƒu biáº¿t vÃ  phÃ¢n tÃ­ch riÃªng cá»§a sinh viÃªn.

> ğŸ’¡ **Gá»£i Ã½ tham kháº£o**:
>
> - DQN Paper: Mnih et al., 2015 â€” "Human-level control through deep reinforcement learning"
> - Double DQN: Van Hasselt et al., 2016 â€” "Deep Reinforcement Learning with Double Q-learning"
> - Microgrid optimization: CÃ¡c paper vá» smart grid + RL

> ğŸ¯ **Tá»· trá»ng Ä‘iá»ƒm**: Pháº§n 3 (Algorithm, 25%) vÃ  Pháº§n 2 (MDP, 20%) chiáº¿m **45%** tá»•ng Ä‘iá»ƒm â†’ cáº§n Ä‘áº§u tÆ° nhiá»u nháº¥t vÃ o pháº§n giáº£i thÃ­ch thuáº­t toÃ¡n vÃ  mÃ´ hÃ¬nh hÃ³a MDP.
