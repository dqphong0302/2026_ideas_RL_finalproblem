# BÃ€I TIá»‚U LUáº¬N

# Tá»I Æ¯U HÃ“A PHÃ‚N PHá»I NÄ‚NG LÆ¯á»¢NG TRONG MICROGRID Sá»¬ Dá»¤NG DEEP REINFORCEMENT LEARNING

**Sinh viÃªn:** [Há» vÃ  tÃªn]
**MSSV:** [MÃ£ sá»‘ sinh viÃªn]
**Lá»›p:** [TÃªn lá»›p]
**Giáº£ng viÃªn hÆ°á»›ng dáº«n:** [TÃªn GV]
**NgÃ y ná»™p:** [DD/MM/YYYY]

---

## TÃ“M Táº®T

BÃ i tiá»ƒu luáº­n nÃ y trÃ¬nh bÃ y viá»‡c á»©ng dá»¥ng thuáº­t toÃ¡n Deep Reinforcement Learning vÃ o bÃ i toÃ¡n tá»‘i Æ°u hÃ³a phÃ¢n phá»‘i nÄƒng lÆ°á»£ng trong há»‡ thá»‘ng microgrid â€” má»™t lÆ°á»›i Ä‘iá»‡n cá»¥c bá»™ tÃ­ch há»£p cÃ¡c nguá»“n nÄƒng lÆ°á»£ng tÃ¡i táº¡o, pin lÆ°u trá»¯ vÃ  káº¿t ná»‘i lÆ°á»›i Ä‘iá»‡n quá»‘c gia. BÃ i toÃ¡n Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a dÆ°á»›i dáº¡ng Markov Decision Process vá»›i khÃ´ng gian tráº¡ng thÃ¡i 8 chiá»u biá»ƒu diá»…n cÃ¡c yáº¿u tá»‘ nhÆ° má»©c pin, nhu cáº§u tiÃªu thá»¥, sáº£n lÆ°á»£ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, giÃ¡ Ä‘iá»‡n lÆ°á»›i vÃ  thá»i gian trong ngÃ y. Agent sá»­ dá»¥ng thuáº­t toÃ¡n [DQN / PPO] Ä‘á»ƒ há»c chÃ­nh sÃ¡ch chá»n má»™t trong 5 hÃ nh Ä‘á»™ng phÃ¢n phá»‘i nÄƒng lÆ°á»£ng tá»‘i Æ°u táº¡i má»—i bÆ°á»›c thá»i gian. Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y agent Ä‘Ã£ cáº£i thiá»‡n [X]% reward so vá»›i baseline ngáº«u nhiÃªn, giáº£m [X]% chi phÃ­ Ä‘iá»‡n lÆ°á»›i vÃ  Ä‘áº¡t [X]% tá»· lá»‡ sá»­ dá»¥ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, chá»©ng minh tiá»m nÄƒng á»©ng dá»¥ng cá»§a RL trong quáº£n lÃ½ nÄƒng lÆ°á»£ng thÃ´ng minh.

**Tá»« khÃ³a:** Reinforcement Learning, Deep Q-Network, Proximal Policy Optimization, Microgrid, Energy Management, Markov Decision Process

---

## PHáº¦N 1: MÃ” Táº¢ Váº¤N Äá»€ (15%)

### 1.1 Giá»›i Thiá»‡u Há»‡ Thá»‘ng Microgrid

Trong bá»‘i cáº£nh chuyá»ƒn Ä‘á»•i nÄƒng lÆ°á»£ng toÃ n cáº§u, há»‡ thá»‘ng microgrid (lÆ°á»›i Ä‘iá»‡n siÃªu nhá») Ä‘ang ná»•i lÃªn nhÆ° má»™t giáº£i phÃ¡p quan trá»ng cho viá»‡c tÃ­ch há»£p nguá»“n nÄƒng lÆ°á»£ng tÃ¡i táº¡o vÃ o háº¡ táº§ng Ä‘iá»‡n lá»±c. Microgrid lÃ  má»™t há»‡ thá»‘ng Ä‘iá»‡n quy mÃ´ nhá», cá»¥c bá»™, cÃ³ kháº£ nÄƒng hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p hoáº·c káº¿t ná»‘i vá»›i lÆ°á»›i Ä‘iá»‡n quá»‘c gia. KhÃ¡c vá»›i há»‡ thá»‘ng Ä‘iá»‡n táº­p trung truyá»n thá»‘ng â€” nÆ¡i Ä‘iá»‡n Ä‘Æ°á»£c sáº£n xuáº¥t táº¡i cÃ¡c nhÃ  mÃ¡y lá»›n vÃ  truyá»n táº£i qua khoáº£ng cÃ¡ch xa â€” microgrid sáº£n xuáº¥t vÃ  tiÃªu thá»¥ Ä‘iá»‡n ngay táº¡i chá»—, giáº£m tá»•n tháº¥t truyá»n táº£i vÃ  tÄƒng Ä‘á»™ tin cáº­y cung cáº¥p Ä‘iá»‡n.

Há»‡ thá»‘ng microgrid trong bÃ i toÃ¡n nÃ y bao gá»“m bá»‘n thÃ nh pháº§n cá»‘t lÃµi. ThÃ nh pháº§n Ä‘áº§u tiÃªn lÃ  **há»‡ thá»‘ng pin máº·t trá»i (Solar PV)**, sáº£n xuáº¥t Ä‘iá»‡n tá»« Ã¡nh sÃ¡ng máº·t trá»i. Sáº£n lÆ°á»£ng cá»§a solar phá»¥ thuá»™c vÃ o cÆ°á»ng Ä‘á»™ bá»©c xáº¡ vÃ  thay Ä‘á»•i theo thá»i gian trong ngÃ y â€” Ä‘áº¡t Ä‘á»‰nh vÃ o khoáº£ng 10h-14h trÆ°a vÃ  khÃ´ng phÃ¡t Ä‘iá»‡n vÃ o ban Ä‘Ãªm. NgoÃ i ra, yáº¿u tá»‘ thá»i tiáº¿t nhÆ° mÃ¢y vÃ  mÆ°a táº¡o ra nhiá»…u ngáº«u nhiÃªn, khiáº¿n sáº£n lÆ°á»£ng thá»±c táº¿ khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c 100%.

ThÃ nh pháº§n thá»© hai lÃ  **tuabin giÃ³ (Wind Turbine)**, phÃ¡t Ä‘iá»‡n tá»« nÄƒng lÆ°á»£ng giÃ³. KhÃ¡c vá»›i solar, tuabin giÃ³ cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng cáº£ ngÃ y láº«n Ä‘Ãªm, nhÆ°ng sáº£n lÆ°á»£ng phá»¥ thuá»™c hoÃ n toÃ n vÃ o tá»‘c Ä‘á»™ giÃ³ â€” má»™t yáº¿u tá»‘ biáº¿n thiÃªn máº¡nh vÃ  khÃ³ dá»± Ä‘oÃ¡n. Sá»± káº¿t há»£p giá»¯a solar vÃ  wind táº¡o ra nguá»“n nÄƒng lÆ°á»£ng tÃ¡i táº¡o bá»• sung láº«n nhau: solar máº¡nh ban ngÃ y, wind cÃ³ thá»ƒ máº¡nh ban Ä‘Ãªm.

ThÃ nh pháº§n thá»© ba lÃ  **pin lÆ°u trá»¯ nÄƒng lÆ°á»£ng (Battery Energy Storage System - BESS)** vá»›i dung lÆ°á»£ng 100 kWh. Pin Ä‘Ã³ng vai trÃ² then chá»‘t nhÆ° má»™t "bá»™ Ä‘á»‡m" nÄƒng lÆ°á»£ng: lÆ°u trá»¯ Ä‘iá»‡n dÆ° thá»«a khi nÄƒng lÆ°á»£ng tÃ¡i táº¡o phÃ¡t nhiá»u hÆ¡n nhu cáº§u, vÃ  giáº£i phÃ³ng nÄƒng lÆ°á»£ng khi nhu cáº§u vÆ°á»£t quÃ¡ kháº£ nÄƒng phÃ¡t. Tuy nhiÃªn, má»—i chu ká»³ sáº¡c/xáº£ Ä‘á»u gÃ¢y hao mÃ²n pin (hiá»‡u suáº¥t 95%), táº¡o ra trade-off giá»¯a giÃ¡ trá»‹ sá»­ dá»¥ng vÃ  tuá»•i thá» pin.

ThÃ nh pháº§n cuá»‘i cÃ¹ng lÃ  **káº¿t ná»‘i lÆ°á»›i Ä‘iá»‡n quá»‘c gia (Utility Grid)** â€” nguá»“n dá»± phÃ²ng khi tÃ¡i táº¡o vÃ  pin khÃ´ng Ä‘á»§ Ä‘Ã¡p á»©ng nhu cáº§u. Äiá»ƒm Ä‘Ã¡ng chÃº Ã½ lÃ  giÃ¡ Ä‘iá»‡n lÆ°á»›i biáº¿n Ä‘á»™ng theo thá»i gian: tháº¥p vÃ o off-peak hours (Ä‘Ãªm, sÃ¡ng sá»›m) vÃ  cao vÃ o peak hours (17h-21h). Sá»± biáº¿n Ä‘á»™ng giÃ¡ nÃ y táº¡o cÆ¡ há»™i tá»‘i Æ°u hÃ³a chi phÃ­ náº¿u cÃ³ chiáº¿n lÆ°á»£c mua Ä‘iá»‡n thÃ´ng minh.

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): BÃ i toÃ¡n "Äi chá»£ thÃ´ng minh"**
>
> HÃ£y tÆ°á»Ÿng tÆ°á»£ng há»‡ thá»‘ng microgrid giá»‘ng nhÆ° viá»‡c quáº£n lÃ½ chi tiÃªu cho báº¿p Äƒn gia Ä‘Ã¬nh báº¡n.
>
> - **Solar & Wind (NÄƒng lÆ°á»£ng tÃ¡i táº¡o):** Giá»‘ng nhÆ° rau cá»§ báº¡n tá»± trá»“ng Ä‘Æ°á»£c trong vÆ°á»n. LÃºc thÃ¬ Ä‘Æ°á»£c mÃ¹a (náº¯ng to, giÃ³ lá»›n), lÃºc thÃ¬ máº¥t mÃ¹a (mÆ°a bÃ£o), nhÆ°ng quan trá»ng lÃ  nÃ³ "miá»…n phÃ­".
> - **Pin lÆ°u trá»¯:** Giá»‘ng nhÆ° cÃ¡i tá»§ láº¡nh. Rau cá»§ Äƒn khÃ´ng háº¿t thÃ¬ bá» tá»§ láº¡nh Ä‘á»ƒ dÃ nh, lÃºc nÃ o vÆ°á»n khÃ´ng cÃ³ rau thÃ¬ láº¥y ra dÃ¹ng. NhÆ°ng tá»§ láº¡nh cÃ³ sá»©c chá»©a giá»›i háº¡n (dung lÆ°á»£ng pin) vÃ  viá»‡c cáº¥t vÃ o/láº¥y ra liÃªn tá»¥c cÅ©ng lÃ m rau bá»›t tÆ°Æ¡i (hao mÃ²n pin).
> - **LÆ°á»›i Ä‘iá»‡n:** Giá»‘ng nhÆ° Ä‘i siÃªu thá»‹ mua rau. SiÃªu thá»‹ lÃºc nÃ o cÅ©ng cÃ³ bÃ¡n, nhÆ°ng giÃ¡ cáº£ thay Ä‘á»•i theo giá» (giá» cao Ä‘iá»ƒm Ä‘áº¯t, giá» tháº¥p Ä‘iá»ƒm ráº»).
>
> **Má»¥c tiÃªu cá»§a AI:** LÃ  ngÆ°á»i quáº£n gia thÃ´ng minh, biáº¿t nhÃ¬n trá»i nhÃ¬n Ä‘áº¥t Ä‘á»ƒ quyáº¿t Ä‘á»‹nh: TrÆ°a nay náº¯ng to rau Ä‘áº§y vÆ°á»n thÃ¬ Äƒn rau vÆ°á»n, dÆ° thÃ¬ cáº¥t tá»§ láº¡nh. Chiá»u tá»‘i rau Ä‘áº¯t thÃ¬ láº¥y tá»§ láº¡nh ra Äƒn chá»© Ä‘á»«ng Ä‘i siÃªu thá»‹. Chá»‰ khi nÃ o vÆ°á»n háº¿t rau, tá»§ láº¡nh trá»‘ng rá»—ng má»›i báº¥t Ä‘áº¯c dÄ© Ä‘i siÃªu thá»‹ mua, mÃ  pháº£i canh lÃºc siÃªu thá»‹ giáº£m giÃ¡ hÃ£y mua.

### 1.2 Táº¡i Sao ÄÃ¢y LÃ  BÃ i ToÃ¡n Ra Quyáº¿t Äá»‹nh Tuáº§n Tá»±?

PhÃ¢n phá»‘i nÄƒng lÆ°á»£ng trong microgrid thá»a mÃ£n Ä‘áº§y Ä‘á»§ cÃ¡c Ä‘áº·c Ä‘iá»ƒm cá»§a má»™t bÃ i toÃ¡n ra quyáº¿t Ä‘á»‹nh tuáº§n tá»± (sequential decision-making problem). Äá»ƒ hiá»ƒu rÃµ, ta phÃ¢n tÃ­ch ba Ä‘áº·c Ä‘iá»ƒm quan trá»ng sau.

**Äáº·c Ä‘iá»ƒm thá»© nháº¥t lÃ  liÃªn káº¿t thá»i gian (Temporal Coupling).** Quyáº¿t Ä‘á»‹nh táº¡i thá»i Ä‘iá»ƒm hiá»‡n táº¡i áº£nh hÆ°á»Ÿng trá»±c tiáº¿p vÃ  khÃ´ng thá»ƒ Ä‘áº£o ngÆ°á»£c Ä‘áº¿n tráº¡ng thÃ¡i tÆ°Æ¡ng lai. Láº¥y vÃ­ dá»¥ cá»¥ thá»ƒ: náº¿u agent quyáº¿t Ä‘á»‹nh xáº£ háº¿t pin lÃºc 15h chiá»u Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u buá»•i chiá»u, thÃ¬ Ä‘áº¿n 18h â€” khi giÃ¡ Ä‘iá»‡n lÆ°á»›i tÄƒng vá»t do peak hours â€” sáº½ khÃ´ng cÃ²n nÄƒng lÆ°á»£ng dá»± trá»¯ trong pin, buá»™c pháº£i mua Ä‘iá»‡n giÃ¡ cao tá»« lÆ°á»›i. Tráº¡ng thÃ¡i pin táº¡i má»—i thá»i Ä‘iá»ƒm lÃ  há»‡ quáº£ trá»±c tiáº¿p cá»§a chuá»—i quyáº¿t Ä‘á»‹nh sáº¡c/xáº£ trÆ°á»›c Ä‘Ã³. Äiá»u nÃ y cho tháº¥y má»—i quyáº¿t Ä‘á»‹nh khÃ´ng thá»ƒ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ má»™t cÃ¡ch cÃ´ láº­p mÃ  pháº£i xem xÃ©t trong bá»‘i cáº£nh chuá»—i quyáº¿t Ä‘á»‹nh liÃªn tá»¥c.

**Äáº·c Ä‘iá»ƒm thá»© hai lÃ  háº­u quáº£ trÃ¬ hoÃ£n (Delayed Consequences).** TÃ¡c Ä‘á»™ng thá»±c sá»± cá»§a má»™t hÃ nh Ä‘á»™ng thÆ°á»ng khÃ´ng thá»ƒ hiá»‡n ngay láº­p tá»©c mÃ  cáº§n thá»i gian Ä‘á»ƒ bá»™c lá»™. VÃ­ dá»¥: viá»‡c sáº¡c pin lÃºc 12h trÆ°a (khi solar Ä‘ang á»Ÿ Ä‘á»‰nh) cÃ³ váº» "lÃ£ng phÃ­" á»Ÿ thá»i Ä‘iá»ƒm hiá»‡n táº¡i â€” vÃ¬ solar Ä‘Ã£ Ä‘á»§ cung cáº¥p cho nhu cáº§u â€” nhÆ°ng chÃ­nh lÆ°á»£ng pin Ä‘Ã£ sáº¡c Ä‘áº§y nÃ y sáº½ táº¡o ra giÃ¡ trá»‹ lá»›n vÃ o buá»•i tá»‘i khi agent cÃ³ thá»ƒ xáº£ pin thay vÃ¬ mua Ä‘iá»‡n giÃ¡ cao. Pháº§n thÆ°á»Ÿng (reward) thá»±c sá»± cá»§a hÃ nh Ä‘á»™ng sáº¡c pin chá»‰ Ä‘Æ°á»£c "thu hoáº¡ch" sau 5-6 giá». ÄÃ¢y chÃ­nh lÃ  Ä‘áº·c trÆ°ng mÃ  cÃ¡c thuáº­t toÃ¡n RL, vá»›i cÆ¡ cháº¿ discount factor Î³, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ hiá»‡u quáº£.

**Äáº·c Ä‘iá»ƒm thá»© ba lÃ  trade-off giá»¯a lá»£i Ã­ch ngáº¯n háº¡n vÃ  dÃ i háº¡n.** Táº¡i má»—i bÆ°á»›c thá»i gian, agent Ä‘á»©ng trÆ°á»›c sá»± lá»±a chá»n: tá»‘i Ä‘a hÃ³a reward tá»©c thÃ¬ hay hy sinh lá»£i Ã­ch hiá»‡n táº¡i Ä‘á»ƒ Ä‘áº¡t tá»•ng reward dÃ i háº¡n lá»›n hÆ¡n? VÃ­ dá»¥, lÃºc trÆ°a khi solar dÆ° thá»«a, agent cÃ³ thá»ƒ bÃ¡n Ä‘iá»‡n cho lÆ°á»›i (reward tá»©c thÃ¬) hoáº·c lÆ°u vÃ o pin Ä‘á»ƒ dÃ¹ng lÃºc tá»‘i (reward tÆ°Æ¡ng lai cao hÆ¡n). Kháº£ nÄƒng cÃ¢n báº±ng trade-off nÃ y chÃ­nh lÃ  tháº¿ máº¡nh cá»‘t lÃµi cá»§a Reinforcement Learning so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u ngáº¯n háº¡n.

### 1.3 Háº¡n Cháº¿ Cá»§a PhÆ°Æ¡ng PhÃ¡p Truyá»n Thá»‘ng

CÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a truyá»n thá»‘ng, tuy Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u rá»™ng rÃ£i, Ä‘á»u bá»™c lá»™ háº¡n cháº¿ Ä‘Ã¡ng ká»ƒ khi Ã¡p dá»¥ng cho bÃ i toÃ¡n microgrid. PhÆ°Æ¡ng phÃ¡p **rule-based** sá»­ dá»¥ng bá»™ quy táº¯c if-then cá»‘ Ä‘á»‹nh (vÃ­ dá»¥: "luÃ´n Æ°u tiÃªn dÃ¹ng solar, náº¿u thiáº¿u thÃ¬ xáº£ pin, cuá»‘i cÃ¹ng má»›i mua lÆ°á»›i"). Tuy Ä‘Æ¡n giáº£n vÃ  dá»… triá»ƒn khai, cÃ¡ch tiáº¿p cáº­n nÃ y hoÃ n toÃ n cá»©ng nháº¯c â€” khÃ´ng thá»ƒ thÃ­ch á»©ng khi thá»i tiáº¿t thay Ä‘á»•i Ä‘á»™t ngá»™t hay khi pattern nhu cáº§u báº¥t thÆ°á»ng. Má»™t ngÃ y nhiá»u mÃ¢y Ä‘á»™t xuáº¥t sáº½ khiáº¿n há»‡ thá»‘ng rule-based hoáº¡t Ä‘á»™ng kÃ©m hiá»‡u quáº£ vÃ¬ nÃ³ khÃ´ng cÃ³ kháº£ nÄƒng "há»c" vÃ  Ä‘iá»u chá»‰nh.

PhÆ°Æ¡ng phÃ¡p **Linear Programming (LP)** tÃ¬m nghiá»‡m tá»‘i Æ°u báº±ng tá»‘i Æ°u hÃ³a toÃ¡n há»c, nhÆ°ng yÃªu cáº§u hai Ä‘iá»u kiá»‡n kháº¯t khe: thá»© nháº¥t, pháº£i cÃ³ dá»± bÃ¡o hoÃ n háº£o vá» demand, solar vÃ  wind trong toÃ n bá»™ horizon; thá»© hai, táº¥t cáº£ quan há»‡ pháº£i tuyáº¿n tÃ­nh. Trong thá»±c táº¿, cáº£ hai Ä‘iá»u kiá»‡n nÃ y Ä‘á»u khÃ´ng thá»a mÃ£n â€” nhu cáº§u vÃ  thá»i tiáº¿t lÃ  stochastic, vÃ  má»‘i quan há»‡ giá»¯a cÃ¡c biáº¿n thÆ°á»ng phi tuyáº¿n.

PhÆ°Æ¡ng phÃ¡p **Model Predictive Control (MPC)** giáº£i bÃ i toÃ¡n tá»‘i Æ°u trÃªn má»™t horizon cuá»™n (rolling horizon), cáº­p nháº­t liÃªn tá»¥c. Tuy linh hoáº¡t hÆ¡n LP, MPC Ä‘Ã²i há»i mÃ´ hÃ¬nh toÃ¡n há»c chÃ­nh xÃ¡c cá»§a há»‡ thá»‘ng vÃ  chi phÃ­ tÃ­nh toÃ¡n ráº¥t cao â€” khÃ´ng phÃ¹ há»£p cho á»©ng dá»¥ng real-time trÃªn edge devices.

CÃ¡c phÆ°Æ¡ng phÃ¡p **heuristic** nhÆ° Genetic Algorithm (GA) hay Particle Swarm Optimization (PSO) cÃ³ thá»ƒ xá»­ lÃ½ khÃ´ng gian phá»©c táº¡p, nhÆ°ng khÃ´ng Ä‘áº£m báº£o tá»‘i Æ°u toÃ n cá»¥c vÃ  thÆ°á»ng há»™i tá»¥ cháº­m.

### 1.4 Táº¡i Sao Reinforcement Learning PhÃ¹ Há»£p?

Reinforcement Learning kháº¯c phá»¥c cÃ¡c háº¡n cháº¿ trÃªn má»™t cÃ¡ch triá»‡t Ä‘á»ƒ. Thá»© nháº¥t, RL lÃ  **model-free** â€” agent khÃ´ng cáº§n biáº¿t trÆ°á»›c mÃ´ hÃ¬nh toÃ¡n há»c cá»§a há»‡ thá»‘ng mÃ  tá»± há»c chÃ­nh sÃ¡ch tá»‘i Æ°u thÃ´ng qua tÆ°Æ¡ng tÃ¡c trá»±c tiáº¿p vá»›i mÃ´i trÆ°á»ng. Thá»© hai, RL tá»± nhiÃªn **adaptive** â€” policy Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c nÃªn cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i cÃ¡c Ä‘iá»u kiá»‡n má»›i. Thá»© ba, cÆ¡ cháº¿ **discount factor Î³** cho phÃ©p agent tá»± Ä‘á»™ng cÃ¢n báº±ng giá»¯a lá»£i Ã­ch ngáº¯n háº¡n vÃ  dÃ i háº¡n mÃ  khÃ´ng cáº§n láº­p trÃ¬nh tÆ°á»ng minh. Cuá»‘i cÃ¹ng, RL xá»­ lÃ½ tá»‘t **uncertainty** â€” vÃ¬ agent Ä‘Æ°á»£c train trÃªn hÃ ng nghÃ¬n episodes vá»›i demand vÃ  renewable generation ngáº«u nhiÃªn, nÃ³ phÃ¡t triá»ƒn policy robust vá»›i nhiá»u ká»‹ch báº£n khÃ¡c nhau.

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): Táº¡i sao cáº§n AI "há»c" (Learning)?**
>
> CÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng giá»‘ng nhÆ° viá»‡c láº­p trÃ¬nh má»™t con robot báº±ng nhá»¯ng cÃ¢u lá»‡nh cá»©ng nháº¯c: "Náº¿u tháº¥y tÆ°á»ng thÃ¬ ráº½ trÃ¡i". Náº¿u gáº·p cÃ¡i há»‘ thay vÃ¬ bá»©c tÆ°á»ng, robot sáº½ Ä‘á»©ng im hoáº·c rÆ¡i xuá»‘ng há»‘ vÃ¬ chÆ°a Ä‘Æ°á»£c dáº¡y tÃ¬nh huá»‘ng Ä‘Ã³.
>
> **Reinforcement Learning (Há»c tÄƒng cÆ°á»ng)** giá»‘ng nhÆ° cÃ¡ch dáº¡y má»™t chÃº chÃ³ hoáº·c má»™t Ä‘á»©a tráº» táº­p Ä‘i xe Ä‘áº¡p:
>
> - Báº¡n khÃ´ng viáº¿t cÃ´ng thá»©c váº­t lÃ½ cÃ¢n báº±ng cho Ä‘á»©a bÃ©.
> - Thay vÃ o Ä‘Ã³, Ä‘á»©a bÃ© tá»± thá»­: Ä‘áº¡p xe -> ngÃ£ (Ä‘au = pháº¡t/pháº§n thÆ°á»Ÿng Ã¢m) -> láº§n sau sá»­a tÆ° tháº¿. Äáº¡p xe -> Ä‘i Ä‘Æ°á»£c má»™t Ä‘oáº¡n (vui = pháº§n thÆ°á»Ÿng dÆ°Æ¡ng).
> - Sau hÃ ng ngÃ n láº§n thá»­ sai (trong mÃ´i trÆ°á»ng giáº£ láº­p), AI tá»± rÃºt ra kinh nghiá»‡m "xÆ°Æ¡ng mÃ¡u" Ä‘á»ƒ Ä‘iá»u khiá»ƒn há»‡ thá»‘ng Ä‘iá»‡n má»™t cÃ¡ch linh hoáº¡t nháº¥t, á»©ng phÃ³ Ä‘Æ°á»£c cáº£ nhá»¯ng tÃ¬nh huá»‘ng mÆ°a náº¯ng tháº¥t thÆ°á»ng mÃ  ngÆ°á»i láº­p trÃ¬nh khÃ´ng lÆ°á»ng trÆ°á»›c háº¿t Ä‘Æ°á»£c.

---

## PHáº¦N 2: MÃ” HÃŒNH HÃ“A MDP (20%)

### 2.1 Tá»•ng Quan Markov Decision Process

BÆ°á»›c Ä‘áº§u tiÃªn vÃ  quan trá»ng nháº¥t trong viá»‡c Ã¡p dá»¥ng Reinforcement Learning lÃ  mÃ´ hÃ¬nh hÃ³a bÃ i toÃ¡n thá»±c táº¿ thÃ nh má»™t Markov Decision Process (MDP) formal. MDP Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi tuple (S, A, P, R, Î³) trong Ä‘Ã³ S lÃ  khÃ´ng gian tráº¡ng thÃ¡i, A lÃ  khÃ´ng gian hÃ nh Ä‘á»™ng, P lÃ  hÃ m chuyá»ƒn Ä‘á»•i tráº¡ng thÃ¡i, R lÃ  hÃ m pháº§n thÆ°á»Ÿng, vÃ  Î³ lÃ  há»‡ sá»‘ chiáº¿t kháº¥u. TÃ­nh cháº¥t Markov Ä‘Ã²i há»i tráº¡ng thÃ¡i hiá»‡n táº¡i pháº£i chá»©a Ä‘á»§ thÃ´ng tin Ä‘á»ƒ dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai mÃ  khÃ´ng cáº§n biáº¿t lá»‹ch sá»­ â€” Ä‘iá»u nÃ y Ä‘Æ°á»£c Ä‘áº£m báº£o trong bÃ i toÃ¡n microgrid vÃ¬ tráº¡ng thÃ¡i 8 chiá»u Ä‘Ã£ capture Ä‘áº§y Ä‘á»§ cÃ¡c yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh: má»©c pin, demand, renewable generation, giÃ¡ Ä‘iá»‡n vÃ  thá»i gian.

### 2.2 KhÃ´ng Gian Tráº¡ng ThÃ¡i

KhÃ´ng gian tráº¡ng thÃ¡i Ä‘Æ°á»£c thiáº¿t káº¿ gá»“m 8 biáº¿n liÃªn tá»¥c, má»—i biáº¿n Ä‘Æ°á»£c chuáº©n hÃ³a (normalize) vá» khoáº£ng [0, 1] hoáº·c [-1, 1] Ä‘á»ƒ giÃºp neural network há»c hiá»‡u quáº£ hÆ¡n.

Biáº¿n Ä‘áº§u tiÃªn lÃ  **battery_level** âˆˆ [0, 1], biá»ƒu diá»…n má»©c nÄƒng lÆ°á»£ng hiá»‡n táº¡i cá»§a pin Ä‘Ã£ chuáº©n hÃ³a theo dung lÆ°á»£ng tá»‘i Ä‘a. GiÃ¡ trá»‹ 0 nghÄ©a lÃ  pin trá»‘ng hoÃ n toÃ n, giÃ¡ trá»‹ 1 nghÄ©a lÃ  pin Ä‘áº§y. Biáº¿n nÃ y quyáº¿t Ä‘á»‹nh trá»±c tiáº¿p agent cÃ³ thá»ƒ xáº£ pin hay khÃ´ng vÃ  cÃ²n bao nhiÃªu dung lÆ°á»£ng Ä‘á»ƒ sáº¡c thÃªm.

Biáº¿n thá»© hai lÃ  **demand** âˆˆ [0, 1], biá»ƒu diá»…n nhu cáº§u tiÃªu thá»¥ Ä‘iá»‡n hiá»‡n táº¡i Ä‘Ã£ chuáº©n hÃ³a. Demand thay Ä‘á»•i theo pattern trong ngÃ y â€” cao vÃ o buá»•i sÃ¡ng (7h-9h) vÃ  buá»•i tá»‘i (18h-21h), tháº¥p vÃ o ban Ä‘Ãªm â€” kÃ¨m nhiá»…u ngáº«u nhiÃªn mÃ´ phá»ng sá»± biáº¿n Ä‘á»™ng thá»±c táº¿.

Biáº¿n thá»© ba vÃ  thá»© tÆ° lÃ  **solar_generation** vÃ  **wind_generation**, láº§n lÆ°á»£t biá»ƒu diá»…n cÃ´ng suáº¥t phÃ¡t Ä‘iá»‡n tá»« pin máº·t trá»i vÃ  tuabin giÃ³. Hai biáº¿n nÃ y cho agent biáº¿t cÃ³ bao nhiÃªu nÄƒng lÆ°á»£ng "miá»…n phÃ­" (khÃ´ng tá»‘n chi phÃ­ biáº¿n Ä‘á»•i) Ä‘ang sáºµn cÃ³ táº¡i thá»i Ä‘iá»ƒm hiá»‡n táº¡i.

Biáº¿n thá»© nÄƒm lÃ  **grid_price** âˆˆ [0, 1], giÃ¡ Ä‘iá»‡n lÆ°á»›i quá»‘c gia Ä‘Ã£ chuáº©n hÃ³a. GiÃ¡ cao trong peak hours (17h-21h) vÃ  tháº¥p trong off-peak, táº¡o cÆ¡ há»™i tá»‘i Æ°u hÃ³a chi phÃ­.

Biáº¿n thá»© sÃ¡u vÃ  thá»© báº£y lÃ  **hour_sin** vÃ  **hour_cos** â€” mÃ£ hÃ³a tuáº§n hoÃ n cá»§a thá»i gian trong ngÃ y báº±ng hÃ m sin vÃ  cos. LÃ½ do sá»­ dá»¥ng mÃ£ hÃ³a nÃ y thay vÃ¬ giÃ¡ trá»‹ giá» trá»±c tiáº¿p: náº¿u dÃ¹ng hour = 0, 1, ..., 23 thÃ¬ model sáº½ "nghÄ©" ráº±ng 23h vÃ  0h ráº¥t xa nhau (khoáº£ng cÃ¡ch = 23), trong khi thá»±c táº¿ chÃºng chá»‰ cÃ¡ch nhau 1 giá». MÃ£ hÃ³a sin/cos giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch Ä‘Æ°a thá»i gian lÃªn vÃ²ng trÃ²n Ä‘Æ¡n vá»‹, nÆ¡i 23h vÃ  0h náº±m cáº¡nh nhau.

Biáº¿n cuá»‘i cÃ¹ng lÃ  **prev_action** âˆˆ [0, 1], hÃ nh Ä‘á»™ng trÆ°á»›c Ä‘Ã³ Ä‘Ã£ chuáº©n hÃ³a, giÃºp agent duy trÃ¬ tÃ­nh nháº¥t quÃ¡n trong chuá»—i quyáº¿t Ä‘á»‹nh vÃ  trÃ¡nh switching liÃªn tá»¥c giá»¯a cÃ¡c cháº¿ Ä‘á»™.

### 2.3 KhÃ´ng Gian HÃ nh Äá»™ng

KhÃ´ng gian hÃ nh Ä‘á»™ng gá»“m 5 hÃ nh Ä‘á»™ng rá»i ráº¡c, má»—i hÃ nh Ä‘á»™ng tÆ°Æ¡ng á»©ng vá»›i má»™t cháº¿ Ä‘á»™ váº­n hÃ nh cá»¥ thá»ƒ cá»§a há»‡ thá»‘ng microgrid.

**Action 0 â€” Xáº£ pin (Discharge):** Agent quyáº¿t Ä‘á»‹nh sá»­ dá»¥ng nÄƒng lÆ°á»£ng trong pin Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u tiÃªu thá»¥. HÃ nh Ä‘á»™ng nÃ y phÃ¹ há»£p khi giÃ¡ Ä‘iá»‡n lÆ°á»›i cao (peak hours) vÃ  pin cÃ²n Ä‘á»§ nÄƒng lÆ°á»£ng. LÆ°á»£ng xáº£ bá»‹ giá»›i háº¡n bá»Ÿi cÃ´ng suáº¥t xáº£ tá»‘i Ä‘a vÃ  má»©c pin hiá»‡n táº¡i.

**Action 1 â€” Sáº¡c tá»« renewable:** Agent lÆ°u nÄƒng lÆ°á»£ng tÃ¡i táº¡o dÆ° thá»«a vÃ o pin. HÃ nh Ä‘á»™ng nÃ y tá»‘i Æ°u khi sáº£n lÆ°á»£ng solar/wind vÆ°á»£t quÃ¡ nhu cáº§u hiá»‡n táº¡i vÃ  pin chÆ°a Ä‘áº§y â€” thÆ°á»ng xáº£y ra vÃ o buá»•i trÆ°a khi solar á»Ÿ Ä‘á»‰nh.

**Action 2 â€” Mua tá»« lÆ°á»›i:** Agent mua toÃ n bá»™ Ä‘iá»‡n cáº§n thiáº¿t tá»« lÆ°á»›i quá»‘c gia. ÄÃ¢y lÃ  hÃ nh Ä‘á»™ng "cuá»‘i cÃ¹ng" khi khÃ´ng Ä‘á»§ renewable vÃ  pin trá»‘ng, hoáº·c khi giÃ¡ lÆ°á»›i Ä‘ang tháº¥p (off-peak) nÃªn mua lÆ°á»›i ráº» hÆ¡n xáº£ pin.

**Action 3 â€” Renewable + Xáº£ pin:** Agent Æ°u tiÃªn sá»­ dá»¥ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o trÆ°á»›c, pháº§n thiáº¿u Ä‘Æ°á»£c bÃ¹ báº±ng xáº£ pin. HÃ nh Ä‘á»™ng nÃ y káº¿t há»£p lá»£i Ã­ch cá»§a renewable (miá»…n phÃ­) vá»›i pin (Ä‘Ã£ sáº¡c trÆ°á»›c Ä‘Ã³), trÃ¡nh hoÃ n toÃ n mua lÆ°á»›i.

**Action 4 â€” Renewable + LÆ°á»›i:** Agent Æ°u tiÃªn renewable, pháº§n thiáº¿u mua tá»« lÆ°á»›i thay vÃ¬ xáº£ pin. HÃ nh Ä‘á»™ng nÃ y báº£o toÃ n pin cho cÃ¡c thá»i Ä‘iá»ƒm quan trá»ng hÆ¡n (peak hours), phÃ¹ há»£p khi pin Ä‘ang tháº¥p hoáº·c giÃ¡ lÆ°á»›i chÆ°a cao.

Viá»‡c sá»­ dá»¥ng action rá»i ráº¡c thay vÃ¬ liÃªn tá»¥c cÃ³ ba lÃ½ do: phÃ¹ há»£p vá»›i thuáº­t toÃ¡n DQN (thiáº¿t káº¿ cho discrete actions), má»—i action tÆ°Æ¡ng á»©ng vá»›i cháº¿ Ä‘á»™ váº­n hÃ nh thá»±c táº¿ dá»… hiá»ƒu cho operator, vÃ  khÃ´ng gian action nhá» giÃºp agent há»c nhanh hÆ¡n.

### 2.4 HÃ m Pháº§n ThÆ°á»Ÿng

HÃ m pháº§n thÆ°á»Ÿng (reward function) Ä‘Æ°á»£c thiáº¿t káº¿ cáº©n tháº­n Ä‘á»ƒ hÆ°á»›ng dáº«n agent há»c chÃ­nh sÃ¡ch mong muá»‘n. Reward táº¡i má»—i bÆ°á»›c Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c:

R(s, a, s') = R_renewable + R_grid + R_unmet + R_battery + R_bonus

```

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): CÃ¡ch cháº¥m Ä‘iá»ƒm cho AI**
>
> Äá»ƒ dáº¡y AI lÃ m Ä‘Ãºng Ã½ mÃ¬nh, chÃºng ta dÃ¹ng há»‡ thá»‘ng thÆ°á»Ÿng/pháº¡t tÆ°Æ¡ng tá»± nhÆ° dáº¡y thÃº cÆ°ng:
> - **R_renewable (+1 Ä‘iá»ƒm):** DÃ¹ng Ä‘iá»‡n máº·t trá»i/giÃ³ -> "Good boy!", cho cÃ¡i bÃ¡nh quy. Khuyáº¿n khÃ­ch hÃ nh vi nÃ y.
> - **R_grid (-2 Ä‘iá»ƒm):** Mua Ä‘iá»‡n lÆ°á»›i -> "Bad boy!", bá»‹ máº¯ng nháº¹. AI sáº½ hiá»ƒu lÃ  nÃªn háº¡n cháº¿, trá»« khi báº¯t buá»™c.
> - **R_unmet (-5 Ä‘iá»ƒm):** Äá»ƒ nhÃ  máº¥t Ä‘iá»‡n -> Pháº¡t náº·ng! AI sáº½ sá»£ vÃ  tÃ¬m má»i cÃ¡ch trÃ¡nh tÃ¬nh huá»‘ng nÃ y, coi Ä‘Ã³ lÃ  Æ°u tiÃªn sá»‘ng cÃ²n.
> - **R_battery (-0.1 Ä‘iá»ƒm):** Nghá»‹ch pin (sáº¡c xáº£ vÃ´ cá»›) -> Bá»‹ nháº¯c nhá»Ÿ nháº¹. Äá»ƒ AI biáº¿t pin cÅ©ng cáº§n giá»¯ gÃ¬n, khÃ´ng nÃªn dÃ¹ng phung phÃ­.
>
> ThÃ´ng qua hÃ ng triá»‡u láº§n chÆ¡i thá»­ vÃ  cá»™ng Ä‘iá»ƒm láº¡i, AI sáº½ tá»± hiá»ƒu chiáº¿n thuáº­t tá»‘i Æ°u: "Ã€, mÃ¬nh pháº£i rÃ¡ng dÃ¹ng Ä‘iá»‡n trá»i cho, háº¡n cháº¿ mua Ä‘iá»‡n, vÃ  tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘á»ƒ máº¥t Ä‘iá»‡n thÃ¬ má»›i Ä‘Æ°á»£c Ä‘iá»ƒm cao nháº¥t!"
```

**ThÃ nh pháº§n R_renewable = +1.0 Ã— (renewable_used / base_demand)** thÆ°á»Ÿng cho viá»‡c sá»­ dá»¥ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o. Há»‡ sá»‘ dÆ°Æ¡ng (+1.0) táº¡o incentive rÃµ rÃ ng cho agent Æ°u tiÃªn solar/wind. Chuáº©n hÃ³a theo base_demand Ä‘áº£m báº£o reward á»•n Ä‘á»‹nh báº¥t ká»ƒ quy mÃ´ nhu cáº§u.

**ThÃ nh pháº§n R_grid = -2.0 Ã— (grid_purchased / base_demand) Ã— normalized_price** pháº¡t viá»‡c mua Ä‘iá»‡n tá»« lÆ°á»›i, nhÃ¢n thÃªm vá»›i giÃ¡ Ä‘iá»‡n hiá»‡n táº¡i. Äiá»u nÃ y cÃ³ nghÄ©a mua lÆ°á»›i lÃºc peak (giÃ¡ cao) bá»‹ pháº¡t náº·ng hÆ¡n mua lÃºc off-peak (giÃ¡ tháº¥p) â€” khuyáº¿n khÃ­ch agent táº­p trung trÃ¡nh mua lÆ°á»›i vÃ o Ä‘Ãºng thá»i Ä‘iá»ƒm Ä‘áº¯t nháº¥t.

**ThÃ nh pháº§n R_unmet = -5.0 Ã— (unmet_demand / base_demand)** pháº¡t náº·ng nháº¥t khi khÃ´ng Ä‘Ã¡p á»©ng Ä‘á»§ nhu cáº§u. Há»‡ sá»‘ -5.0 (lá»›n nháº¥t) pháº£n Ã¡nh Æ°u tiÃªn hÃ ng Ä‘áº§u lÃ  Ä‘áº£m báº£o reliability â€” khÃ´ng cÃ³ há»™ gia Ä‘Ã¬nh nÃ o bá»‹ máº¥t Ä‘iá»‡n. Agent sáº½ cháº¥p nháº­n chi phÃ­ cao hÆ¡n Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»§ Ä‘iá»‡n.

**ThÃ nh pháº§n R_battery = -0.1 Ã— battery_activity** pháº¡t nháº¹ cho má»—i láº§n sáº¡c/xáº£, mÃ´ phá»ng chi phÃ­ hao mÃ²n pin thá»±c táº¿. Há»‡ sá»‘ nhá» (-0.1) Ä‘á»§ Ä‘á»ƒ ngÄƒn agent sáº¡c/xáº£ liÃªn tá»¥c khÃ´ng cáº§n thiáº¿t, nhÆ°ng khÃ´ng lá»›n Ä‘áº¿n má»©c ngÄƒn cáº£n sá»­ dá»¥ng pin khi cáº§n.

**ThÃ nh pháº§n R_bonus = +0.5** Ä‘Æ°á»£c cá»™ng khi agent thÃ nh cÃ´ng trÃ¡nh mua lÆ°á»›i trong peak hours (17h-21h). Bonus nÃ y táº¡o incentive bá»• sung cho chiáº¿n lÆ°á»£c "sáº¡c trÆ°á»›c peak, xáº£ trong peak" mÃ  chÃºng ta mong muá»‘n agent há»c Ä‘Æ°á»£c.

### 2.5 Transition Dynamics vÃ  Äiá»u Kiá»‡n Káº¿t ThÃºc

Äá»™ng lá»±c chuyá»ƒn Ä‘á»•i tráº¡ng thÃ¡i káº¿t há»£p cáº£ yáº¿u tá»‘ deterministic vÃ  stochastic. Má»©c pin Ä‘Æ°á»£c cáº­p nháº­t theo cÃ´ng thá»©c váº­t lÃ½: `B_{t+1} = clip(B_t + charge Ã— 0.95 - discharge, 0, capacity)`, trong Ä‘Ã³ hiá»‡u suáº¥t 95% mÃ´ phá»ng tá»•n tháº¥t nÄƒng lÆ°á»£ng thá»±c táº¿ khi sáº¡c/xáº£. Demand vÃ  renewable generation Ä‘Æ°á»£c mÃ´ phá»ng stochastic vá»›i pattern theo giá» cá»™ng nhiá»…u ngáº«u nhiÃªn, pháº£n Ã¡nh tÃ­nh báº¥t Ä‘á»‹nh cá»§a nhu cáº§u vÃ  thá»i tiáº¿t trong thá»±c táº¿.

Episode káº¿t thÃºc khi hoÃ n thÃ nh 24 giá» váº­n hÃ nh (1 ngÃ y), hoáº·c khi pin cáº¡n kiá»‡t Ä‘á»“ng thá»i unmet demand vÆ°á»£t 50% (failure case).

---

## PHáº¦N 3: THUáº¬T TOÃN RL VÃ€ IMPLEMENTATION (25%)

> **[Chá»n Má»˜T trong hai má»¥c 3.A hoáº·c 3.B, xÃ³a má»¥c cÃ²n láº¡i]**

### 3.A â€” DQN (Deep Q-Network)

#### LÃ½ Do Chá»n DQN

Deep Q-Network (Mnih et al., 2015) Ä‘Æ°á»£c chá»n lÃ m thuáº­t toÃ¡n chÃ­nh vÃ¬ sá»± phÃ¹ há»£p tá»± nhiÃªn vá»›i Ä‘áº·c Ä‘iá»ƒm bÃ i toÃ¡n. State space liÃªn tá»¥c 8 chiá»u Ä‘Ã²i há»i function approximation mÃ  neural network cÃ³ thá»ƒ cung cáº¥p, trong khi action space discrete 5 actions chÃ­nh xÃ¡c lÃ  loáº¡i bÃ i toÃ¡n DQN Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t. Hai ká»¹ thuáº­t then chá»‘t cá»§a DQN â€” experience replay buffer cho phÃ©p tÃ¡i sá»­ dá»¥ng má»—i transition nhiá»u láº§n (sample efficient), vÃ  target network giÃºp á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh training báº±ng cÃ¡ch cung cáº¥p target cá»‘ Ä‘á»‹nh trong 1000 steps â€” Ä‘áº·c biá»‡t quan trá»ng cho bÃ i toÃ¡n microgrid nÆ¡i reward signal cÃ³ thá»ƒ noisy do tÃ­nh stochastic cá»§a demand vÃ  weather.

#### Kiáº¿n TrÃºc Neural Network

Máº¡ng Q-Network sá»­ dá»¥ng kiáº¿n trÃºc Multi-Layer Perceptron (MLP) gá»“m 3 hidden layers vá»›i kÃ­ch thÆ°á»›c [256, 256, 128] neurons. Input layer nháº­n state vector 8 chiá»u, output layer cÃ³ 5 neurons tÆ°Æ¡ng á»©ng vá»›i Q-value cá»§a 5 actions. Activation function ReLU (f(x) = max(0, x)) Ä‘Æ°á»£c sá»­ dá»¥ng giá»¯a cÃ¡c hidden layers vÃ¬ tÃ­nh Ä‘Æ¡n giáº£n vÃ  kháº£ nÄƒng trÃ¡nh vanishing gradient. Dropout 0.1 Ä‘Æ°á»£c thÃªm vÃ o má»—i layer Ä‘á»ƒ regularization, giáº£m nguy cÆ¡ overfitting. Output layer khÃ´ng cÃ³ activation function vÃ¬ Q-values cÃ³ thá»ƒ nháº­n giÃ¡ trá»‹ Ã¢m hoáº·c dÆ°Æ¡ng. Weights Ä‘Æ°á»£c khá»Ÿi táº¡o báº±ng Xavier initialization Ä‘á»ƒ Ä‘áº£m báº£o gradients á»•n Ä‘á»‹nh ngay tá»« Ä‘áº§u training.

#### CÃ¡c ThÃ nh Pháº§n ChÃ­nh vÃ  CÃ´ng Thá»©c

**Experience Replay:** Má»—i transition (s, a, r, s', done) Ä‘Æ°á»£c lÆ°u vÃ o buffer cÃ³ capacity 100,000. Khi training, batch 64 samples Ä‘Æ°á»£c random sample tá»« buffer, phÃ¡ vá»¡ temporal correlation giá»¯a cÃ¡c samples liÃªn tiáº¿p vÃ  cho phÃ©p má»—i transition Ä‘Æ°á»£c há»c nhiá»u láº§n.

**Target Network:** Hai máº¡ng riÃªng biá»‡t Ä‘Æ°á»£c sá»­ dá»¥ng â€” Q-network (update má»—i step) vÃ  Target network (copy weights tá»« Q-network má»—i 1000 steps). CÃ´ng thá»©c cáº­p nháº­t: `y = r + Î³ Ã— max Q_target(s', a')`, `L = (Q(s,a) - y)Â²`. Ká»¹ thuáº­t Double DQN Ä‘Æ°á»£c Ã¡p dá»¥ng thÃªm: chá»n action báº±ng online network nhÆ°ng Ä‘Ã¡nh giÃ¡ báº±ng target network, giáº£m overestimation bias.

**Epsilon-Greedy:** Exploration strategy báº¯t Ä‘áº§u vá»›i Îµ = 1.0 (100% random), giáº£m dáº§n theo há»‡ sá»‘ 0.995 má»—i episode Ä‘áº¿n Îµ_min = 0.01 (1% random). Ban Ä‘áº§u agent khÃ¡m phÃ¡ toÃ n bá»™ action space, dáº§n dáº§n chuyá»ƒn sang exploit policy Ä‘Ã£ há»c.

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): DQN hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?**
>
> HÃ£y tÆ°á»Ÿng tÆ°á»£ng DQN giá»‘ng nhÆ° má»™t ngÆ°á»i chÆ¡i cá» vua há»c báº±ng cÃ¡ch ghi nhá»› "GiÃ¡ trá»‹" cá»§a tá»«ng nÆ°á»›c Ä‘i.
>
> - **Q-Value (GiÃ¡ trá»‹):** Vá»›i má»—i tháº¿ cá» (tráº¡ng thÃ¡i), DQN cá»‘ gáº¯ng Æ°á»›c lÆ°á»£ng xem Ä‘i nÆ°á»›c nÃ o thÃ¬ cuá»‘i vÃ¡n sáº½ tháº¯ng to nháº¥t.
> - **Experience Replay (Ã”n bÃ i):** Thay vÃ¬ chÆ¡i xong quÃªn luÃ´n, DQN ghi láº¡i má»i vÃ¡n Ä‘áº¥u vÃ o má»™t cuá»‘n "nháº­t kÃ½". Tá»‘i vá», nÃ³ láº¥y ngáº«u nhiÃªn cÃ¡c trang nháº­t kÃ½ ra Ä‘á»c láº¡i Ä‘á»ƒ rÃºt kinh nghiá»‡m. Viá»‡c nÃ y giÃºp nÃ³ khÃ´ng bá»‹ "há»c váº¹t" theo trÃ¬nh tá»± vÃ¡n Ä‘áº¥u mÃ  hiá»ƒu sÃ¢u báº£n cháº¥t váº¥n Ä‘á».
> - **Exploration (KhÃ¡m phÃ¡):** Ban Ä‘áº§u, nÃ³ Ä‘Ã¡nh lung tung (random) Ä‘á»ƒ biáº¿t tháº¿ nÃ o lÃ  hay, tháº¿ nÃ o lÃ  dá»Ÿ. Sau khi "khÃ´n" ra rá»“i, nÃ³ má»›i báº¯t Ä‘áº§u Ä‘Ã¡nh theo bÃ i báº£n (exploit) nhÆ°ng thá»‰nh thoáº£ng váº«n thá»­ nÆ°á»›c láº¡ Ä‘á»ƒ xem cÃ³ gÃ¬ má»›i máº» khÃ´ng.

---

### 3.B â€” PPO (Proximal Policy Optimization)

#### LÃ½ Do Chá»n PPO

Proximal Policy Optimization (Schulman et al., 2017) Ä‘Æ°á»£c chá»n vÃ¬ thuá»™c nhÃ³m Policy Gradient â€” má»™t hÆ°á»›ng tiáº¿p cáº­n khÃ¡c biá»‡t hoÃ n toÃ n so vá»›i DQN. Thay vÃ¬ há»c Q-value rá»“i rÃºt ra policy giÃ¡n tiáº¿p, PPO **trá»±c tiáº¿p tá»‘i Æ°u hÃ³a policy** Ï€(a|s). Kiáº¿n trÃºc Actor-Critic káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a cáº£ policy gradient (Actor há»c policy) vÃ  value-based (Critic Æ°á»›c lÆ°á»£ng value), giÃºp giáº£m variance trong gradient estimation. Äáº·c biá»‡t, clipped surrogate objective â€” sÃ¡ng táº¡o cá»‘t lÃµi cá»§a PPO â€” ngÄƒn policy thay Ä‘á»•i quÃ¡ lá»›n trong má»—i update, táº¡o ra quÃ¡ trÃ¬nh training á»•n Ä‘á»‹nh Ä‘Ã¡ng ká»ƒ mÃ  khÃ´ng cáº§n tuning phá»©c táº¡p.

#### Kiáº¿n TrÃºc Actor-Critic

Máº¡ng Actor-Critic sá»­ dá»¥ng shared feature extractor gá»“m 2 hidden layers [128, 128] vá»›i Tanh activation (Ä‘Æ°á»£c Æ°a chuá»™ng hÆ¡n ReLU cho policy gradient vÃ¬ output bounded [-1,1], giÃºp training á»•n Ä‘á»‹nh). Tá»« shared features, hai nhÃ¡nh (heads) Ä‘Æ°á»£c tÃ¡ch ra: **Actor head** gá»“m Linear(128â†’5) + Softmax, output xÃ¡c suáº¥t chá»n má»—i action (Ï€(a|s), tá»•ng = 1); **Critic head** gá»“m Linear(128â†’1) khÃ´ng activation, output scalar V(s) Æ°á»›c lÆ°á»£ng tá»•ng reward ká»³ vá»ng tá»« state s. Orthogonal initialization Ä‘Æ°á»£c sá»­ dá»¥ng thay cho Xavier, theo chuáº©n thá»±c hÃ nh tá»‘t nháº¥t cho actor-critic methods.

#### CÃ¡c ThÃ nh Pháº§n ChÃ­nh vÃ  CÃ´ng Thá»©c

**GAE (Generalized Advantage Estimation):** `A_t = Î£ (Î³Î»)^l Ã— Î´_{t+l}` vá»›i `Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)`. Hyperparameter Î» = 0.95 cÃ¢n báº±ng giá»¯a bias (Î» nhá») vÃ  variance (Î» lá»›n).

**Clipped Objective:** `L = min(ratio Ã— A, clip(ratio, 1-Îµ, 1+Îµ) Ã— A)` vá»›i `ratio = Ï€_new(a|s) / Ï€_old(a|s)`. Clip Îµ = 0.2 nghÄ©a lÃ  policy chá»‰ Ä‘Æ°á»£c thay Ä‘á»•i tá»‘i Ä‘a Â±20% má»—i update.

**Total Loss:** `L_total = L_policy + 0.5 Ã— L_value - 0.01 Ã— Entropy`. Entropy bonus khuyáº¿n khÃ­ch exploration tá»± nhiÃªn mÃ  khÃ´ng cáº§n Îµ-greedy.

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): PPO khÃ¡c gÃ¬ DQN?**
>
> Náº¿u DQN há»c báº±ng cÃ¡ch "nhá»› giÃ¡ trá»‹", thÃ¬ PPO há»c báº±ng cÃ¡ch "tinh chá»‰nh hÃ nh vi" (Policy) giá»‘ng nhÆ° má»™t huáº¥n luyá»‡n viÃªn thá»ƒ thao:
>
> - **Policy Gradient:** Thay vÃ¬ cháº¥m Ä‘iá»ƒm tá»«ng nÆ°á»›c Ä‘i, PPO nhÃ¬n vÃ o káº¿t quáº£ cuá»‘i cÃ¹ng vÃ  nÃ³i: "Tráº­n nÃ y tháº¯ng, nhá»¯ng gÃ¬ em lÃ m nÃ£y giá» lÃ  tá»‘t, hÃ£y lÃ m tháº¿ nhiá»u hÆ¡n má»™t chÃºt".
> - **Clipped Objective (Giá»›i háº¡n thay Ä‘á»•i):** ÄÃ¢y lÃ  Ä‘iá»ƒm hay nháº¥t cá»§a PPO. NÃ³ cáº¥m AI thay Ä‘á»•i chiáº¿n thuáº­t quÃ¡ Ä‘á»™t ngá»™t. Giá»‘ng nhÆ° khi sá»­a dÃ¡ng swing golf: sá»­a tá»« tá»« tá»«ng chÃºt má»™t thÃ¬ sáº½ tiáº¿n bá»™ cháº¯c cháº¯n. Náº¿u sá»­a Ä‘á»•i loáº¡n xáº¡ quÃ¡ nhanh, ngÆ°á»i chÆ¡i sáº½ bá»‹ "táº©u há»a nháº­p ma", há»ng luÃ´n cáº£ nhá»¯ng ká»¹ nÄƒng Ä‘Ã£ há»c Ä‘Æ°á»£c trÆ°á»›c Ä‘Ã³. ChÃ­nh vÃ¬ sá»± "bÃ¬nh tÄ©nh" nÃ y mÃ  PPO ráº¥t Ä‘Æ°á»£c Æ°a chuá»™ng vÃ¬ tÃ­nh á»•n Ä‘á»‹nh cao.

---

## PHáº¦N 4: PHÃ‚N TÃCH Tá»I Æ¯U HÃ“A AI (15%)

### 4.1 Chiáº¿n LÆ°á»£c NÄƒng LÆ°á»£ng ÄÃ£ Há»c

Sau quÃ¡ trÃ¬nh training, agent Ä‘Ã£ tá»± phÃ¡t hiá»‡n vÃ  há»c Ä‘Æ°á»£c má»™t chiáº¿n lÆ°á»£c quáº£n lÃ½ nÄƒng lÆ°á»£ng tinh vi, pháº£n Ã¡nh sá»± hiá»ƒu biáº¿t sÃ¢u sáº¯c vá» dynamics cá»§a há»‡ thá»‘ng microgrid. VÃ o ban Ä‘Ãªm (0h-6h), khi solar khÃ´ng phÃ¡t Ä‘iá»‡n nhÆ°ng wind cÃ³ thá»ƒ máº¡nh vÃ  giÃ¡ lÆ°á»›i tháº¥p nháº¥t trong ngÃ y, agent chá»§ yáº¿u chá»n káº¿t há»£p renewable vá»›i grid â€” táº­n dá»¥ng giÃ¡ ráº» vÃ  báº£o toÃ n pin cho cÃ¡c giá» quan trá»ng hÆ¡n. Sang buá»•i sÃ¡ng (7h-9h), khi solar báº¯t Ä‘áº§u phÃ¡t vÃ  nhu cáº§u tÄƒng, agent chuyá»ƒn sang renewable káº¿t há»£p xáº£ pin â€” trÃ¡nh mua lÆ°á»›i Ä‘ang báº¯t Ä‘áº§u tÄƒng giÃ¡. ÄÃ¢y lÃ  hÃ nh vi thÃ´ng minh: agent "biáº¿t" ráº±ng solar sáº¯p Ä‘áº¡t Ä‘á»‰nh nÃªn xáº£ pin bÃ¢y giá» lÃ  há»£p lÃ½ vÃ¬ sáº½ sáº¡c láº¡i Ä‘Æ°á»£c.

Giai Ä‘oáº¡n then chá»‘t nháº¥t lÃ  buá»•i trÆ°a (10h-14h) khi solar á»Ÿ Ä‘á»‰nh. Agent nháº¥t quÃ¡n chá»n sáº¡c pin â€” lÆ°u trá»¯ nÄƒng lÆ°á»£ng "miá»…n phÃ­" dÆ° thá»«a Ä‘á»ƒ dÃ¹ng vÃ o buá»•i tá»‘i. ÄÃ¢y lÃ  minh chá»©ng rÃµ rÃ ng cho kháº£ nÄƒng láº­p káº¿ hoáº¡ch dÃ i háº¡n: agent hy sinh reward tá»©c thÃ¬ (cÃ³ thá»ƒ dÃ¹ng solar ngay) Ä‘á»ƒ Ä‘áº¡t tá»•ng reward cao hÆ¡n trong ngÃ y. Battery level thÆ°á»ng Ä‘áº¡t gáº§n 100% vÃ o khoáº£ng 14h.

Buá»•i tá»‘i (18h-21h) â€” peak hours vá»›i giÃ¡ lÆ°á»›i cao nháº¥t â€” agent tá»‘i Ä‘a xáº£ pin Ä‘Ã£ sáº¡c Ä‘áº§y tá»« trÆ°a, káº¿t há»£p vá»›i wind energy náº¿u cÃ³. Káº¿t quáº£: agent háº§u nhÆ° khÃ´ng mua lÆ°á»›i trong peak hours, tiáº¿t kiá»‡m chi phÃ­ Ä‘Ã¡ng ká»ƒ. Battery level giáº£m dáº§n tá»« ~100% xuá»‘ng ~20-30% vÃ o cuá»‘i ngÃ y.

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): "Chiáº¿n thuáº­t con buÃ´n" cá»§a AI**
>
> Sau quÃ¡ trÃ¬nh tá»± luyá»‡n táº­p, AI Ä‘Ã£ trá»Ÿ thÃ nh má»™t nhÃ  buÃ´n nÄƒng lÆ°á»£ng lÃ£o luyá»‡n vá»›i chiáº¿n thuáº­t "mua Ä‘Ã¡y bÃ¡n Ä‘á»‰nh":
>
> 1. **SÃ¡ng sá»›m & ÄÃªm:** GiÃ¡ Ä‘iá»‡n ráº», giÃ³ láº¡i nhiá»u -> DÃ¹ng Ä‘iá»‡n giÃ³, thiáº¿u thÃ¬ mua chÃºt Ä‘iá»‡n lÆ°á»›i. Giá»¯ pin Ä‘Ã³, Ä‘á»«ng Ä‘á»™ng vÃ o.
> 2. **TrÆ°a náº¯ng (Äá»‰nh Ä‘iá»ƒm):** Äiá»‡n máº·t trá»i dÆ° thá»«a Ãª há» -> Thay vÃ¬ bá» phÃ­, hÃ£y nhÃ©t háº¿t vÃ o pin (Sáº¡c Ä‘áº§y). ÄÃ¢y lÃ  lÃºc tÃ­ch trá»¯ "hÃ ng".
> 3. **Chiá»u tá»‘i (Cao Ä‘iá»ƒm):** GiÃ¡ Ä‘iá»‡n lÆ°á»›i tÄƒng vá»t -> Tuyá»‡t Ä‘á»‘i khÃ´ng mua! Láº¥y kho hÃ ng (pin) Ä‘Ã£ tÃ­ch trá»¯ lÃºc trÆ°a ra dÃ¹ng.
>
> Káº¿t quáº£ lÃ  hÃ³a Ä‘Æ¡n tiá»n Ä‘iá»‡n giáº£m háº³n vÃ¬ AI toÃ n dÃ¹ng Ä‘á»“ "cá»§a nhÃ  trá»“ng Ä‘Æ°á»£c" vÃ o lÃºc ngÆ°á»i khÃ¡c pháº£i Ä‘i mua giÃ¡ Ä‘áº¯t.

### 4.2 PhÃ¢n TÃ­ch Trade-offs

Agent Ä‘Ã£ há»c cÃ¡ch cÃ¢n báº±ng ba cáº·p trade-off chÃ­nh. Äá»‘i vá»›i **chi phÃ­ vs renewable usage**, agent Æ°u tiÃªn renewable ngay cáº£ khi mua lÆ°á»›i off-peak ráº» hÆ¡n, vÃ¬ tá»•ng reward dÃ i háº¡n (bao gá»“m renewable bonus) cao hÆ¡n. Äá»‘i vá»›i **hao mÃ²n pin vs giÃ¡ trá»‹ lÆ°u trá»¯**, agent chá»‰ sáº¡c/xáº£ khi thá»±c sá»± cáº§n thiáº¿t â€” khÃ´ng switching liÃªn tá»¥c â€” cho tháº¥y Ä‘Ã£ "hiá»ƒu" battery wear penalty. Äá»‘i vá»›i **reward ngáº¯n háº¡n vs dÃ i háº¡n**, vá»›i Î³ = 0.99, agent nhÃ¬n xa ~100 steps, Ä‘á»§ Ä‘á»ƒ lÃªn káº¿ hoáº¡ch cho cáº£ ngÃ y 24 giá».

### 4.3 PhÃ¢n TÃ­ch Há»™i Tá»¥

> [DÃ¡n training log vÃ  phÃ¢n tÃ­ch tá»‘c Ä‘á»™ convergence, variance, stability]

### 4.4 Háº¡n Cháº¿

Approach hiá»‡n táº¡i cÃ³ má»™t sá»‘ háº¡n cháº¿ cáº§n lÆ°u Ã½. Discrete action space (5 actions) khÃ´ng cho phÃ©p control chÃ­nh xÃ¡c lÆ°á»£ng kW sáº¡c/xáº£ â€” cÃ³ thá»ƒ giáº£i quyáº¿t báº±ng DDPG/SAC cho continuous action space. Training trÃªn single-day episodes khÃ´ng capture seasonal patterns (mÃ¹a hÃ¨ solar nhiá»u hÆ¡n mÃ¹a Ä‘Ã´ng). Agent chá»‰ quan sÃ¡t demand hiá»‡n táº¡i mÃ  khÃ´ng cÃ³ kháº£ nÄƒng dá»± bÃ¡o â€” tÃ­ch há»£p LSTM forecasting cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ.

---

## PHáº¦N 5: Káº¾T QUáº¢ VÃ€ ÄÃNH GIÃ (15%)

### 5.1 Performance Metrics

> [ChÃ¨n báº£ng káº¿t quáº£: Trained Agent vs Random Baseline vá»›i improvement %]

### 5.2 Biá»ƒu Äá»“ vÃ  PhÃ¢n TÃ­ch

> [ChÃ¨n training_curves.png, comparison.png, episode_analysis.png]

> [PhÃ¢n tÃ­ch chi tiáº¿t: tá»‘c Ä‘á»™ há»™i tá»¥, patterns Ä‘Ã£ há»c, so sÃ¡nh vá»›i baseline]

### 5.3 Tháº£o Luáº­n

Agent Ä‘Ã£ chá»©ng minh kháº£ nÄƒng há»c policy hiá»‡u quáº£, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i random baseline trÃªn táº¥t cáº£ metrics. Tá»· lá»‡ renewable usage cao cho tháº¥y reward shaping Ä‘Ã£ thÃ nh cÃ´ng hÆ°á»›ng dáº«n agent Æ°u tiÃªn nÄƒng lÆ°á»£ng sáº¡ch. Chi phÃ­ giáº£m máº¡nh nhá» chiáº¿n lÆ°á»£c "sáº¡c trÆ°a, xáº£ tá»‘i" â€” táº­n dá»¥ng tá»‘i Ä‘a chÃªnh lá»‡ch giÃ¡ peak/off-peak.

---

## PHáº¦N 6: XEM XÃ‰T Äáº O Äá»¨C, THá»°C TIá»„N VÃ€ TÆ¯Æ NG LAI (10%)

### 6.1 Váº¥n Äá» Äáº¡o Äá»©c

Viá»‡c triá»ƒn khai AI tá»± Ä‘á»™ng trong quáº£n lÃ½ nÄƒng lÆ°á»£ng Ä‘áº·t ra nhiá»u cÃ¢u há»i Ä‘áº¡o Ä‘á»©c cáº§n Ä‘Æ°á»£c xem xÃ©t nghiÃªm tÃºc. Váº¥n Ä‘á» **cÃ´ng báº±ng nÄƒng lÆ°á»£ng** lÃ  má»‘i quan tÃ¢m hÃ ng Ä‘áº§u: má»™t thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a chi phÃ­ tá»•ng thá»ƒ cÃ³ thá»ƒ vÃ´ tÃ¬nh Æ°u tiÃªn cung cáº¥p Ä‘iá»‡n cho khu vá»±c giÃ u hÆ¡n (nÆ¡i margin lá»£i nhuáº­n cao) trong khi cáº¯t giáº£m cung cáº¥p cho cá»™ng Ä‘á»“ng yáº¿u tháº¿. Äá»ƒ giáº£m thiá»ƒu rá»§i ro nÃ y, hÃ m reward cáº§n tÃ­ch há»£p constraint Ä‘áº£m báº£o má»©c demand satisfaction tá»‘i thiá»ƒu cho má»i Ä‘á»‘i tÆ°á»£ng, báº¥t ká»ƒ hiá»‡u quáº£ kinh táº¿.

Váº¥n Ä‘á» **tá»± Ä‘á»™ng hÃ³a vÃ  rá»§i ro** cÅ©ng cáº§n Ä‘Æ°á»£c cÃ¢n nháº¯c ká»¹ lÆ°á»¡ng. Má»™t há»‡ thá»‘ng AI hoÃ n toÃ n tá»± Ä‘á»™ng, khÃ´ng cÃ³ giÃ¡m sÃ¡t con ngÆ°á»i, cÃ³ thá»ƒ gÃ¢y ra háº­u quáº£ nghiÃªm trá»ng náº¿u Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh sai â€” vÃ­ dá»¥, xáº£ háº¿t pin trÆ°á»›c peak hours do Ä‘Ã¡nh giÃ¡ sai demand. Giáº£i phÃ¡p thá»±c tiá»…n lÃ  triá»ƒn khai cÆ¡ cháº¿ "human-in-the-loop": AI Ä‘á» xuáº¥t quyáº¿t Ä‘á»‹nh nhÆ°ng operator cÃ³ quyá»n override, kÃ¨m theo alert system khi confidence tháº¥p hoáº·c behavior báº¥t thÆ°á»ng.

Váº¥n Ä‘á» **báº£o máº­t dá»¯ liá»‡u** phÃ¡t sinh tá»« viá»‡c thu tháº­p dá»¯ liá»‡u tiÃªu thá»¥ Ä‘iá»‡n chi tiáº¿t theo giá» â€” thÃ´ng tin nÃ y cÃ³ thá»ƒ tiáº¿t lá»™ thÃ³i quen sinh hoáº¡t, lá»‹ch trÃ¬nh váº¯ng nhÃ , tháº­m chÃ­ tÃ¬nh tráº¡ng sá»©c khá»e cá»§a cÆ° dÃ¢n. Ãp dá»¥ng differential privacy trong training, sá»­ dá»¥ng aggregate data thay vÃ¬ individual, vÃ  tuÃ¢n thá»§ GDPR lÃ  nhá»¯ng biá»‡n phÃ¡p cáº§n thiáº¿t.

### 6.2 ThÃ¡ch Thá»©c Triá»ƒn Khai Thá»±c Táº¿

Khoáº£ng cÃ¡ch giá»¯a simulation vÃ  thá»±c táº¿ (sim-to-real gap) lÃ  thÃ¡ch thá»©c lá»›n nháº¥t. MÃ´i trÆ°á»ng mÃ´ phá»ng sá»­ dá»¥ng mÃ´ hÃ¬nh probabilistic Ä‘Æ¡n giáº£n cho demand vÃ  weather, trong khi thá»±c táº¿ phá»©c táº¡p hÆ¡n nhiá»u â€” bao gá»“m extreme events, thiáº¿t bá»‹ xuá»‘ng cáº¥p, vÃ  thay Ä‘á»•i hÃ nh vi ngÆ°á»i dÃ¹ng. Äá»ƒ thu háº¹p gap nÃ y, cáº§n domain randomization trong training (thay Ä‘á»•i parameters ngáº«u nhiÃªn), continuous learning sau deployment, vÃ  fallback to rule-based khi agent gáº·p tÃ¬nh huá»‘ng ngoÃ i phÃ¢n phá»‘i training.

YÃªu cáº§u real-time (response < 1 giÃ¢y) cÅ©ng lÃ  thÃ¡ch thá»©c, Ä‘áº·c biá»‡t trÃªn edge devices vá»›i compute power háº¡n cháº¿. Model compression (quantization, pruning) vÃ  edge deployment lÃ  hÆ°á»›ng giáº£i quyáº¿t. NgoÃ i ra, redundant sensors vÃ  anomaly detection cáº§n Ä‘Æ°á»£c triá»ƒn khai Ä‘á»ƒ Ä‘áº£m báº£o state observation chÃ­nh xÃ¡c.

### 6.3 HÆ°á»›ng PhÃ¡t Triá»ƒn TÆ°Æ¡ng Lai

CÃ³ nhiá»u hÆ°á»›ng phÃ¡t triá»ƒn há»©a háº¹n cho nghiÃªn cá»©u tiáº¿p theo. **Multi-Agent RL** cho phÃ©p nhiá»u microgrid há»£p tÃ¡c â€” chia sáº» nÄƒng lÆ°á»£ng dÆ°, tá»‘i Æ°u pricing qua game theory. **Demand forecasting integration** (LSTM/Transformer) giÃºp agent "nhÃ¬n trÆ°á»›c" 24 giá», chá»§ Ä‘á»™ng lÃªn káº¿ hoáº¡ch thay vÃ¬ reactive. **Continuous action space** (DDPG/SAC) cho phÃ©p control chÃ­nh xÃ¡c kW sáº¡c/xáº£, tá»‘i Æ°u hÆ¡n 5 cháº¿ Ä‘á»™ rá»i ráº¡c hiá»‡n táº¡i. **Transfer learning** â€” train trÃªn 1 microgrid, fine-tune nhanh cho cÃ¡c microgrid khÃ¡c â€” giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ deployment. Cuá»‘i cÃ¹ng, **Safe RL** vá»›i constrained optimization Ä‘áº£m báº£o agent khÃ´ng bao giá» vi pháº¡m rÃ ng buá»™c an toÃ n váº­t lÃ½ cá»§a há»‡ thá»‘ng.

### 6.4 Káº¿t Luáº­n

Dá»± Ã¡n Ä‘Ã£ chá»©ng minh Deep Reinforcement Learning lÃ  cÃ´ng cá»¥ máº¡nh máº½ vÃ  phÃ¹ há»£p cho bÃ i toÃ¡n tá»‘i Æ°u hÃ³a phÃ¢n phá»‘i nÄƒng lÆ°á»£ng trong microgrid. Agent Ä‘Ã£ tá»± há»c Ä‘Æ°á»£c chiáº¿n lÆ°á»£c quáº£n lÃ½ nÄƒng lÆ°á»£ng thÃ´ng minh â€” sáº¡c pin khi solar Ä‘á»‰nh, xáº£ pin khi peak price, Æ°u tiÃªn renewable â€” mÃ  khÃ´ng cáº§n láº­p trÃ¬nh tÆ°á»ng minh. Káº¿t quáº£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i baseline trÃªn má»i metrics. Tuy nhiÃªn, viá»‡c Ä‘Æ°a vÃ o váº­n hÃ nh thá»±c táº¿ Ä‘Ã²i há»i xem xÃ©t toÃ n diá»‡n cÃ¡c yáº¿u tá»‘ Ä‘áº¡o Ä‘á»©c, an toÃ n, vÃ  kháº£ nÄƒng má»Ÿ rá»™ng cá»§a há»‡ thá»‘ng.

---

## TÃ€I LIá»†U THAM KHáº¢O

1. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." *Nature*, 518(7540), 529-533.
2. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." *arXiv preprint arXiv:1707.06347*.
3. Van Hasselt, H., et al. (2016). "Deep Reinforcement Learning with Double Q-learning." *AAAI*.
4. Schulman, J., et al. (2015). "High-Dimensional Continuous Control Using Generalized Advantage Estimation." *ICLR*.
5. Vazquez-Canteli, J. R., & Nagy, Z. (2019). "Reinforcement learning for demand response: A review." *Applied Energy*.
6. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
7. FranÃ§ois-Lavet, V., et al. (2018). "An Introduction to Deep Reinforcement Learning." *Foundations and Trends in Machine Learning*.

---

## PHá»¤ Lá»¤C

### A. Source Code

ToÃ n bá»™ source code Ä‘Æ°á»£c cung cáº¥p trong cÃ¡c file notebook vÃ  script Python Ä‘i kÃ¨m, bao gá»“m cáº£ phiÃªn báº£n DQN vÃ  PPO.

### B. Cáº¥u HÃ¬nh ÄÃ£ Sá»­ Dá»¥ng

> [Liá»‡t kÃª Ä‘áº§y Ä‘á»§ CONFIG: SEED, EPISODES, LR, GAMMA, ...]

### C. Training Logs

> [DÃ¡n output terminal / training log Ä‘áº§y Ä‘á»§]

---

*BÃ i tiá»ƒu luáº­n hoÃ n thÃ nh ngÃ y [DD/MM/YYYY]*
