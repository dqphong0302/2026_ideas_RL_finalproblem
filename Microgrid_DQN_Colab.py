# -*- coding: utf-8 -*-
"""
================================================================================
üîã MICROGRID ENERGY OPTIMIZATION USING DEEP REINFORCEMENT LEARNING
================================================================================
Google Colab Standalone Script - Run All Cells to Train & Evaluate DQN Agent

This notebook contains:
1. Environment setup and installations
2. Microgrid Environment (Gymnasium-compatible)
3. DQN Agent with Experience Replay and Target Network
4. Training Loop with Logging
5. Evaluation and Visualization
6. Full Execution Pipeline

Author: Deep RL Assignment
Date: February 2026

================================================================================
üìù H∆Ø·ªöNG D·∫™N CHO SINH VI√äN (STUDENT CUSTOMIZATION GUIDE)
================================================================================
File n√†y l√† B√ÄI M·∫™U cho 5-7 sinh vi√™n tham kh·∫£o.
ƒê·ªÉ B√ÄI L√ÄM c·ªßa m·ªói ng∆∞·ªùi KH√ÅC NHAU, h√£y thay ƒë·ªïi c√°c gi√° tr·ªã ƒë∆∞·ª£c ƒë√°nh d·∫•u:
    üîß [CUSTOMIZABLE] - C√≥ th·ªÉ thay ƒë·ªïi s·ªë li·ªáu
    ‚ö†Ô∏è [REQUIRED CHANGE] - B·∫ÆT BU·ªòC thay ƒë·ªïi ƒë·ªÉ tr√°nh tr√πng l·∫∑p

C√°c g·ª£i √Ω thay ƒë·ªïi:
- Thay ƒë·ªïi hyperparameters (learning rate, batch size, hidden layers)
- Thay ƒë·ªïi c·∫•u tr√∫c m·∫°ng neural (s·ªë layers, neurons)
- Thay ƒë·ªïi reward weights
- Thay ƒë·ªïi seed ƒë·ªÉ c√≥ k·∫øt qu·∫£ kh√°c
- Thay ƒë·ªïi s·ªë episodes training
================================================================================
"""

#@title 1Ô∏è‚É£ Install Dependencies & Imports
!pip install gymnasium torch numpy matplotlib -q

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import json
import os

# Check GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üñ•Ô∏è Using device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

#@title 2Ô∏è‚É£ Configuration Parameters
"""
================================================================================
‚öôÔ∏è HYPERPARAMETERS - C·∫§U H√åNH THAM S·ªê
================================================================================
üîß [CUSTOMIZABLE] - Thay ƒë·ªïi c√°c gi√° tr·ªã n√†y ƒë·ªÉ t·∫°o b√†i l√†m kh√°c bi·ªát!

G·ª¢I √ù THAY ƒê·ªîI CHO M·ªñI SINH VI√äN:
- Student 1: seed=42, lr=1e-4, hidden=[256,256,128], episodes=500
- Student 2: seed=123, lr=5e-4, hidden=[512,256], episodes=600
- Student 3: seed=2024, lr=2e-4, hidden=[128,128,64], episodes=400
- Student 4: seed=999, lr=1e-3, hidden=[256,128,64], episodes=550
- Student 5: seed=7777, lr=3e-4, hidden=[384,192,96], episodes=450
================================================================================
"""

CONFIG = {
    # =========================================================================
    # üîã ENVIRONMENT PARAMETERS - Tham s·ªë m√¥i tr∆∞·ªùng Microgrid
    # =========================================================================
    
    # üîß [CUSTOMIZABLE] Battery capacity in kWh
    # G·ª£i √Ω: 80-150 kWh (thay ƒë·ªïi ¬±20% so v·ªõi 100)
    "battery_capacity": 100.0,
    
    # üîß [CUSTOMIZABLE] Battery round-trip efficiency
    # G·ª£i √Ω: 0.90-0.98 (pin lithium th∆∞·ªùng 0.92-0.96)
    "battery_efficiency": 0.95,
    
    # üîß [CUSTOMIZABLE] Maximum charge/discharge rate in kW
    # G·ª£i √Ω: 15-30 kW
    "max_charge_rate": 20.0,
    "max_discharge_rate": 20.0,
    
    # üîß [CUSTOMIZABLE] Renewable energy capacity
    # G·ª£i √Ω: Solar 40-70 kW, Wind 20-50 kW
    "max_solar": 50.0,
    "max_wind": 30.0,
    
    # üîß [CUSTOMIZABLE] Consumer demand parameters
    # G·ª£i √Ω: base 30-60 kW, std 5-15 kW
    "base_demand": 40.0,
    "demand_std": 10.0,
    
    # üîß [CUSTOMIZABLE] Grid electricity price ($/kWh)
    # G·ª£i √Ω: min 0.03-0.08, max 0.20-0.35
    "grid_price_min": 0.05,
    "grid_price_max": 0.25,
    
    # Hours per episode (1 day = 24 hours)
    "hours_per_episode": 24,
    
    # =========================================================================
    # üß† DQN PARAMETERS - Tham s·ªë thu·∫≠t to√°n DQN
    # =========================================================================
    
    # State and Action dimensions (KH√îNG THAY ƒê·ªîI)
    "state_dim": 8,
    "action_dim": 5,
    
    # ‚ö†Ô∏è [REQUIRED CHANGE] Neural network architecture
    # M·ªói sinh vi√™n PH·∫¢I thay ƒë·ªïi c·∫•u tr√∫c n√†y!
    # G·ª£i √Ω: [128,128,64], [256,128], [512,256,128], [384,192,96]
    "hidden_dims": [256, 256, 128],
    
    # ‚ö†Ô∏è [REQUIRED CHANGE] Learning rate
    # G·ª£i √Ω: 1e-4, 2e-4, 5e-4, 1e-3, 3e-4
    "learning_rate": 1e-4,
    
    # üîß [CUSTOMIZABLE] Discount factor (gamma)
    # G·ª£i √Ω: 0.95-0.99
    "gamma": 0.99,
    
    # üîß [CUSTOMIZABLE] Epsilon-greedy exploration
    # G·ª£i √Ω: start 0.9-1.0, end 0.01-0.05, decay 0.990-0.998
    "epsilon_start": 1.0,
    "epsilon_end": 0.01,
    "epsilon_decay": 0.995,
    
    # üîß [CUSTOMIZABLE] Batch size for training
    # G·ª£i √Ω: 32, 64, 128, 256
    "batch_size": 64,
    
    # üîß [CUSTOMIZABLE] Replay buffer size
    # G·ª£i √Ω: 50000-200000
    "buffer_size": 100000,
    
    # üîß [CUSTOMIZABLE] Target network update frequency
    # G·ª£i √Ω: 500-2000 steps
    "target_update_freq": 1000,
    
    # =========================================================================
    # üìä TRAINING PARAMETERS - Tham s·ªë hu·∫•n luy·ªán
    # =========================================================================
    
    # ‚ö†Ô∏è [REQUIRED CHANGE] Number of training episodes
    # M·ªói sinh vi√™n n√™n d√πng s·ªë kh√°c nhau: 400-700
    "num_episodes": 500,
    
    # Steps per episode (= hours per day)
    "max_steps_per_episode": 24,
    
    # Checkpoint save frequency
    "save_freq": 100,
    
    # Logging frequency
    "log_freq": 10,
    
    # ‚ö†Ô∏è [REQUIRED CHANGE] Random seed - M·ªñI SINH VI√äN PH·∫¢I KH√ÅC!
    # G·ª£i √Ω: 42, 123, 456, 789, 999, 2024, 7777
    "seed": 42,
    
    # =========================================================================
    # üéØ REWARD SHAPING - Tr·ªçng s·ªë ph·∫ßn th∆∞·ªüng
    # =========================================================================
    # üîß [CUSTOMIZABLE] Reward weights - Thay ƒë·ªïi ƒë·ªÉ ∆∞u ti√™n m·ª•c ti√™u kh√°c nhau
    
    # Positive reward for using renewable energy
    # G·ª£i √Ω: 0.5-2.0
    "reward_renewable": 1.0,
    
    # Negative penalty for grid electricity purchase
    # G·ª£i √Ω: -1.0 ƒë·∫øn -3.0
    "reward_grid_penalty": -2.0,
    
    # Heavy penalty for unmet demand
    # G·ª£i √Ω: -3.0 ƒë·∫øn -10.0
    "reward_unmet_penalty": -5.0,
    
    # Small penalty for battery wear (cycling)
    # G·ª£i √Ω: -0.05 ƒë·∫øn -0.2
    "reward_battery_wear": -0.1,
    
    # Bonus for avoiding grid during peak hours
    # G·ª£i √Ω: 0.3-1.0
    "reward_peak_bonus": 0.5,
    
    # =========================================================================
    # üõë EPISODE TERMINATION CONDITIONS - ƒêi·ªÅu ki·ªán k·∫øt th√∫c episode
    # =========================================================================
    # üîß [CUSTOMIZABLE] Th√™m ƒëi·ªÅu ki·ªán k·∫øt th√∫c s·ªõm
    
    # Critical low battery threshold (fraction of capacity)
    # N·∫øu pin < threshold, episode k·∫øt th√∫c. G·ª£i √Ω: 0.05-0.15
    "battery_critical_low": 0.05,
    
    # Critical high battery threshold (fraction of capacity)
    # N·∫øu pin > threshold, k·∫øt th√∫c. G·ª£i √Ω: 0.95-1.0
    "battery_critical_high": 1.0,
    
    # Maximum cumulative unmet demand ratio before termination
    # G·ª£i √Ω: 0.15-0.30
    "max_unmet_ratio": 0.20,
}

print("‚úÖ Configuration loaded!")
print(f"   Episodes: {CONFIG['num_episodes']}")
print(f"   Learning Rate: {CONFIG['learning_rate']}")
print(f"   Gamma: {CONFIG['gamma']}")
print(f"   Hidden Layers: {CONFIG['hidden_dims']}")
print(f"   Seed: {CONFIG['seed']}")

#@title 3Ô∏è‚É£ Microgrid Environment
"""
================================================================================
üîã MICROGRID ENVIRONMENT - M√¥i tr∆∞·ªùng m√¥ ph·ªèng l∆∞·ªõi ƒëi·ªán si√™u nh·ªè
================================================================================

Gymnasium-compatible environment simulating:
- Solar and wind renewable generation (ph√°t ƒëi·ªán t√°i t·∫°o)
- Battery storage with efficiency losses (l∆∞u tr·ªØ pin v·ªõi t·ªïn hao)
- Grid connection with time-varying prices (k·∫øt n·ªëi l∆∞·ªõi v·ªõi gi√° bi·∫øn ƒë·ªïi)
- Consumer demand with stochastic variations (nhu c·∫ßu ng·∫´u nhi√™n)

State Space (8D continuous):
    [battery_level, demand, solar, wind, grid_price, hour_sin, hour_cos, prev_action]

Action Space (5 discrete):
    0: Discharge battery (X·∫£ pin)
    1: Charge from renewable (S·∫°c t·ª´ nƒÉng l∆∞·ª£ng t√°i t·∫°o)
    2: Buy from grid (Mua t·ª´ l∆∞·ªõi ƒëi·ªán)
    3: Renewable + Discharge (K·∫øt h·ª£p t√°i t·∫°o + x·∫£ pin)
    4: Renewable + Grid (K·∫øt h·ª£p t√°i t·∫°o + l∆∞·ªõi)
================================================================================
"""

class MicrogridEnv:
    """
    Microgrid Energy Management Environment
    
    M√¥i tr∆∞·ªùng qu·∫£n l√Ω nƒÉng l∆∞·ª£ng l∆∞·ªõi ƒëi·ªán si√™u nh·ªè, m√¥ ph·ªèng:
    - Ph√°t ƒëi·ªán m·∫∑t tr·ªùi (solar) theo th·ªùi gian trong ng√†y
    - Ph√°t ƒëi·ªán gi√≥ (wind) v·ªõi bi·∫øn ƒë·ªông ng·∫´u nhi√™n
    - Pin l∆∞u tr·ªØ v·ªõi hi·ªáu su·∫•t round-trip
    - Nhu c·∫ßu ti√™u th·ª• v·ªõi peak s√°ng v√† t·ªëi
    - Gi√° ƒëi·ªán l∆∞·ªõi bi·∫øn ƒë·ªïi theo gi·ªù
    """
    
    def __init__(self, config: Dict):
        """
        Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng v·ªõi c·∫•u h√¨nh.
        
        Args:
            config: Dictionary ch·ª©a t·∫•t c·∫£ tham s·ªë c·∫•u h√¨nh
        """
        self.config = config
        
        # üîã Battery parameters - Tham s·ªë pin
        self.battery_capacity = config["battery_capacity"]  # kWh
        self.battery_efficiency = config["battery_efficiency"]  # Hi·ªáu su·∫•t
        self.max_charge = config["max_charge_rate"]  # kW - t·ªëc ƒë·ªô s·∫°c t·ªëi ƒëa
        self.max_discharge = config["max_discharge_rate"]  # kW - t·ªëc ƒë·ªô x·∫£ t·ªëi ƒëa
        
        # ‚òÄÔ∏è Renewable energy parameters - Tham s·ªë nƒÉng l∆∞·ª£ng t√°i t·∫°o
        self.max_solar = config["max_solar"]  # kW peak solar
        self.max_wind = config["max_wind"]  # kW peak wind
        
        # üè† Demand parameters - Tham s·ªë nhu c·∫ßu ti√™u th·ª•
        self.base_demand = config["base_demand"]  # kW trung b√¨nh
        self.demand_std = config["demand_std"]  # ƒê·ªô l·ªách chu·∫©n
        
        # üí∞ Grid price parameters - Tham s·ªë gi√° ƒëi·ªán l∆∞·ªõi
        self.grid_price_min = config["grid_price_min"]  # $/kWh off-peak
        self.grid_price_max = config["grid_price_max"]  # $/kWh peak
        
        # ‚è∞ Time parameters
        self.hours_per_episode = config["hours_per_episode"]
        
        # üéØ Reward weights - Tr·ªçng s·ªë ph·∫ßn th∆∞·ªüng
        self.r_renewable = config["reward_renewable"]
        self.r_grid = config["reward_grid_penalty"]
        self.r_unmet = config["reward_unmet_penalty"]
        self.r_wear = config["reward_battery_wear"]
        self.r_bonus = config["reward_peak_bonus"]
        
        # üõë Termination conditions - ƒêi·ªÅu ki·ªán k·∫øt th√∫c
        self.battery_critical_low = config.get("battery_critical_low", 0.05)
        self.battery_critical_high = config.get("battery_critical_high", 1.0)
        self.max_unmet_ratio = config.get("max_unmet_ratio", 0.20)
        
        # State tracking
        self.reset()
    
    def reset(self, seed: Optional[int] = None) -> np.ndarray:
        """
        Reset environment to initial state.
        ƒê·∫∑t l·∫°i m√¥i tr∆∞·ªùng v·ªÅ tr·∫°ng th√°i ban ƒë·∫ßu.
        
        Args:
            seed: Random seed for reproducibility
            
        Returns:
            Initial observation vector (8D)
        """
        if seed is not None:
            np.random.seed(seed)
        
        # üîß [CUSTOMIZABLE] Initial battery level
        # G·ª£i √Ω: Thay ƒë·ªïi m·ª©c pin ban ƒë·∫ßu (0.3-0.7 c·ªßa capacity)
        self.battery_level = self.battery_capacity * 0.5  # Start at 50%
        self.current_hour = 0
        self.prev_action = 0
        
        # Episode tracking - Theo d√µi episode
        self.total_demand = 0.0
        self.total_renewable_used = 0.0
        self.total_grid_cost = 0.0
        self.total_unmet = 0.0
        self.episode_history = []
        
        return self._get_obs()
    
    def _get_obs(self) -> np.ndarray:
        """
        Generate normalized observation vector.
        T·∫°o vector quan s√°t ƒë√£ chu·∫©n h√≥a.
        
        Observation vector g·ªìm 8 th√†nh ph·∫ßn:
        - battery_level: M·ª©c pin hi·ªán t·∫°i [0,1]
        - demand: Nhu c·∫ßu ti√™u th·ª• [0,1]
        - solar: S·∫£n l∆∞·ª£ng ƒëi·ªán m·∫∑t tr·ªùi [0,1]
        - wind: S·∫£n l∆∞·ª£ng ƒëi·ªán gi√≥ [0,1]
        - grid_price: Gi√° ƒëi·ªán l∆∞·ªõi [0,1]
        - hour_sin: Sin c·ªßa gi·ªù (cyclic encoding)
        - hour_cos: Cos c·ªßa gi·ªù (cyclic encoding)
        - prev_action: H√†nh ƒë·ªông tr∆∞·ªõc ƒë√≥ [0,1]
        
        Returns:
            Normalized observation array (8D)
        """
        demand = self._get_demand(self.current_hour)
        solar = self._get_solar(self.current_hour)
        wind = self._get_wind(self.current_hour)
        price = self._get_price(self.current_hour)
        
        # Normalize all features to [0, 1]
        obs = np.array([
            self.battery_level / self.battery_capacity,
            demand / (self.base_demand * 2),
            solar / self.max_solar,
            wind / self.max_wind,
            (price - self.grid_price_min) / (self.grid_price_max - self.grid_price_min),
            (np.sin(2 * np.pi * self.current_hour / 24) + 1) / 2,
            (np.cos(2 * np.pi * self.current_hour / 24) + 1) / 2,
            self.prev_action / 4.0,
        ], dtype=np.float32)
        
        return np.clip(obs, 0.0, 1.0)
    
    def _get_demand(self, hour: int) -> float:
        """
        Generate stochastic demand with morning and evening peaks.
        T·∫°o nhu c·∫ßu ng·∫´u nhi√™n v·ªõi ƒë·ªânh s√°ng v√† t·ªëi.
        
        üîß [CUSTOMIZABLE] C√≥ th·ªÉ thay ƒë·ªïi:
        - Gi·ªù peak s√°ng (m·∫∑c ƒë·ªãnh: 8h)
        - Gi·ªù peak t·ªëi (m·∫∑c ƒë·ªãnh: 19h)
        - C∆∞·ªùng ƒë·ªô peak (0.3 v√† 0.4)
        
        Args:
            hour: Current hour (0-23)
            
        Returns:
            Demand in kW
        """
        # üîß [CUSTOMIZABLE] Peak hours - Thay ƒë·ªïi gi·ªù cao ƒëi·ªÉm
        morning_peak_hour = 8  # G·ª£i √Ω: 7-9
        evening_peak_hour = 19  # G·ª£i √Ω: 18-21
        
        morning_peak = np.exp(-((hour - morning_peak_hour) ** 2) / 8)
        evening_peak = np.exp(-((hour - evening_peak_hour) ** 2) / 8)
        
        # üîß [CUSTOMIZABLE] Peak intensity - Thay ƒë·ªïi c∆∞·ªùng ƒë·ªô peak
        base = self.base_demand * (0.5 + 0.3 * morning_peak + 0.4 * evening_peak)
        noise = np.random.normal(0, self.demand_std * 0.3)
        
        return max(0, base + noise)
    
    def _get_solar(self, hour: int) -> float:
        """
        Generate solar power based on time of day.
        T·∫°o s·∫£n l∆∞·ª£ng ƒëi·ªán m·∫∑t tr·ªùi theo th·ªùi gian trong ng√†y.
        
        üîß [CUSTOMIZABLE] C√≥ th·ªÉ thay ƒë·ªïi:
        - Gi·ªù m·∫∑t tr·ªùi m·ªçc/l·∫∑n (m·∫∑c ƒë·ªãnh: 6-18h)
        - Bi·∫øn ƒë·ªông th·ªùi ti·∫øt (0.8-1.2)
        
        Args:
            hour: Current hour (0-23)
            
        Returns:
            Solar power in kW
        """
        # üîß [CUSTOMIZABLE] Sunrise/sunset hours
        sunrise = 6  # G·ª£i √Ω: 5-7
        sunset = 18  # G·ª£i √Ω: 17-19
        
        if sunrise <= hour <= sunset:
            base = self.max_solar * np.sin(np.pi * (hour - sunrise) / (sunset - sunrise))
            # üîß [CUSTOMIZABLE] Weather variability
            noise = np.random.uniform(0.8, 1.2)  # G·ª£i √Ω: 0.7-1.3
            return max(0, base * noise)
        return 0.0
    
    def _get_wind(self, hour: int) -> float:
        """
        Generate stochastic wind power.
        T·∫°o s·∫£n l∆∞·ª£ng ƒëi·ªán gi√≥ ng·∫´u nhi√™n.
        
        üîß [CUSTOMIZABLE] C√≥ th·ªÉ thay ƒë·ªïi:
        - Base wind level (0.5)
        - Variation amplitude
        - Noise range (0.5-1.5)
        
        Args:
            hour: Current hour (0-23)
            
        Returns:
            Wind power in kW
        """
        # üîß [CUSTOMIZABLE] Wind pattern
        base_level = 0.5  # G·ª£i √Ω: 0.3-0.6
        base = self.max_wind * base_level
        variation = self.max_wind * (1 - base_level) * np.sin(np.pi * hour / 12)
        
        # üîß [CUSTOMIZABLE] Wind variability
        noise = np.random.uniform(0.5, 1.5)  # G·ª£i √Ω: 0.4-1.6
        
        return max(0, (base + variation) * noise)
    
    def _get_price(self, hour: int) -> float:
        """
        Generate time-varying grid electricity price.
        T·∫°o gi√° ƒëi·ªán l∆∞·ªõi bi·∫øn ƒë·ªïi theo th·ªùi gian.
        
        üîß [CUSTOMIZABLE] C√≥ th·ªÉ thay ƒë·ªïi:
        - Peak hours (7-9, 18-21)
        - Off-peak hours (22-6)
        - Price variation (0.9-1.1)
        
        Args:
            hour: Current hour (0-23)
            
        Returns:
            Price in $/kWh
        """
        # üîß [CUSTOMIZABLE] Peak/off-peak hours
        if 7 <= hour <= 9 or 18 <= hour <= 21:  # Peak hours
            base = self.grid_price_max
        elif 22 <= hour or hour <= 6:  # Off-peak
            base = self.grid_price_min
        else:  # Mid-peak
            base = (self.grid_price_min + self.grid_price_max) / 2
        
        # üîß [CUSTOMIZABLE] Price variability
        noise = np.random.uniform(0.9, 1.1)  # G·ª£i √Ω: 0.85-1.15
        
        return base * noise
    
    def _check_termination(self) -> Tuple[bool, str]:
        """
        Check if episode should terminate early.
        Ki·ªÉm tra ƒëi·ªÅu ki·ªán k·∫øt th√∫c s·ªõm episode.
        
        Returns:
            (should_terminate, reason)
        """
        # Check end of day
        if self.current_hour >= self.hours_per_episode:
            return True, "end_of_day"
        
        # Check critical battery level
        battery_ratio = self.battery_level / self.battery_capacity
        if battery_ratio < self.battery_critical_low:
            return True, "battery_critical_low"
        if battery_ratio > self.battery_critical_high:
            return True, "battery_critical_high"
        
        # Check cumulative unmet demand ratio
        if self.total_demand > 0:
            unmet_ratio = self.total_unmet / self.total_demand
            if unmet_ratio > self.max_unmet_ratio:
                return True, "max_unmet_exceeded"
        
        return False, ""
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        Execute one time step.
        Th·ª±c hi·ªán m·ªôt b∆∞·ªõc th·ªùi gian (1 gi·ªù).
        
        Args:
            action: Integer action (0-4)
            
        Returns:
            (next_state, reward, done, info)
        """
        demand = self._get_demand(self.current_hour)
        solar = self._get_solar(self.current_hour)
        wind = self._get_wind(self.current_hour)
        price = self._get_price(self.current_hour)
        renewable = solar + wind
        
        # Initialize energy flows
        renewable_used = 0.0
        grid_purchased = 0.0
        battery_charge = 0.0
        battery_discharge = 0.0
        unmet_demand = 0.0
        
        # Process action - X·ª≠ l√Ω h√†nh ƒë·ªông
        if action == 0:  # Discharge battery - X·∫£ pin
            discharge = min(self.battery_level, self.max_discharge, demand)
            battery_discharge = discharge
            self.battery_level -= discharge
            remaining = demand - discharge * self.battery_efficiency
            if remaining > 0:
                unmet_demand = remaining
        
        elif action == 1:  # Charge from renewable - S·∫°c t·ª´ t√°i t·∫°o
            supply = min(renewable, demand)
            renewable_used = supply
            remaining = demand - supply
            if remaining > 0:
                unmet_demand = remaining
            excess = renewable - supply
            if excess > 0:
                charge = min(excess, self.max_charge, 
                           self.battery_capacity - self.battery_level)
                battery_charge = charge
                self.battery_level += charge * self.battery_efficiency
        
        elif action == 2:  # Buy from grid - Mua t·ª´ l∆∞·ªõi
            grid_purchased = demand
        
        elif action == 3:  # Renewable + Discharge - T√°i t·∫°o + X·∫£ pin
            renewable_used = min(renewable, demand)
            remaining = demand - renewable_used
            if remaining > 0:
                discharge = min(self.battery_level, self.max_discharge, remaining)
                battery_discharge = discharge
                self.battery_level -= discharge
                remaining -= discharge * self.battery_efficiency
            if remaining > 0:
                unmet_demand = remaining
        
        elif action == 4:  # Renewable + Grid - T√°i t·∫°o + L∆∞·ªõi
            renewable_used = min(renewable, demand)
            remaining = demand - renewable_used
            if remaining > 0:
                grid_purchased = remaining
        
        # Calculate reward - T√≠nh ph·∫ßn th∆∞·ªüng
        normalized_price = (price - self.grid_price_min) / (self.grid_price_max - self.grid_price_min)
        is_peak = 18 <= self.current_hour <= 21
        
        reward = (
            self.r_renewable * (renewable_used / self.base_demand) +
            self.r_grid * (grid_purchased / self.base_demand) * normalized_price +
            self.r_unmet * (unmet_demand / self.base_demand) +
            self.r_wear * ((battery_charge + battery_discharge) / self.max_charge)
        )
        
        # Bonus for avoiding grid during peak
        if is_peak and grid_purchased == 0:
            reward += self.r_bonus
        
        # Update tracking
        self.total_demand += demand
        self.total_renewable_used += renewable_used
        self.total_grid_cost += grid_purchased * price
        self.total_unmet += unmet_demand
        
        # Store history
        self.episode_history.append({
            "hour": self.current_hour,
            "demand": demand,
            "solar": solar,
            "wind": wind,
            "price": price,
            "action": action,
            "renewable_used": renewable_used,
            "grid_purchased": grid_purchased,
            "battery_level": self.battery_level,
            "reward": reward,
        })
        
        # Advance time
        self.current_hour += 1
        self.prev_action = action
        
        # Check termination
        done, termination_reason = self._check_termination()
        
        info = {
            "total_cost": self.total_grid_cost,
            "renewable_ratio": self.total_renewable_used / max(1, self.total_demand),
            "unmet_ratio": self.total_unmet / max(1, self.total_demand),
            "termination_reason": termination_reason,
        }
        
        return self._get_obs(), reward, done, info

print("‚úÖ MicrogridEnv class defined!")

#@title 4Ô∏è‚É£ Neural Network & Replay Buffer
"""
================================================================================
üß† DQN COMPONENTS - C√°c th√†nh ph·∫ßn DQN
================================================================================
- Q-Network: Multi-layer perceptron v·ªõi ReLU activations
- Replay Buffer: Uniform sampling for experience replay
================================================================================
"""

class QNetwork(nn.Module):
    """
    Deep Q-Network for value function approximation.
    M·∫°ng Q s√¢u ƒë·ªÉ x·∫•p x·ªâ h√†m gi√° tr·ªã.
    
    üîß [CUSTOMIZABLE] C√≥ th·ªÉ thay ƒë·ªïi:
    - S·ªë layers v√† neurons (trong CONFIG['hidden_dims'])
    - Dropout rate
    - Activation functions
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int]):
        """
        Kh·ªüi t·∫°o m·∫°ng Q.
        
        Args:
            state_dim: Dimension of state space (8)
            action_dim: Number of actions (5)
            hidden_dims: List of hidden layer sizes, e.g., [256, 256, 128]
        """
        super().__init__()
        
        layers = []
        prev_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            # üîß [CUSTOMIZABLE] Dropout rate
            # G·ª£i √Ω: 0.0-0.3
            layers.append(nn.Dropout(0.1))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, action_dim))
        
        self.network = nn.Sequential(*layers)
        self._init_weights()
    
    def _init_weights(self):
        """
        Xavier initialization for stable training.
        Kh·ªüi t·∫°o Xavier ƒë·ªÉ hu·∫•n luy·ªán ·ªïn ƒë·ªãnh.
        
        üîß [CUSTOMIZABLE] C√≥ th·ªÉ thay ƒë·ªïi ph∆∞∆°ng ph√°p init:
        - xavier_uniform_
        - kaiming_uniform_
        - orthogonal_
        """
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass qua m·∫°ng."""
        return self.network(x)


class ReplayBuffer:
    """
    Experience replay buffer for DQN training.
    B·ªô ƒë·ªám ph√°t l·∫°i kinh nghi·ªám cho hu·∫•n luy·ªán DQN.
    
    Gi√∫p ph√° v·ª° correlation gi·ªØa c√°c m·∫´u li√™n ti·∫øp.
    """
    
    def __init__(self, capacity: int):
        """
        Kh·ªüi t·∫°o buffer.
        
        Args:
            capacity: Maximum number of transitions to store
        """
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """Th√™m transition v√†o buffer."""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int):
        """
        Sample random batch from buffer.
        L·∫•y m·∫´u ng·∫´u nhi√™n t·ª´ buffer.
        
        Args:
            batch_size: Number of samples
            
        Returns:
            Tuple of (states, actions, rewards, next_states, dones)
        """
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        return (
            torch.FloatTensor(np.array(states)),
            torch.LongTensor(actions),
            torch.FloatTensor(rewards),
            torch.FloatTensor(np.array(next_states)),
            torch.FloatTensor(dones),
        )
    
    def __len__(self):
        """Return current buffer size."""
        return len(self.buffer)

print("‚úÖ QNetwork and ReplayBuffer defined!")

#@title 5Ô∏è‚É£ DQN Agent (with Double DQN)
"""
================================================================================
ü§ñ DOUBLE DQN AGENT - Agent s·ª≠ d·ª•ng thu·∫≠t to√°n Double DQN
================================================================================

C·∫£i ti·∫øn so v·ªõi vanilla DQN:
- Separate target network: M·∫°ng target ri√™ng ƒë·ªÉ ·ªïn ƒë·ªãnh Q-values
- Double DQN: Gi·∫£m overestimation bias
- Epsilon-greedy exploration: Kh√°m ph√° v·ªõi epsilon decay

Reference paper:
- DQN: Mnih et al., 2015 "Human-level control through deep RL"
- Double DQN: Van Hasselt et al., 2016 "Deep RL with Double Q-learning"
================================================================================
"""

class DQNAgent:
    """
    Double DQN Agent for Microgrid Optimization.
    Agent Double DQN cho t·ªëi ∆∞u h√≥a Microgrid.
    """
    
    def __init__(self, config: Dict, device: torch.device):
        """
        Kh·ªüi t·∫°o agent.
        
        Args:
            config: Configuration dictionary
            device: torch.device (cuda/cpu)
        """
        self.config = config
        self.device = device
        
        self.state_dim = config["state_dim"]
        self.action_dim = config["action_dim"]
        self.gamma = config["gamma"]
        self.batch_size = config["batch_size"]
        self.target_update_freq = config["target_update_freq"]
        
        # Epsilon parameters - Tham s·ªë epsilon cho exploration
        self.epsilon = config["epsilon_start"]
        self.epsilon_end = config["epsilon_end"]
        self.epsilon_decay = config["epsilon_decay"]
        
        # Networks - M·∫°ng neural
        self.q_network = QNetwork(
            self.state_dim, 
            self.action_dim, 
            config["hidden_dims"]
        ).to(device)
        
        self.target_network = QNetwork(
            self.state_dim, 
            self.action_dim, 
            config["hidden_dims"]
        ).to(device)
        
        # Copy weights to target network
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # üîß [CUSTOMIZABLE] Optimizer
        # G·ª£i √Ω thay ƒë·ªïi: SGD, RMSprop, AdamW
        self.optimizer = optim.Adam(
            self.q_network.parameters(), 
            lr=config["learning_rate"]
        )
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer(config["buffer_size"])
        
        # Training tracking
        self.training_step = 0
        self.losses = []
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        Epsilon-greedy action selection.
        Ch·ªçn h√†nh ƒë·ªông theo chi·∫øn l∆∞·ª£c epsilon-greedy.
        
        Args:
            state: Current state observation
            training: Whether in training mode (use exploration)
            
        Returns:
            Selected action (0-4)
        """
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
            return q_values.argmax(dim=1).item()
    
    def store_transition(self, state, action, reward, next_state, done):
        """
        Store transition in replay buffer.
        L∆∞u transition v√†o replay buffer.
        """
        self.replay_buffer.push(state, action, reward, next_state, float(done))
    
    def update(self) -> Optional[float]:
        """
        Perform one gradient update step.
        Th·ª±c hi·ªán m·ªôt b∆∞·ªõc c·∫≠p nh·∫≠t gradient.
        
        Returns:
            Loss value or None if buffer too small
        """
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # Sample batch
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        
        # Current Q values
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Double DQN: use online network to select action, target network to evaluate
        # Double DQN: d√πng m·∫°ng online ch·ªçn action, m·∫°ng target ƒë√°nh gi√°
        with torch.no_grad():
            next_actions = self.q_network(next_states).argmax(dim=1, keepdim=True)
            next_q = self.target_network(next_states).gather(1, next_actions).squeeze(1)
            target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # üîß [CUSTOMIZABLE] Loss function
        # G·ª£i √Ω thay ƒë·ªïi: MSELoss, SmoothL1Loss (Huber)
        loss = nn.SmoothL1Loss()(current_q, target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        
        # üîß [CUSTOMIZABLE] Gradient clipping
        # G·ª£i √Ω: 1.0-20.0
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10.0)
        self.optimizer.step()
        
        # Update target network periodically
        self.training_step += 1
        if self.training_step % self.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        loss_value = loss.item()
        self.losses.append(loss_value)
        return loss_value
    
    def decay_epsilon(self):
        """
        Decay exploration rate.
        Gi·∫£m t·ª∑ l·ªá kh√°m ph√°.
        """
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
    
    def save(self, path: str):
        """
        Save model weights.
        L∆∞u tr·ªçng s·ªë model.
        """
        torch.save({
            "q_network": self.q_network.state_dict(),
            "target_network": self.target_network.state_dict(),
            "optimizer": self.optimizer.state_dict(),
            "epsilon": self.epsilon,
            "training_step": self.training_step,
        }, path)
    
    def load(self, path: str):
        """
        Load model weights.
        T·∫£i tr·ªçng s·ªë model.
        """
        checkpoint = torch.load(path, map_location=self.device)
        self.q_network.load_state_dict(checkpoint["q_network"])
        self.target_network.load_state_dict(checkpoint["target_network"])
        self.optimizer.load_state_dict(checkpoint["optimizer"])
        self.epsilon = checkpoint["epsilon"]
        self.training_step = checkpoint["training_step"]

print("‚úÖ DQNAgent class defined!")

#@title 6Ô∏è‚É£ Training Function
"""
================================================================================
üöÄ TRAINING LOOP - V√≤ng l·∫∑p hu·∫•n luy·ªán
================================================================================

Implements the main training procedure with:
- Episode-based learning
- Periodic logging
- Best model checkpointing
================================================================================
"""

def train(config: Dict, device: torch.device) -> Dict[str, List]:
    """
    Train the DQN agent.
    Hu·∫•n luy·ªán agent DQN.
    
    Args:
        config: Configuration dictionary
        device: torch.device
        
    Returns:
        Training history dictionary
    """
    
    print("=" * 60)
    print("üöÄ STARTING TRAINING")
    print("=" * 60)
    
    # Set random seeds for reproducibility
    seed = config["seed"]
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # Initialize environment and agent
    env = MicrogridEnv(config)
    agent = DQNAgent(config, device)
    
    # Training history
    history = {
        "rewards": [],
        "costs": [],
        "renewable_ratios": [],
        "epsilons": [],
        "losses": [],
    }
    
    best_reward = float("-inf")
    
    # Training loop
    for episode in range(config["num_episodes"]):
        state = env.reset(seed=seed + episode)
        episode_reward = 0
        episode_losses = []
        
        for step in range(config["max_steps_per_episode"]):
            # Select and execute action
            action = agent.select_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            # Store and learn
            agent.store_transition(state, action, reward, next_state, done)
            loss = agent.update()
            
            if loss is not None:
                episode_losses.append(loss)
            
            state = next_state
            episode_reward += reward
            
            if done:
                break
        
        # Decay epsilon
        agent.decay_epsilon()
        
        # Store history
        history["rewards"].append(episode_reward)
        history["costs"].append(info["total_cost"])
        history["renewable_ratios"].append(info["renewable_ratio"])
        history["epsilons"].append(agent.epsilon)
        if episode_losses:
            history["losses"].append(np.mean(episode_losses))
        
        # Track best
        if episode_reward > best_reward:
            best_reward = episode_reward
            agent.save("best_model.pt")
        
        # Logging
        if (episode + 1) % config["log_freq"] == 0:
            print(f"Episode {episode+1:4d} | "
                  f"Reward: {episode_reward:7.2f} | "
                  f"Cost: ${info['total_cost']:6.2f} | "
                  f"Renewable: {info['renewable_ratio']*100:5.1f}% | "
                  f"Œµ: {agent.epsilon:.3f}")
    
    # Save final model
    agent.save("final_model.pt")
    
    print("=" * 60)
    print(f"‚úÖ Training complete! Best reward: {best_reward:.2f}")
    print("=" * 60)
    
    return history, agent, env

#@title 7Ô∏è‚É£ Evaluation Functions
"""
================================================================================
üìä EVALUATION - ƒê√°nh gi√° model
================================================================================

Functions for:
- Testing trained agent
- Comparing with random baseline
- Generating visualizations
================================================================================
"""

def evaluate_agent(agent: DQNAgent, env: MicrogridEnv, 
                   num_episodes: int = 20, seed: int = 42) -> Dict:
    """
    Evaluate trained agent.
    ƒê√°nh gi√° agent ƒë√£ hu·∫•n luy·ªán.
    """
    
    rewards = []
    costs = []
    renewable_ratios = []
    unmet_ratios = []
    
    for ep in range(num_episodes):
        state = env.reset(seed=seed + ep)
        episode_reward = 0
        
        while True:
            action = agent.select_action(state, training=False)
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            state = next_state
            if done:
                break
        
        rewards.append(episode_reward)
        costs.append(info["total_cost"])
        renewable_ratios.append(info["renewable_ratio"])
        unmet_ratios.append(info["unmet_ratio"])
    
    return {
        "mean_reward": np.mean(rewards),
        "std_reward": np.std(rewards),
        "mean_cost": np.mean(costs),
        "mean_renewable": np.mean(renewable_ratios),
        "mean_unmet": np.mean(unmet_ratios),
    }


def evaluate_random(env: MicrogridEnv, num_episodes: int = 20, seed: int = 42) -> Dict:
    """
    Evaluate random baseline.
    ƒê√°nh gi√° baseline ng·∫´u nhi√™n.
    """
    
    rewards = []
    costs = []
    renewable_ratios = []
    unmet_ratios = []
    
    for ep in range(num_episodes):
        state = env.reset(seed=seed + ep)
        episode_reward = 0
        
        while True:
            action = random.randint(0, 4)
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            state = next_state
            if done:
                break
        
        rewards.append(episode_reward)
        costs.append(info["total_cost"])
        renewable_ratios.append(info["renewable_ratio"])
        unmet_ratios.append(info["unmet_ratio"])
    
    return {
        "mean_reward": np.mean(rewards),
        "std_reward": np.std(rewards),
        "mean_cost": np.mean(costs),
        "mean_renewable": np.mean(renewable_ratios),
        "mean_unmet": np.mean(unmet_ratios),
    }


def plot_results(history: Dict):
    """
    Generate training visualization plots.
    T·∫°o bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a training.
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle("üîã DQN Training Results for Microgrid Optimization", fontsize=14, fontweight='bold')
    
    # Reward curve
    ax = axes[0, 0]
    rewards = history["rewards"]
    ax.plot(rewards, alpha=0.4, label="Raw")
    if len(rewards) >= 20:
        smoothed = np.convolve(rewards, np.ones(20)/20, mode='valid')
        ax.plot(range(19, len(rewards)), smoothed, linewidth=2, label="Smoothed (MA20)")
    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel("Episode")
    ax.set_ylabel("Episode Reward")
    ax.set_title("Reward Curve")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Cost curve
    ax = axes[0, 1]
    costs = history["costs"]
    ax.plot(costs, alpha=0.4, color='orange', label="Raw")
    if len(costs) >= 20:
        smoothed = np.convolve(costs, np.ones(20)/20, mode='valid')
        ax.plot(range(19, len(costs)), smoothed, linewidth=2, color='red', label="Smoothed")
    ax.set_xlabel("Episode")
    ax.set_ylabel("Daily Grid Cost ($)")
    ax.set_title("Grid Electricity Cost")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Renewable ratio
    ax = axes[1, 0]
    ratios = [r * 100 for r in history["renewable_ratios"]]
    ax.plot(ratios, alpha=0.4, color='green', label="Raw")
    if len(ratios) >= 20:
        smoothed = np.convolve(ratios, np.ones(20)/20, mode='valid')
        ax.plot(range(19, len(ratios)), smoothed, linewidth=2, color='darkgreen', label="Smoothed")
    ax.set_xlabel("Episode")
    ax.set_ylabel("Renewable Usage (%)")
    ax.set_title("Renewable Energy Utilization")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Epsilon decay
    ax = axes[1, 1]
    ax.plot(history["epsilons"], color='purple', linewidth=2)
    ax.set_xlabel("Episode")
    ax.set_ylabel("Epsilon (Œµ)")
    ax.set_title("Exploration Rate Decay")
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("training_curves.png", dpi=150, bbox_inches='tight')
    plt.show()
    print("üìä Saved: training_curves.png")


def plot_episode_analysis(env: MicrogridEnv):
    """
    Analyze single episode behavior.
    Ph√¢n t√≠ch h√†nh vi trong m·ªôt episode.
    """
    
    if not env.episode_history:
        print("No episode history to plot!")
        return
    
    history = env.episode_history
    hours = [h["hour"] for h in history]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle("üîã 24-Hour Episode Analysis", fontsize=14, fontweight='bold')
    
    # Energy balance
    ax = axes[0, 0]
    ax.bar(hours, [h["demand"] for h in history], alpha=0.5, label="Demand", color='red')
    ax.bar(hours, [h["solar"] for h in history], alpha=0.7, label="Solar", color='gold')
    ax.bar(hours, [h["wind"] for h in history], alpha=0.7, bottom=[h["solar"] for h in history], 
           label="Wind", color='skyblue')
    ax.set_xlabel("Hour")
    ax.set_ylabel("Power (kW)")
    ax.set_title("Energy Supply & Demand")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Battery level
    ax = axes[0, 1]
    ax.plot(hours, [h["battery_level"] for h in history], 'b-', linewidth=2, marker='o', markersize=4)
    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5, label="Max Capacity")
    ax.set_xlabel("Hour")
    ax.set_ylabel("Battery Level (kWh)")
    ax.set_title("Battery State of Charge")
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 110)
    
    # Actions taken
    ax = axes[1, 0]
    action_names = ["Discharge", "Charge", "Grid", "Renew+Disch", "Renew+Grid"]
    actions = [h["action"] for h in history]
    colors = ['red', 'green', 'orange', 'blue', 'purple']
    for i, h in enumerate(history):
        ax.bar(h["hour"], 1, bottom=0, color=colors[h["action"]], alpha=0.7)
    ax.set_xlabel("Hour")
    ax.set_ylabel("Action")
    ax.set_title("Actions Taken by Agent")
    ax.set_yticks([])
    # Legend
    handles = [plt.Rectangle((0,0),1,1, color=c, alpha=0.7) for c in colors]
    ax.legend(handles, action_names, loc='upper right', fontsize=8)
    
    # Cumulative reward
    ax = axes[1, 1]
    cumulative = np.cumsum([h["reward"] for h in history])
    ax.plot(hours, cumulative, 'g-', linewidth=2, marker='o', markersize=4)
    ax.fill_between(hours, cumulative, alpha=0.3, color='green')
    ax.set_xlabel("Hour")
    ax.set_ylabel("Cumulative Reward")
    ax.set_title("Reward Accumulation")
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("episode_analysis.png", dpi=150, bbox_inches='tight')
    plt.show()
    print("üìä Saved: episode_analysis.png")

print("‚úÖ Evaluation functions defined!")

#@title 8Ô∏è‚É£ üöÄ RUN TRAINING & EVALUATION
"""
================================================================================
üéØ MAIN EXECUTION - Th·ª±c thi ch√≠nh
================================================================================

Run this cell to train and evaluate the DQN agent!
Ch·∫°y cell n√†y ƒë·ªÉ hu·∫•n luy·ªán v√† ƒë√°nh gi√° agent DQN!
================================================================================
"""

# Train the agent
history, agent, env = train(CONFIG, device)

# Plot training results
plot_results(history)

# Evaluate trained agent
print("\n" + "=" * 60)
print("üìä EVALUATION RESULTS")
print("=" * 60)

agent_metrics = evaluate_agent(agent, env, num_episodes=20)
random_metrics = evaluate_random(env, num_episodes=20)

print(f"\n{'Metric':<25} {'Trained Agent':>15} {'Random Baseline':>15} {'Improvement':>12}")
print("-" * 70)
print(f"{'Mean Reward':<25} {agent_metrics['mean_reward']:>15.2f} {random_metrics['mean_reward']:>15.2f} "
      f"{(agent_metrics['mean_reward'] - random_metrics['mean_reward'])/abs(random_metrics['mean_reward'])*100:>+10.1f}%")
print(f"{'Daily Cost ($)':<25} {agent_metrics['mean_cost']:>15.2f} {random_metrics['mean_cost']:>15.2f} "
      f"{(random_metrics['mean_cost'] - agent_metrics['mean_cost'])/random_metrics['mean_cost']*100:>+10.1f}%")
print(f"{'Renewable Usage (%)':<25} {agent_metrics['mean_renewable']*100:>14.1f}% {random_metrics['mean_renewable']*100:>14.1f}% "
      f"{(agent_metrics['mean_renewable'] - random_metrics['mean_renewable'])*100:>+10.1f}pp")
print(f"{'Unmet Demand (%)':<25} {agent_metrics['mean_unmet']*100:>14.1f}% {random_metrics['mean_unmet']*100:>14.1f}% "
      f"{(random_metrics['mean_unmet'] - agent_metrics['mean_unmet'])*100:>+10.1f}pp")

# Run and analyze a demo episode
print("\n" + "=" * 60)
print("üéÆ DEMO EPISODE")
print("=" * 60)

state = env.reset(seed=999)
total_reward = 0

print(f"\n{'Hour':>5} | {'Action':<15} | {'Demand':>8} | {'Solar':>8} | {'Wind':>8} | {'Battery':>8} | {'Reward':>8}")
print("-" * 80)

action_names = ["Discharge", "Charge", "Grid", "Renew+Disch", "Renew+Grid"]
for step in range(24):
    action = agent.select_action(state, training=False)
    next_state, reward, done, info = env.step(action)
    
    h = env.episode_history[-1]
    print(f"{h['hour']:>5} | {action_names[action]:<15} | {h['demand']:>8.1f} | {h['solar']:>8.1f} | "
          f"{h['wind']:>8.1f} | {h['battery_level']:>8.1f} | {reward:>+8.2f}")
    
    state = next_state
    total_reward += reward
    if done:
        break

print("-" * 80)
print(f"{'TOTAL':>5} | {'':15} | {'':>8} | {'':>8} | {'':>8} | {'':>8} | {total_reward:>+8.2f}")
print(f"\nüìà Total Reward: {total_reward:.2f}")
print(f"üí∞ Grid Cost: ${info['total_cost']:.2f}")
print(f"üå± Renewable Usage: {info['renewable_ratio']*100:.1f}%")

# Plot episode analysis
plot_episode_analysis(env)

print("\n" + "=" * 60)
print("‚úÖ ALL DONE!")
print("=" * 60)
print("\nFiles created:")
print("  üìÅ best_model.pt - Best trained model weights")
print("  üìÅ final_model.pt - Final trained model weights")
print("  üìÅ training_curves.png - Training visualization")
print("  üìÅ episode_analysis.png - 24-hour episode analysis")
