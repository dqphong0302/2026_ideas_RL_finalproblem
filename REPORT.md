# BÃO CÃO Äá»’ ÃN

# Tá»I Æ¯U HÃ“A PHÃ‚N PHá»I NÄ‚NG LÆ¯á»¢NG TRONG MICROGRID Sá»¬ Dá»¤NG DEEP REINFORCEMENT LEARNING

---

## PHáº¦N 1: MÃ” Táº¢ Váº¤N Äá»€ (15%)

### 1.1 Giá»›i Thiá»‡u Há»‡ Thá»‘ng Microgrid

Microgrid lÃ  má»™t lÆ°á»›i Ä‘iá»‡n nhá», cá»¥c bá»™ bao gá»“m:

- **Nguá»“n nÄƒng lÆ°á»£ng tÃ¡i táº¡o**: Solar panels (Ä‘iá»‡n máº·t trá»i) vÃ  wind turbines (tuabin giÃ³)
- **Há»‡ thá»‘ng lÆ°u trá»¯**: Pin lÆ°u trá»¯ nÄƒng lÆ°á»£ng (battery storage)
- **Káº¿t ná»‘i lÆ°á»›i chÃ­nh**: CÃ³ thá»ƒ mua Ä‘iá»‡n tá»« lÆ°á»›i Ä‘iá»‡n quá»‘c gia
- **Táº£i tiÃªu thá»¥**: Nhu cáº§u Ä‘iá»‡n tá»« há»™ gia Ä‘Ã¬nh, cÃ´ng nghiá»‡p, thÆ°Æ¡ng máº¡i

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): BÃ i toÃ¡n "Äi chá»£ thÃ´ng minh"**
>
> HÃ£y tÆ°á»Ÿng tÆ°á»£ng há»‡ thá»‘ng nÃ y giá»‘ng nhÆ° viá»‡c quáº£n lÃ½ báº¿p Äƒn cho má»™t gia Ä‘Ã¬nh lá»›n:
>
> - **Solar & Wind:** NhÆ° rau cá»§ tá»± trá»“ng Ä‘Æ°á»£c. LÃºc Ä‘Æ°á»£c mÃ¹a (náº¯ng/giÃ³ nhiá»u) thÃ¬ tha há»“ dÃ¹ng, lÃºc máº¥t mÃ¹a thÃ¬ chá»‹u. Quan trá»ng lÃ  nÃ³ miá»…n phÃ­!
> - **Pin lÆ°u trá»¯:** NhÆ° cÃ¡i tá»§ láº¡nh. Rau Äƒn khÃ´ng háº¿t thÃ¬ cáº¥t tá»§ láº¡nh (sáº¡c pin), khi nÃ o ngoÃ i vÆ°á»n khÃ´ng cÃ³ rau thÃ¬ láº¥y trong tá»§ ra Äƒn (xáº£ pin).
> - **LÆ°á»›i Ä‘iá»‡n:** NhÆ° Ä‘i siÃªu thá»‹. SiÃªu thá»‹ lÃºc nÃ o cÅ©ng cÃ³ Ä‘á»“, nhÆ°ng giÃ¡ cáº£ thay Ä‘á»•i theo giá» (giá» cao Ä‘iá»ƒm Ä‘áº¯t, tháº¥p Ä‘iá»ƒm ráº»).
>
> **Nhiá»‡m vá»¥ cá»§a AI:** LÃ m sao Ä‘á»ƒ cáº£ nhÃ  luÃ´n no bá»¥ng (Ä‘á»§ Ä‘iá»‡n) mÃ  tá»‘n Ã­t tiá»n Ä‘i siÃªu thá»‹ nháº¥t? AI pháº£i tÃ­nh toÃ¡n: "TrÆ°a nay náº¯ng to, rau Ä‘áº§y vÆ°á»n, Äƒn khÃ´ng háº¿t thÃ¬ cáº¥t tá»§ láº¡nh ngay. Tá»‘i nay rau siÃªu thá»‹ Ä‘áº¯t láº¯m, láº¥y Ä‘á»“ trong tá»§ láº¡nh ra Äƒn chá»© Ä‘á»«ng Ä‘i mua!"

### 1.2 Táº¡i Sao ÄÃ¢y LÃ  BÃ i ToÃ¡n Quyáº¿t Äá»‹nh Tuáº§n Tá»±?

PhÃ¢n phá»‘i nÄƒng lÆ°á»£ng lÃ  bÃ i toÃ¡n **sequential decision-making** vÃ¬:

1. **Quyáº¿t Ä‘á»‹nh hiá»‡n táº¡i áº£nh hÆ°á»Ÿng tÆ°Æ¡ng lai**: Náº¿u sá»­ dá»¥ng háº¿t pin bÃ¢y giá», sáº½ khÃ´ng cÃ³ nÄƒng lÆ°á»£ng dá»± trá»¯ cho peak hours
2. **Tráº¡ng thÃ¡i thay Ä‘á»•i liÃªn tá»¥c**: Má»©c pin, nhu cáº§u, sáº£n lÆ°á»£ng renewable thay Ä‘á»•i má»—i giá»
3. **Chi phÃ­ biáº¿n Ä‘á»•i theo thá»i gian**: GiÃ¡ Ä‘iá»‡n grid cao vÃ o peak hours, tháº¥p vÃ o off-peak

### 1.3 Háº¡n Cháº¿ Cá»§a PhÆ°Æ¡ng PhÃ¡p Truyá»n Thá»‘ng

| PhÆ°Æ¡ng phÃ¡p | Háº¡n cháº¿ |
|-------------|---------|
| **Rule-based scheduling** | KhÃ´ng thÃ­ch á»©ng vá»›i biáº¿n Ä‘á»•i thá»i tiáº¿t, cá»‘ Ä‘á»‹nh khÃ´ng há»c |
| **Linear programming** | Giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh, khÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c uncertainty |
| **Heuristic methods** | KhÃ´ng tá»‘i Æ°u toÃ n cá»¥c, dá»… rÆ¡i vÃ o local optima |

### 1.4 Táº¡i Sao Reinforcement Learning Hiá»‡u Quáº£ HÆ¡n?

RL cÃ³ Æ°u Ä‘iá»ƒm:

- **Adaptive**: Tá»± Ä‘á»™ng há»c tá»« mÃ´i trÆ°á»ng, thÃ­ch á»©ng vá»›i thay Ä‘á»•i
- **Long-term optimization**: Xem xÃ©t háº­u quáº£ dÃ i háº¡n cá»§a quyáº¿t Ä‘á»‹nh
- **Handle uncertainty**: Xá»­ lÃ½ tá»‘t vá»›i stochastic demand vÃ  renewable generation
- **No model required**: KhÃ´ng cáº§n mÃ´ hÃ¬nh chÃ­nh xÃ¡c cá»§a há»‡ thá»‘ng (model-free)

- **No model required**: KhÃ´ng cáº§n mÃ´ hÃ¬nh chÃ­nh xÃ¡c cá»§a há»‡ thá»‘ng (model-free)

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): Táº¡i sao cáº§n AI "há»c" (RL)?**
>
> CÃ¡c phÆ°Æ¡ng phÃ¡p cÅ© giá»‘ng nhÆ° láº­p trÃ¬nh cho robot má»™t bá»™ luáº­t cá»©ng nháº¯c: "Cá»© 6h tá»‘i lÃ  báº­t Ä‘Ã¨n". NhÆ°ng lá»¡ hÃ´m Ä‘Ã³ trá»i tá»‘i sá»›m tá»« 5h thÃ¬ sao? Robot sáº½ khÃ´ng biáº¿t linh hoáº¡t.
>
> **Reinforcement Learning (Há»c tÄƒng cÆ°á»ng)** giá»‘ng nhÆ° cÃ¡ch báº¡n dáº¡y chÃº cÃºn cÆ°ng:
>
> - Báº¡n khÃ´ng giáº£i thÃ­ch váº­t lÃ½ hay logic cho cÃºn.
> - CÃºn lÃ m Ä‘Ãºng (ngá»“i xuá»‘ng khi báº£o) -> Báº¡n cho bÃ¡nh thÆ°á»Ÿng (+Reward).
> - CÃºn lÃ m sai (cáº¯n giÃ y) -> Báº¡n máº¯ng nháº¹ (-Penalty).
> - Sau nhiá»u láº§n, cÃºn tá»± hiá»ƒu: "Ã€, muá»‘n Ä‘Æ°á»£c bÃ¡nh thÃ¬ pháº£i lÃ m tháº¿ nÃ y, muá»‘n khÃ´ng bá»‹ máº¯ng thÃ¬ trÃ¡nh lÃ m tháº¿ kia".
>
> AI trong bÃ i toÃ¡n nÃ y cÅ©ng váº­y: nÃ³ tá»± thá»­ nghiá»‡m hÃ ng triá»‡u láº§n trong giáº£ láº­p Ä‘á»ƒ rÃºt ra kinh nghiá»‡m xÆ°Æ¡ng mÃ¡u vá» cÃ¡ch Ä‘iá»u khiá»ƒn Ä‘iá»‡n, thay vÃ¬ Ä‘Æ°á»£c láº­p trÃ¬nh sáºµn tá»«ng dÃ²ng lá»‡nh if-else.

---

## PHáº¦N 2: MÃ” HÃŒNH HÃ“A MDP (20%)

### 2.1 Markov Decision Process (MDP)

MDP Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi tuple (S, A, P, R, Î³):

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   ENVIRONMENT                        â”‚
         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
         â”‚  â”‚ Battery â”‚    â”‚ Demand  â”‚    â”‚ Weather â”‚          â”‚
         â”‚  â”‚  Level  â”‚    â”‚ Pattern â”‚    â”‚(Solar/  â”‚          â”‚
         â”‚  â”‚  (SoC)  â”‚    â”‚         â”‚    â”‚  Wind)  â”‚          â”‚
         â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚
         â”‚       â”‚              â”‚              â”‚                â”‚
         â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
         â”‚                      â–¼                               â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
         â”‚              â”‚  STATE (s_t)  â”‚                       â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   RL AGENT (DQN)  â”‚
                    â”‚                   â”‚
                    â”‚  Q(s, a) = Neural â”‚
                    â”‚     Network       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ACTION (a_t)    â”‚
                    â”‚ 0: Discharge      â”‚
                    â”‚ 1: Charge         â”‚
                    â”‚ 2: Buy Grid       â”‚
                    â”‚ 3: Renew+Dischargeâ”‚
                    â”‚ 4: Renew+Grid     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                   ENVIRONMENT                        â”‚
         â”‚                                                      â”‚
         â”‚    Execute Action â†’ Update Battery â†’ Calculate      â”‚
         â”‚    Reward â†’ Return New State s_{t+1}                â”‚
         â”‚                                                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      REWARD (r_t)       â”‚
                    â”‚ + Renewable usage       â”‚
                    â”‚ - Grid purchase cost    â”‚
                    â”‚ - Unmet demand penalty  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 State Space (KhÃ´ng Gian Tráº¡ng ThÃ¡i)

**Dimension: 8**

| # | Component | Range | Ã nghÄ©a |
|---|-----------|-------|---------|
| 1 | battery_level | [0, 1] | Má»©c pin hiá»‡n táº¡i (% capacity) |
| 2 | demand | [0, 1] | Nhu cáº§u nÄƒng lÆ°á»£ng (normalized) |
| 3 | solar_generation | [0, 1] | Sáº£n lÆ°á»£ng Ä‘iá»‡n máº·t trá»i |
| 4 | wind_generation | [0, 1] | Sáº£n lÆ°á»£ng Ä‘iá»‡n giÃ³ |
| 5 | grid_price | [0, 1] | GiÃ¡ Ä‘iá»‡n lÆ°á»›i (normalized) |
| 6 | hour_sin | [0, 1] | Sin encoding cá»§a giá» |
| 7 | hour_cos | [0, 1] | Cos encoding cá»§a giá» |
| 8 | prev_action | [0, 1] | HÃ nh Ä‘á»™ng trÆ°á»›c Ä‘Ã³ |

**Giáº£i thÃ­ch Sin/Cos Encoding:**
DÃ¹ng sin/cos Ä‘á»ƒ encode thá»i gian vÃ¬ nÃ³ capture Ä‘Æ°á»£c tÃ­nh chu ká»³ (giá» 23 â†’ 0 gáº§n nhau vá» nghÄ©a).

### 2.3 Action Space (KhÃ´ng Gian HÃ nh Äá»™ng)

**Discrete: 5 actions**

| Action | Name | MÃ´ táº£ |
|--------|------|-------|
| 0 | Discharge | Xáº£ pin Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u |
| 1 | Charge | Sáº¡c pin tá»« renewable dÆ° thá»«a |
| 2 | Buy Grid | Mua Ä‘iá»‡n tá»« lÆ°á»›i chÃ­nh |
| 3 | Renewable + Discharge | Æ¯u tiÃªn renewable, xáº£ pin náº¿u thiáº¿u |
| 4 | Renewable + Grid | Æ¯u tiÃªn renewable, mua grid náº¿u thiáº¿u |

### 2.4 Reward Function (HÃ m ThÆ°á»Ÿng)

```
R(s, a, s') = R_renewable + R_grid + R_unmet + R_battery + R_bonus

Cá»¥ thá»ƒ:
R_renewable = +1.0 Ã— (renewable_used / base_demand)     # ThÆ°á»Ÿng dÃ¹ng renewable
R_grid      = -2.0 Ã— (grid_purchased / base_demand) Ã— normalized_price  # Pháº¡t mua grid
R_unmet     = -5.0 Ã— (unmet_demand / base_demand)       # Pháº¡t náº·ng náº¿u khÃ´ng Ä‘á»§
R_battery   = -0.1 Ã— battery_activity                    # Pháº¡t nháº¹ hao mÃ²n pin
R_bonus     = +0.5 náº¿u khÃ´ng mua grid khi giÃ¡ cao        # Bonus tiáº¿t kiá»‡m
R_battery   = -0.1 Ã— battery_activity                    # Pháº¡t nháº¹ hao mÃ²n pin
R_bonus     = +0.5 náº¿u khÃ´ng mua grid khi giÃ¡ cao        # Bonus tiáº¿t kiá»‡m
```

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): Báº£ng Ä‘iá»ƒm cá»§a AI**
>
> Äá»ƒ AI biáº¿t tháº¿ nÃ o lÃ  "lÃ m tá»‘t", ta táº¡o ra má»™t báº£ng Ä‘iá»ƒm:
>
> - **DÃ¹ng Ä‘iá»‡n máº·t trá»i (+1 Ä‘iá»ƒm):** "Hoan hÃ´! Tiáº¿t kiá»‡m tiá»n vÃ  báº£o vá»‡ mÃ´i trÆ°á»ng."
> - **Mua Ä‘iá»‡n lÆ°á»›i (-2 Ä‘iá»ƒm):** "ChÃª nhÃ©! Tá»‘n tiá»n quÃ¡." (Trá»« náº·ng hÆ¡n náº¿u mua lÃºc giÃ¡ Ä‘áº¯t).
> - **Äá»ƒ máº¥t Ä‘iá»‡n (-5 Ä‘iá»ƒm):** "QUÃ Tá»†! ÄÃ¢y lÃ  lá»—i nghiÃªm trá»ng nháº¥t, khÃ´ng Ä‘Æ°á»£c phÃ©p Ä‘á»ƒ xáº£y ra."
> - **Nghá»‹ch pin liÃªn tá»¥c (-0.1 Ä‘iá»ƒm):** "DÃ¹ng vá»«a thÃ´i, há»ng pin bÃ¢y giá»."
>
> AI sáº½ chÆ¡i "game" nÃ y hÃ ng ngÃ n láº§n vÃ  cá»‘ gáº¯ng Ä‘áº¡t Ä‘iá»ƒm cao nháº¥t cÃ³ thá»ƒ. Tá»± kháº¯c nÃ³ sáº½ há»c Ä‘Æ°á»£c cÃ¡ch: Æ¯u tiÃªn dÃ¹ng Ä‘iá»‡n máº·t trá»i > Háº¡n cháº¿ mua lÆ°á»›i > Tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘á»ƒ máº¥t Ä‘iá»‡n.

**Justification:**

- Unmet demand penalty cao nháº¥t (-5.0) vÃ¬ Ä‘áº£m báº£o reliability lÃ  Æ°u tiÃªn hÃ ng Ä‘áº§u
- Grid purchase penalty (-2.0) khuyáº¿n khÃ­ch dÃ¹ng renewable
- Renewable reward (+1.0) thÃºc Ä‘áº©y sá»­ dá»¥ng nÄƒng lÆ°á»£ng sáº¡ch

### 2.5 Transition Dynamics

**Battery Update:**

```
B_{t+1} = clip(B_t + charge Ã— efficiency - discharge, 0, capacity)
efficiency = 0.95 (5% loss khi sáº¡c/xáº£)
```

**Demand Pattern:**

```
demand(t) = base_demand Ã— (0.5 + 0.3 Ã— morning_peak + 0.4 Ã— evening_peak) + noise
morning_peak = exp(-(t - 8)Â² / 8)   # Peak 8:00 AM
evening_peak = exp(-(t - 19)Â² / 8)  # Peak 7:00 PM
```

**Solar Generation:**

```
solar(t) = max_solar Ã— sin(Ï€ Ã— (t - 6) / 12) Ã— weather_factor, náº¿u 6 â‰¤ t â‰¤ 18
         = 0, náº¿u t < 6 hoáº·c t > 18
```

### 2.6 Episode Termination

Episode káº¿t thÃºc khi:

1. Háº¿t 24 giá» (1 ngÃ y)
2. Pin cáº¡n kiá»‡t VÃ€ unmet demand > 50%

---

## PHáº¦N 3: THUáº¬T TOÃN RL VÃ€ IMPLEMENTATION (25%)

### 3.1 Táº¡i Sao Chá»n DQN?

**So sÃ¡nh cÃ¡c thuáº­t toÃ¡n:**

| Thuáº­t toÃ¡n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p? |
|------------|---------|------------|----------|
| Q-Learning | ÄÆ¡n giáº£n | KhÃ´ng scale vá»›i high-dim state | âŒ |
| **DQN** | Handle continuous state, stable | Cáº§n tuning | âœ… |
| Policy Gradient | Cho continuous action | Variance cao, sample inefficient | âš ï¸ |
| Actor-Critic | Káº¿t há»£p cáº£ hai | Phá»©c táº¡p | âš ï¸ |

**DQN phÃ¹ há»£p vÃ¬:**

- State space liÃªn tá»¥c (8D) â†’ cáº§n function approximation
- Action space discrete (5 actions) â†’ Q-learning hiá»‡u quáº£
- Experience replay giÃºp sample efficient
- Target network giÃºp stable training

### 3.2 Kiáº¿n TrÃºc Neural Network

```
Input Layer    Hidden Layer 1   Hidden Layer 2   Hidden Layer 3   Output Layer
    (8)            (256)            (256)            (128)            (5)
    â—‹              â—‹ â—‹ â—‹            â—‹ â—‹ â—‹            â—‹ â—‹ â—‹            â—‹
    â—‹       â†’      â—‹ â—‹ â—‹     â†’     â—‹ â—‹ â—‹     â†’     â—‹ â—‹ â—‹     â†’     â—‹
    â—‹              â—‹ â—‹ â—‹            â—‹ â—‹ â—‹            â—‹ â—‹ â—‹            â—‹
   ...             ...              ...              ...             ...
    â—‹              â—‹ â—‹ â—‹            â—‹ â—‹ â—‹            â—‹ â—‹ â—‹            â—‹
            ReLU           ReLU           ReLU           (Linear)
```

**Giáº£i thÃ­ch:**

- **ReLU activation**: f(x) = max(0, x), giÃºp non-linearity vÃ  trÃ¡nh vanishing gradient
- **KhÃ´ng cÃ³ activation á»Ÿ output**: Q-values cÃ³ thá»ƒ Ã¢m hoáº·c dÆ°Æ¡ng
- **Xavier initialization**: Khá»Ÿi táº¡o weights Ä‘á»ƒ gradients á»•n Ä‘á»‹nh

### 3.3 Experience Replay

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REPLAY BUFFER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ (sâ‚, aâ‚, râ‚, s'â‚, doneâ‚)                               â”‚ â”‚
â”‚  â”‚ (sâ‚‚, aâ‚‚, râ‚‚, s'â‚‚, doneâ‚‚)                               â”‚ â”‚
â”‚  â”‚ (sâ‚ƒ, aâ‚ƒ, râ‚ƒ, s'â‚ƒ, doneâ‚ƒ)                               â”‚ â”‚
â”‚  â”‚ ...                                                     â”‚ â”‚
â”‚  â”‚ (sâ‚™, aâ‚™, râ‚™, s'â‚™, doneâ‚™)     Capacity: 100,000         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â”‚                                   â”‚
â”‚                Random Sample (batch_size = 64)               â”‚
â”‚                          â–¼                                   â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                  â”‚  Mini-Batch   â”‚                           â”‚
â”‚                  â”‚ for Training  â”‚                           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Táº¡i sao cáº§n Experience Replay:**

1. **Decorrelation**: Samples liÃªn tiáº¿p cÃ³ correlation cao â†’ unstable training
2. **Sample efficiency**: Má»—i transition Ä‘Æ°á»£c há»c nhiá»u láº§n
3. **Stable learning**: Diverse batches â†’ gradients á»•n Ä‘á»‹nh hÆ¡n

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): Táº¡i sao cáº§n "Ã”n bÃ i" (Replay)?**
>
> Khi Ä‘i há»c, náº¿u báº¡n chá»‰ há»c bÃ i má»›i vÃ  quÃªn ngay bÃ i cÅ©, báº¡n sáº½ khÃ´ng thá»ƒ giá»i Ä‘Æ°á»£c.
> **Experience Replay** giá»‘ng nhÆ° cuá»‘n vá»Ÿ ghi chÃ©p cá»§a AI.
>
> - Má»—i khi AI thá»­ má»™t hÃ nh Ä‘á»™ng (vÃ­ dá»¥: xáº£ pin lÃºc 10h sÃ¡ng), nÃ³ ghi láº¡i káº¿t quáº£ vÃ o vá»Ÿ: "Xáº£ pin lÃºc 10h sÃ¡ng -> Háº¿t pin lÃºc tá»‘i -> Bá»‹ pháº¡t náº·ng".
> - Má»—i tá»‘i, AI khÃ´ng chá»‰ há»c bÃ i cá»§a ngÃ y hÃ´m nay, mÃ  cÃ²n láº¥y ngáº«u nhiÃªn cÃ¡c trang vá»Ÿ cÅ© ra Ã´n láº¡i.
> - Viá»‡c nÃ y giÃºp AI nhá»› lÃ¢u: "Ã€, bÃ i há»c xÆ°Æ¡ng mÃ¡u tá»« tuáº§n trÆ°á»›c lÃ  khÃ´ng Ä‘Æ°á»£c xáº£ pin bá»«a bÃ£i". NÃ³ giÃºp AI khÃ´ng bá»‹ "há»c váº¹t" chá»‰ biáº¿t lÃ m theo thÃ³i quen gáº§n nháº¥t.

### 3.4 Target Network

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Q-Network       â”‚      â”‚  Target Network   â”‚
â”‚   Î¸ (online)      â”‚      â”‚  Î¸â» (frozen)      â”‚
â”‚                   â”‚      â”‚                   â”‚
â”‚ Used for:         â”‚      â”‚ Used for:         â”‚
â”‚ - Select action   â”‚      â”‚ - Calculate targetâ”‚
â”‚ - Update weights  â”‚      â”‚ - KhÃ´ng update    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚
          â”‚      Copy weights        â”‚
          â”‚  (every 1000 steps)      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CÃ´ng thá»©c cáº­p nháº­t:**

```
Target:    y = r + Î³ Ã— max_a' Q_target(s', a')
Loss:      L = (Q(s, a) - y)Â²
Update:    Î¸ â† Î¸ - Î± Ã— âˆ‡_Î¸ L
```

### 3.5 Epsilon-Greedy Exploration

```
        Îµ = 1.0 (ban Ä‘áº§u)
          â”‚
          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Îµ-decay = 0.995
          â”‚    \
          â”‚     \
          â”‚      \
          â”‚       \_____________________ Îµ_min = 0.01
          â”‚
Episode:  0   100  200  300  400  500
```

**Chiáº¿n lÆ°á»£c:**

- **Îµ = 1.0**: Explore hoÃ n toÃ n (random actions)
- **Îµ â†’ 0.01**: Exploit nhiá»u hÆ¡n (best Q-value actions)
- Decay má»—i episode: Îµ â† Îµ Ã— 0.995

### 3.6 Hyperparameters

| Parameter | Value | LÃ½ do |
|-----------|-------|-------|
| Learning rate | 1e-4 | Äá»§ nhá» Ä‘á»ƒ stable vá»›i continuous state |
| Gamma (Î³) | 0.99 | Quan tÃ¢m long-term (nÄƒng lÆ°á»£ng cáº§n planning) |
| Batch size | 64 | Balance giá»¯a speed vÃ  stability |
| Buffer size | 100,000 | Äá»§ lá»›n Ä‘á»ƒ diverse experiences |
| Target update | 1000 steps | Äá»§ thÆ°á»ng xuyÃªn nhÆ°ng khÃ´ng quÃ¡ fast |
| Epsilon decay | 0.995 | Giáº£m dáº§n Ä‘á»u qua 500 episodes |

### 3.7 Training Process (Pseudocode)

```python
for episode in range(500):
    state = env.reset()
    total_reward = 0
    
    for step in range(24):  # 24 giá»
        # 1. Select action (Îµ-greedy)
        if random() < epsilon:
            action = random_action()
        else:
            action = argmax(Q_network(state))
        
        # 2. Execute action
        next_state, reward, done = env.step(action)
        
        # 3. Store transition
        replay_buffer.push(state, action, reward, next_state, done)
        
        # 4. Sample and update
        if buffer.is_ready():
            batch = buffer.sample(64)
            
            # Calculate target
            with no_grad():
                next_q = target_network(batch.next_states).max()
                target = batch.rewards + gamma * next_q * (1 - batch.dones)
            
            # Calculate loss and backprop
            current_q = q_network(batch.states)[batch.actions]
            loss = SmoothL1Loss(current_q, target)
            optimizer.step()
        
        # 5. Update target network
        if step % 1000 == 0:
            target_network.load_state_dict(q_network.state_dict())
        
        state = next_state
        total_reward += reward
    
    # 6. Decay epsilon
    epsilon = max(0.01, epsilon * 0.995)
```

---

## PHáº¦N 4: PHÃ‚N TÃCH Tá»I Æ¯U HÃ“A AI (15%)

### 4.1 Agent ÄÃ£ Tá»‘i Æ¯u NhÆ° Tháº¿ NÃ o?

**Quan sÃ¡t tá»« Demo 24 giá»:**

| Giai Ä‘oáº¡n | Giá» | HÃ nh vi Agent | LÃ½ do |
|-----------|-----|---------------|-------|
| ÄÃªm | 0-6 | Renewable+Grid | Wind generation cao, giÃ¡ grid tháº¥p |
| SÃ¡ng | 7-9 | Renewable+Discharge | Peak price, dÃ¹ng pin thay vÃ¬ mua grid |
| TrÆ°a | 10-14 | Charge | Solar cao nháº¥t, sáº¡c pin lÃªn 100% |
| Chiá»u | 15-17 | Mixed | Chuyá»ƒn tiáº¿p, duy trÃ¬ pin |
| Tá»‘i | 18-21 | Renewable+Discharge | Peak price cao nháº¥t, xáº£ pin |
| ÄÃªm | 22-23 | Renewable+Discharge | GiÃ¡ giáº£m, váº«n Æ°u tiÃªn dÃ¹ng pin |

### 4.2 Trade-offs ÄÆ°á»£c CÃ¢n Báº±ng

#### 1. Cost vs Renewable Usage

```
Trade-off: ÄÃ´i khi mua grid ráº» hÆ¡n chá» solar/wind

Agent's Solution: 
- Chá»‰ mua grid khi giÃ¡ tháº¥p (off-peak hours)
- Æ¯u tiÃªn renewable ngay cáº£ khi hiá»‡u quáº£ tháº¥p hÆ¡n
- Káº¿t quáº£: 79% renewable usage, chá»‰ $4.45/ngÃ y
```

#### 2. Battery Wear vs Storage Value

```
Trade-off: Sáº¡c/xáº£ nhiá»u lÃ m hao mÃ²n pin, nhÆ°ng khÃ´ng dÃ¹ng thÃ¬ lÃ£ng phÃ­

Agent's Solution:
- Chá»‰ sáº¡c khi renewable dÆ° (trÆ°a)
- Chá»‰ xáº£ khi peak price (sÃ¡ng sá»›m, tá»‘i)
- TrÃ¡nh charge/discharge liÃªn tá»¥c
```

#### 3. Immediate Reward vs Future Planning

```
Trade-off: DÃ¹ng pin ngay láº¥y reward, hay giá»¯ cho peak demand?

Agent's Solution:
- Î³ = 0.99 â†’ quan tÃ¢m future reward
- Sáº¡c Ä‘áº§y pin trÆ°á»›c peak hours
- Xáº£ pin Ä‘Ãºng lÃºc giÃ¡ cao nháº¥t
```

> **ğŸ’¡ GÃ³c nhÃ¬n cho ngÆ°á»i khÃ´ng chuyÃªn (Non-IT): Chiáº¿n thuáº­t "Con buÃ´n" cá»§a AI**
>
> Sau khi tá»± há»c, AI Ä‘Ã£ trá»Ÿ thÃ nh má»™t nhÃ  buÃ´n nÄƒng lÆ°á»£ng thÃ´ng minh vá»›i chiáº¿n thuáº­t **"Mua Ä‘Ã¡y, BÃ¡n Ä‘á»‰nh"**:
>
> 1. **SÃ¡ng sá»›m & ÄÃªm (GiÃ¡ ráº»):** "HÃ ng" Ä‘áº§y chá»£ (giÃ³ nhiá»u, giÃ¡ lÆ°á»›i ráº»). AI tranh thá»§ dÃ¹ng, vÃ  quan trá»ng lÃ  **giá»¯ nguyÃªn kho hÃ ng (pin)**, khÃ´ng bÃ¡n ra.
> 2. **TrÆ°a (Náº¯ng to):** "HÃ ng" miá»…n phÃ­ rÆ¡i Ä‘áº§y sÃ¢n (Ä‘iá»‡n máº·t trá»i). AI nháº·t háº¿t vÃ o kho (sáº¡c Ä‘áº§y pin 100%). ÄÃ¢y lÃ  lÃºc tÃ­ch trá»¯.
> 3. **Chiá»u tá»‘i (GiÃ¡ Ä‘áº¯t cáº¯t cá»•):** LÃºc nÃ y ai cÅ©ng cáº§n Ä‘iá»‡n, giÃ¡ tÄƒng vá»t. AI má»Ÿ kho (xáº£ pin) ra dÃ¹ng, tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘i mua ngoÃ i.
>
> Káº¿t quáº£: Nhá» biáº¿t tÃ­ch trá»¯ lÃºc ráº»/miá»…n phÃ­ vÃ  tung ra lÃºc Ä‘áº¯t, AI giÃºp gia chá»§ tiáº¿t kiá»‡m tá»›i 92% tiá»n Ä‘iá»‡n!

### 4.3 Learning Convergence Analysis

**Tá»« Training Logs (Láº§n cháº¡y má»›i nháº¥t - Feb 2026):**

```
Episode   10 | Reward:   -3.10 | Eps: 0.951  â† Äang explore
Episode   20 | Reward:    1.06 | Eps: 0.905  â† Báº¯t Ä‘áº§u há»c
Episode   50 | Reward:   -1.87 | Eps: 0.778  â† ChÆ°a stable
Episode   80 | Reward:    1.77 | Eps: 0.670  â† Improving
Episode  100 | Reward:    2.62 | Eps: 0.606  â† Near optimal

Training time: 1.5 seconds (CPU)
Best episode reward: 13.37
```

**Observation:**

- Epsilon giáº£m â†’ Agent exploit more â†’ Reward tÄƒng
- Renewable usage tÄƒng tá»« 45.8% â†’ 59.8% trong 100 episodes
- Training ráº¥t nhanh (~70 episodes/second trÃªn CPU)

### 4.4 Háº¡n Cháº¿ Cá»§a Approach

| Háº¡n cháº¿ | Giáº£i thÃ­ch | Possible Solution |
|---------|------------|-------------------|
| **Discrete actions** | KhÃ´ng thá»ƒ control chÃ­nh xÃ¡c kW | Continuous action space |
| **Single episode pattern** | Chá»‰ train trÃªn 1 mÃ¹a | ThÃªm seasonal variation |
| **No demand forecasting** | KhÃ´ng biáº¿t demand tÆ°Æ¡ng lai | Add LSTM for prediction |
| **Overfitting risk** | CÃ³ thá»ƒ overfit Ä‘áº¿n specific patterns | Regularization, more data |

---

## PHáº¦N 5: Káº¾T QUáº¢ VÃ€ ÄÃNH GIÃ (15%)

### 5.1 Performance Metrics

#### So sÃ¡nh vá»›i Random Baseline (Káº¿t quáº£ má»›i nháº¥t - Feb 2026)

| Metric | Trained Agent | Random | Improvement |
|--------|---------------|--------|-------------|
| Mean Episode Reward | 14.75 | -3.34 | **+541.1%** |
| Daily Grid Cost | $1.26 | $16.42 | **-92.3%** |
| Renewable Usage | 82.5% | 47.8% | +34.7pp |
| Demand Satisfaction | 96.6% | 83.9% | +12.7pp |
| Unmet Demand Ratio | 3.4% | 16.1% | -12.7pp |

### 5.2 Biá»ƒu Äá»“ Training Curves

*(Xem file: evaluation_results/training_curves.png)*

**PhÃ¢n tÃ­ch:**

- **Reward curve**: TÄƒng tá»« Ã¢m lÃªn dÆ°Æ¡ng, báº¯t Ä‘áº§u converge ~episode 50
- **Cost curve**: Giáº£m máº¡nh tá»« ~$18 xuá»‘ng ~$1.26
- **Renewable ratio**: TÄƒng tá»« 47.8% lÃªn 82.5%
- **Epsilon decay**: Giáº£m mÆ°á»£t tá»« 1.0 xuá»‘ng 0.01

### 5.3 Episode Analysis (24h Operation)

*(Xem file: evaluation_results/episode_analysis.png)*

**Patterns há»c Ä‘Æ°á»£c:**

1. **Charge at noon**: Battery tá»« 50% â†’ 100% tá»« 8AM-2PM
2. **Hold during transition**: Giá»¯ 100% tá»« 2PM-5PM
3. **Discharge at peak**: Xáº£ tá»« 100% â†’ 21% tá»« 5PM-11PM

### 5.4 Failure Scenarios Analysis

| Scenario | What happens | Agent behavior | Result |
|----------|--------------|----------------|--------|
| Cloudy day | Solar = 0 | Rely on wind + grid | Cost â†‘ 20% |
| High demand spike | Demand 2x normal | Discharge + grid | 5% unmet |
| Battery low + peak | Battery < 10% | Force buy grid | Cost â†‘, penalty |

### 5.5 Comparative Analysis

**vs Rule-based heuristic:**

```
Simple rule: "Always use renewable first, then battery, then grid"

Results:
- Rule-based reward: ~8.5
- DQN agent reward: ~13.9
- Improvement: +64%

Reason: DQN learns WHEN to charge/discharge optimally
```

---

## PHáº¦N 6: XEM XÃ‰T Äáº O Äá»¨C, THá»°C TIá»„N VÃ€ TÆ¯Æ NG LAI (10%)

### 6.1 Ethical Considerations

#### 1. Fairness in Energy Distribution

```
Concern: AI cÃ³ thá»ƒ Æ°u tiÃªn efficiency over equity
         â†’ Má»™t sá»‘ users cÃ³ thá»ƒ bá»‹ blackout nhiá»u hÆ¡n

Mitigation:
- Äáº£m báº£o unmet_demand penalty Ä‘á»§ cao
- Monitoring fairness metrics across user groups
- Human oversight trong critical decisions
```

#### 2. Automated Decision-Making Risks

```
Concern: Full automation khÃ´ng cÃ³ human oversight
         â†’ Lá»—i AI cÃ³ thá»ƒ gÃ¢y blackout lá»›n

Mitigation:
- LuÃ´n cÃ³ manual override option
- Alert system khi agent behavior báº¥t thÆ°á»ng
- Fallback to rule-based khi confidence tháº¥p
```

#### 3. Privacy Concerns

```
Concern: Demand data cÃ³ thá»ƒ reveal user behavior
         â†’ Privacy violation

Mitigation:
- Aggregate data instead of individual
- Differential privacy trong training
- Clear data usage policies
```

### 6.2 Practical Deployment Issues

#### 1. Scalability

```
Current: Single microgrid, simulated data
Real-world:
- Multiple interconnected microgrids
- Real-time data tá»« sensors
- Coordination between agents

Solution: Multi-agent RL, distributed training
```

#### 2. Real-time Requirements

```
Current: Batch processing, hourly decisions
Real-world:
- Millisecond response time needed
- Real-time market prices
- Instant demand changes

Solution: Edge computing, model compression
```

#### 3. Sensor Reliability

```
Problem: Sensors cÃ³ thá»ƒ fail hoáº·c give wrong readings
         â†’ Agent receives incorrect state

Solution:
- Redundant sensors
- Anomaly detection
- Robust RL (train with noise)
```

### 6.3 Future Enhancements

#### 1. Multi-Agent RL

```
Scenario: Multiple microgrids trading energy

Approach:
- Each microgrid = 1 agent
- Cooperative/competitive learning
- Shared rewards for grid stability
```

#### 2. Demand Forecasting Integration

```
Current: Agent only sees current demand
Enhanced:
- LSTM to predict next 24h demand
- Include weather forecast
- Better planning capability
```

#### 3. IoT Integration

```
Current: Simulated sensors
Enhanced:
- Real smart meters
- Weather APIs
- Real-time pricing APIs
- Plant condition monitoring
```

#### 4. Continuous Action Space

```
Current: 5 discrete actions
Enhanced:
- Continuous control: "charge 15.3 kWh"
- Use PPO or SAC algorithms
- More fine-grained optimization
```

### 6.4 Recommendations for Ethical Deployment

1. **Transparency**: Explain agent decisions to operators
2. **Auditing**: Regular review of agent performance
3. **Backup systems**: Rule-based fallback always available
4. **Human-in-the-loop**: Critical decisions require approval
5. **Continuous monitoring**: Real-time anomaly detection

---

## TÃ€I LIá»†U THAM KHáº¢O

1. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." Nature.
2. Lillicrap, T. P., et al. (2015). "Continuous control with deep reinforcement learning." ICLR.
3. Vazquez-Canteli, J. R., & Nagy, Z. (2019). "Reinforcement learning for demand response: A review." Applied Energy.
4. FranÃ§ois-Lavet, V., et al. (2018). "An Introduction to Deep Reinforcement Learning." Foundations and Trends in Machine Learning.

---

## PHá»¤ Lá»¤C: CODE SNIPPETS

### A1. Environment Step Function

```python
def step(self, action):
    # Get current state
    demand = self._get_demand(self.current_hour)
    solar = self._get_solar_generation(self.current_hour)
    wind = self._get_wind_generation(self.current_hour)
    
    # Process action
    if action == 4:  # Renewable + Grid
        renewable_used = min(solar + wind, demand)
        grid_purchased = demand - renewable_used
    
    # Calculate reward
    reward = self._calculate_reward(...)
    
    return next_state, reward, done, info
```

### A2. DQN Update Function

```python
def update(self):
    # Sample batch
    states, actions, rewards, next_states, dones = buffer.sample(64)
    
    # Current Q values
    current_q = q_network(states).gather(1, actions)
    
    # Target Q values
    with torch.no_grad():
        next_q = target_network(next_states).max(dim=1)[0]
        target_q = rewards + 0.99 * next_q * (1 - dones)
    
    # Update
    loss = F.smooth_l1_loss(current_q, target_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### A3. Full Source Code

Xem thÆ° má»¥c: `/Volumes/DATA/workspace/RL-ideas/microgrid_rl/`
