{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ”‹ Tá»‘i Æ¯u HÃ³a NÄƒng LÆ°á»£ng Microgrid - Deep Reinforcement Learning\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ“Œ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng (CHá»ˆ Cáº¦N 3 BÆ¯á»šC)\n",
                "\n",
                "| BÆ°á»›c | HÃ nh Äá»™ng | Thá»i Gian |\n",
                "|------|-----------|------------|\n",
                "| **1** | Cháº¡y Ã´ \"ğŸ“¦ CÃ i Äáº·t\" | ~10 giÃ¢y |\n",
                "| **2** | Cháº¡y Ã´ \"ğŸš€ Huáº¥n Luyá»‡n\" | ~30 giÃ¢y |\n",
                "| **3** | Cháº¡y Ã´ \"ğŸ“Š Káº¿t Quáº£\" | ~5 giÃ¢y |\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ¯ BÃ i ToÃ¡n\n",
                "\n",
                "**Má»¥c tiÃªu:** Dáº¡y AI quáº£n lÃ½ nÄƒng lÆ°á»£ng thÃ´ng minh cho há»‡ thá»‘ng microgrid\n",
                "\n",
                "```\n",
                "â˜€ï¸ NÄƒng lÆ°á»£ng máº·t trá»i  â”€â”\n",
                "ğŸŒ¬ï¸ NÄƒng lÆ°á»£ng giÃ³      â”€â”¼â”€â–º ğŸ¤– AI Agent â”€â–º âš¡ Cung cáº¥p Ä‘iá»‡n cho há»™ gia Ä‘Ã¬nh\n",
                "ğŸ”‹ Pin lÆ°u trá»¯         â”€â”¤\n",
                "âš¡ LÆ°á»›i Ä‘iá»‡n           â”€â”˜\n",
                "```\n",
                "\n",
                "**AI sáº½ há»c cÃ¡ch:**\n",
                "- âœ… DÃ¹ng nÄƒng lÆ°á»£ng xanh (máº·t trá»i, giÃ³) nhiá»u nháº¥t cÃ³ thá»ƒ\n",
                "- âœ… Tiáº¿t kiá»‡m tiá»n Ä‘iá»‡n báº±ng cÃ¡ch trÃ¡nh mua tá»« lÆ°á»›i khi giÃ¡ cao\n",
                "- âœ… Sáº¡c pin khi cÃ³ thá»«a nÄƒng lÆ°á»£ng, xáº£ pin khi thiáº¿u"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“¦ BÆ¯á»šC 1: CÃ i Äáº·t (Cháº¡y 1 láº§n)\n",
                "\n",
                "Ã” nÃ y cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t vÃ  táº¡o mÃ´i trÆ°á»ng microgrid."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸ“¦ BÆ¯á»šC 1: CÃ€I Äáº¶T - Cháº¡y Ã´ nÃ y trÆ°á»›c\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Œ CÃ€I Äáº¶T THÆ¯ VIá»†N (chá»‰ cáº§n cháº¡y 1 láº§n)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "!pip install gymnasium torch numpy matplotlib -q\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from collections import deque\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Kiá»ƒm tra GPU\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"ğŸ–¥ï¸ Thiáº¿t bá»‹: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ”§ Cáº¤U HÃŒNH - CÃ³ thá»ƒ thay Ä‘á»•i cÃ¡c giÃ¡ trá»‹ nÃ y!\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "SEED = 42          # ğŸ”§ Thay Ä‘á»•i: 42, 123, 456, 789, 999\n",
                "EPISODES = 100     # ğŸ”§ Thay Ä‘á»•i: 50-500 (nhiá»u hÆ¡n = há»c tá»‘t hÆ¡n)\n",
                "LEARNING_RATE = 0.0001  # ğŸ”§ Thay Ä‘á»•i: 0.0001 - 0.001\n",
                "\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "random.seed(SEED)\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸŒ MÃ”I TRÆ¯á»œNG MICROGRID (khÃ´ng cáº§n hiá»ƒu chi tiáº¿t)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class MicrogridEnv:\n",
                "    \"\"\"\n",
                "    MÃ´i trÆ°á»ng mÃ´ phá»ng há»‡ thá»‘ng microgrid:\n",
                "    - Pin máº·t trá»i + Turbine giÃ³ + Pin lÆ°u trá»¯ + LÆ°á»›i Ä‘iá»‡n\n",
                "    - Agent pháº£i quyáº¿t Ä‘á»‹nh cÃ¡ch phÃ¢n phá»‘i nÄƒng lÆ°á»£ng má»—i giá»\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        self.battery_capacity = 100.0  # kWh\n",
                "        self.battery_level = 50.0\n",
                "        self.current_hour = 0\n",
                "        \n",
                "    def reset(self):\n",
                "        self.battery_level = 50.0\n",
                "        self.current_hour = 0\n",
                "        return self._get_state()\n",
                "    \n",
                "    def _get_state(self):\n",
                "        hour = self.current_hour % 24\n",
                "        # MÃ´ phá»ng nÄƒng lÆ°á»£ng máº·t trá»i (cao vÃ o buá»•i trÆ°a)\n",
                "        solar = max(0, np.sin((hour - 6) * np.pi / 12)) * 50 * (0.8 + 0.4 * np.random.random())\n",
                "        # MÃ´ phá»ng nÄƒng lÆ°á»£ng giÃ³ (ngáº«u nhiÃªn)\n",
                "        wind = 30 * np.random.random()\n",
                "        # Nhu cáº§u Ä‘iá»‡n (cao vÃ o sÃ¡ng vÃ  tá»‘i)\n",
                "        demand = 40 + 20 * np.sin((hour - 6) * np.pi / 12) + 10 * np.random.random()\n",
                "        # GiÃ¡ Ä‘iá»‡n (cao vÃ o giá» cao Ä‘iá»ƒm)\n",
                "        price = 0.15 + 0.1 * (1 if 17 <= hour <= 21 else 0)\n",
                "        \n",
                "        return np.array([\n",
                "            self.battery_level / self.battery_capacity,\n",
                "            demand / 100,\n",
                "            solar / 50,\n",
                "            wind / 30,\n",
                "            price / 0.25,\n",
                "            np.sin(2 * np.pi * hour / 24),\n",
                "            np.cos(2 * np.pi * hour / 24),\n",
                "            0.5\n",
                "        ], dtype=np.float32)\n",
                "    \n",
                "    def step(self, action):\n",
                "        state = self._get_state()\n",
                "        demand = state[1] * 100\n",
                "        solar = state[2] * 50\n",
                "        wind = state[3] * 30\n",
                "        price = state[4] * 0.25\n",
                "        renewable = solar + wind\n",
                "        \n",
                "        # Xá»­ lÃ½ hÃ nh Ä‘á»™ng\n",
                "        grid_used = 0\n",
                "        renewable_used = 0\n",
                "        battery_change = 0\n",
                "        \n",
                "        if action == 0:  # Xáº£ pin\n",
                "            discharge = min(20, self.battery_level, demand)\n",
                "            battery_change = -discharge\n",
                "            renewable_used = min(renewable, demand - discharge)\n",
                "            grid_used = max(0, demand - discharge - renewable_used)\n",
                "        elif action == 1:  # Sáº¡c tá»« renewable\n",
                "            renewable_used = min(renewable, demand)\n",
                "            grid_used = max(0, demand - renewable_used)\n",
                "            excess = renewable - renewable_used\n",
                "            battery_change = min(20, self.battery_capacity - self.battery_level, excess)\n",
                "        elif action == 2:  # Mua tá»« lÆ°á»›i\n",
                "            grid_used = demand\n",
                "        elif action == 3:  # Renewable + xáº£ pin\n",
                "            renewable_used = min(renewable, demand)\n",
                "            remaining = demand - renewable_used\n",
                "            discharge = min(20, self.battery_level, remaining)\n",
                "            battery_change = -discharge\n",
                "            grid_used = max(0, remaining - discharge)\n",
                "        else:  # Renewable + lÆ°á»›i\n",
                "            renewable_used = min(renewable, demand)\n",
                "            grid_used = max(0, demand - renewable_used)\n",
                "        \n",
                "        # Cáº­p nháº­t pin\n",
                "        self.battery_level = np.clip(self.battery_level + battery_change, 0, self.battery_capacity)\n",
                "        \n",
                "        # TÃ­nh reward\n",
                "        reward = renewable_used / 40 - 2 * grid_used / 40 * price - 0.1 * abs(battery_change) / 20\n",
                "        if grid_used == 0 and 17 <= self.current_hour % 24 <= 21:\n",
                "            reward += 0.5\n",
                "        \n",
                "        self.current_hour += 1\n",
                "        done = self.current_hour >= 24\n",
                "        \n",
                "        return self._get_state(), reward, done, {\n",
                "            'renewable_used': renewable_used,\n",
                "            'grid_used': grid_used,\n",
                "            'cost': grid_used * price\n",
                "        }\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ§  Máº NG NEURAL NETWORK (DQN)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class QNetwork(nn.Module):\n",
                "    \"\"\"Máº¡ng neural network Ä‘á»ƒ Æ°á»›c lÆ°á»£ng Q-values\"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.network = nn.Sequential(\n",
                "            nn.Linear(8, 256),   # 8 inputs (state)\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 5)    # 5 outputs (actions)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.network(x)\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ¤– DQN AGENT\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "class DQNAgent:\n",
                "    \"\"\"Agent sá»­ dá»¥ng Deep Q-Learning\"\"\"\n",
                "    def __init__(self):\n",
                "        self.q_net = QNetwork().to(device)\n",
                "        self.target_net = QNetwork().to(device)\n",
                "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
                "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=LEARNING_RATE)\n",
                "        self.buffer = deque(maxlen=10000)\n",
                "        self.epsilon = 1.0\n",
                "        self.gamma = 0.99\n",
                "        self.update_count = 0\n",
                "        \n",
                "    def select_action(self, state):\n",
                "        if random.random() < self.epsilon:\n",
                "            return random.randint(0, 4)\n",
                "        with torch.no_grad():\n",
                "            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "            return self.q_net(state_t).argmax().item()\n",
                "    \n",
                "    def store(self, state, action, reward, next_state, done):\n",
                "        self.buffer.append((state, action, reward, next_state, done))\n",
                "    \n",
                "    def train(self):\n",
                "        if len(self.buffer) < 64:\n",
                "            return\n",
                "        \n",
                "        batch = random.sample(self.buffer, 64)\n",
                "        states = torch.FloatTensor([t[0] for t in batch]).to(device)\n",
                "        actions = torch.LongTensor([t[1] for t in batch]).to(device)\n",
                "        rewards = torch.FloatTensor([t[2] for t in batch]).to(device)\n",
                "        next_states = torch.FloatTensor([t[3] for t in batch]).to(device)\n",
                "        dones = torch.FloatTensor([t[4] for t in batch]).to(device)\n",
                "        \n",
                "        current_q = self.q_net(states).gather(1, actions.unsqueeze(1))\n",
                "        with torch.no_grad():\n",
                "            next_q = self.target_net(next_states).max(1)[0]\n",
                "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
                "        \n",
                "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "        \n",
                "        self.update_count += 1\n",
                "        if self.update_count % 100 == 0:\n",
                "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
                "\n",
                "# Khá»Ÿi táº¡o\n",
                "env = MicrogridEnv()\n",
                "agent = DQNAgent()\n",
                "print(\"\\nâœ… CÃ i Ä‘áº·t hoÃ n táº¥t! Chuyá»ƒn sang BÆ¯á»šC 2.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸš€ BÆ¯á»šC 2: Huáº¥n Luyá»‡n AI\n",
                "\n",
                "Ã” nÃ y huáº¥n luyá»‡n AI Ä‘á»ƒ há»c cÃ¡ch quáº£n lÃ½ nÄƒng lÆ°á»£ng tá»‘i Æ°u.\n",
                "\n",
                "**QuÃ¡ trÃ¬nh:**\n",
                "1. AI thá»­ cÃ¡c hÃ nh Ä‘á»™ng ngáº«u nhiÃªn\n",
                "2. Nháº­n pháº£n há»“i (reward) tá»« mÃ´i trÆ°á»ng\n",
                "3. Há»c tá»« kinh nghiá»‡m Ä‘á»ƒ cáº£i thiá»‡n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸš€ BÆ¯á»šC 2: HUáº¤N LUYá»†N AI - Cháº¡y Ã´ nÃ y Ä‘á»ƒ train\n",
                "\n",
                "print(\"â•\" * 60)\n",
                "print(\"ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N DQN AGENT\")\n",
                "print(\"â•\" * 60)\n",
                "print(f\"ğŸ“Š Sá»‘ episodes: {EPISODES}\")\n",
                "print(f\"ğŸ² Random seed: {SEED}\")\n",
                "print(f\"ğŸ“ˆ Learning rate: {LEARNING_RATE}\")\n",
                "print(\"â•\" * 60)\n",
                "\n",
                "rewards_history = []\n",
                "epsilon_history = []\n",
                "renewable_history = []\n",
                "\n",
                "for episode in range(1, EPISODES + 1):\n",
                "    state = env.reset()\n",
                "    total_reward = 0\n",
                "    total_renewable = 0\n",
                "    total_grid = 0\n",
                "    \n",
                "    for step in range(24):  # 24 giá»/ngÃ y\n",
                "        action = agent.select_action(state)\n",
                "        next_state, reward, done, info = env.step(action)\n",
                "        agent.store(state, action, reward, next_state, done)\n",
                "        agent.train()\n",
                "        \n",
                "        state = next_state\n",
                "        total_reward += reward\n",
                "        total_renewable += info['renewable_used']\n",
                "        total_grid += info['grid_used']\n",
                "        \n",
                "        if done:\n",
                "            break\n",
                "    \n",
                "    # Giáº£m epsilon\n",
                "    agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
                "    \n",
                "    # LÆ°u history\n",
                "    rewards_history.append(total_reward)\n",
                "    epsilon_history.append(agent.epsilon)\n",
                "    renewable_ratio = total_renewable / (total_renewable + total_grid + 1e-6) * 100\n",
                "    renewable_history.append(renewable_ratio)\n",
                "    \n",
                "    # In tiáº¿n trÃ¬nh\n",
                "    if episode % 10 == 0:\n",
                "        avg_reward = np.mean(rewards_history[-10:])\n",
                "        print(f\"Episode {episode:3d} | Reward: {avg_reward:+7.2f} | Îµ: {agent.epsilon:.3f} | Renewable: {renewable_ratio:.1f}%\")\n",
                "\n",
                "print(\"\\n\" + \"â•\" * 60)\n",
                "print(\"âœ… HUáº¤N LUYá»†N HOÃ€N Táº¤T!\")\n",
                "print(f\"ğŸ“ˆ Best Reward: {max(rewards_history):.2f}\")\n",
                "print(f\"ğŸ“ˆ Final Avg Reward: {np.mean(rewards_history[-10:]):.2f}\")\n",
                "print(\"â•\" * 60)\n",
                "print(\"\\nğŸ‘‰ Chuyá»ƒn sang BÆ¯á»šC 3 Ä‘á»ƒ xem káº¿t quáº£!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“Š BÆ¯á»šC 3: Xem Káº¿t Quáº£\n",
                "\n",
                "Ã” nÃ y hiá»ƒn thá»‹:\n",
                "- ğŸ“ˆ Äá»“ thá»‹ quÃ¡ trÃ¬nh há»c\n",
                "- ğŸ“Š So sÃ¡nh AI Ä‘Ã£ train vs Random\n",
                "- ğŸ“‹ Báº£ng sá»‘ liá»‡u káº¿t quáº£"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title ğŸ“Š BÆ¯á»šC 3: XEM Káº¾T QUáº¢ - Cháº¡y Ã´ nÃ y Ä‘á»ƒ xem Ä‘á»“ thá»‹\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Š Váº¼ Äá»’ THá»Š TRAINING\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Äá»“ thá»‹ 1: Reward theo episode\n",
                "axes[0].plot(rewards_history, alpha=0.3, color='blue')\n",
                "window = min(10, len(rewards_history))\n",
                "if len(rewards_history) >= window:\n",
                "    smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
                "    axes[0].plot(range(window-1, len(rewards_history)), smoothed, color='blue', linewidth=2)\n",
                "axes[0].set_title('ğŸ“ˆ Reward Theo Episode', fontsize=12)\n",
                "axes[0].set_xlabel('Episode')\n",
                "axes[0].set_ylabel('Total Reward')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Äá»“ thá»‹ 2: Epsilon decay\n",
                "axes[1].plot(epsilon_history, color='green', linewidth=2)\n",
                "axes[1].set_title('ğŸ² Epsilon Decay (Exploration)', fontsize=12)\n",
                "axes[1].set_xlabel('Episode')\n",
                "axes[1].set_ylabel('Epsilon')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# Äá»“ thá»‹ 3: Renewable usage\n",
                "axes[2].plot(renewable_history, color='orange', linewidth=2)\n",
                "axes[2].set_title('â˜€ï¸ Tá»· Lá»‡ NÄƒng LÆ°á»£ng TÃ¡i Táº¡o', fontsize=12)\n",
                "axes[2].set_xlabel('Episode')\n",
                "axes[2].set_ylabel('Renewable %')\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Š SO SÃNH Vá»šI RANDOM BASELINE\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"\\n\" + \"â•\" * 60)\n",
                "print(\"ğŸ“Š ÄÃNH GIÃ: TRAINED AGENT vs RANDOM BASELINE\")\n",
                "print(\"â•\" * 60)\n",
                "\n",
                "def evaluate(agent, use_random=False, num_episodes=10):\n",
                "    total_rewards = []\n",
                "    total_costs = []\n",
                "    total_renewable = []\n",
                "    \n",
                "    for _ in range(num_episodes):\n",
                "        state = env.reset()\n",
                "        ep_reward = 0\n",
                "        ep_cost = 0\n",
                "        ep_renewable = 0\n",
                "        ep_total = 0\n",
                "        \n",
                "        for step in range(24):\n",
                "            if use_random:\n",
                "                action = random.randint(0, 4)\n",
                "            else:\n",
                "                old_eps = agent.epsilon\n",
                "                agent.epsilon = 0  # No exploration\n",
                "                action = agent.select_action(state)\n",
                "                agent.epsilon = old_eps\n",
                "            \n",
                "            next_state, reward, done, info = env.step(action)\n",
                "            ep_reward += reward\n",
                "            ep_cost += info['cost']\n",
                "            ep_renewable += info['renewable_used']\n",
                "            ep_total += info['renewable_used'] + info['grid_used']\n",
                "            state = next_state\n",
                "            if done:\n",
                "                break\n",
                "        \n",
                "        total_rewards.append(ep_reward)\n",
                "        total_costs.append(ep_cost)\n",
                "        total_renewable.append(ep_renewable / ep_total * 100 if ep_total > 0 else 0)\n",
                "    \n",
                "    return {\n",
                "        'reward': np.mean(total_rewards),\n",
                "        'cost': np.mean(total_costs),\n",
                "        'renewable': np.mean(total_renewable)\n",
                "    }\n",
                "\n",
                "trained_results = evaluate(agent, use_random=False)\n",
                "random_results = evaluate(agent, use_random=True)\n",
                "\n",
                "# TÃ­nh improvement\n",
                "reward_improvement = ((trained_results['reward'] - random_results['reward']) / abs(random_results['reward'])) * 100 if random_results['reward'] != 0 else 0\n",
                "cost_improvement = ((random_results['cost'] - trained_results['cost']) / random_results['cost']) * 100 if random_results['cost'] != 0 else 0\n",
                "\n",
                "print(f\"\\n{'Metric':<25} {'Trained Agent':>15} {'Random':>15} {'Improvement':>15}\")\n",
                "print(\"-\" * 70)\n",
                "print(f\"{'Mean Episode Reward':<25} {trained_results['reward']:>+15.2f} {random_results['reward']:>+15.2f} {reward_improvement:>+14.1f}%\")\n",
                "print(f\"{'Daily Cost ($)':<25} {trained_results['cost']:>15.2f} {random_results['cost']:>15.2f} {cost_improvement:>+14.1f}%\")\n",
                "print(f\"{'Renewable Usage (%)':<25} {trained_results['renewable']:>14.1f}% {random_results['renewable']:>14.1f}%\")\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“Š Váº¼ BIá»‚U Äá»’ SO SÃNH\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
                "\n",
                "metrics = ['Reward', 'Cost ($)', 'Renewable %']\n",
                "trained_vals = [trained_results['reward'], trained_results['cost'], trained_results['renewable']]\n",
                "random_vals = [random_results['reward'], random_results['cost'], random_results['renewable']]\n",
                "colors = ['#2ecc71', '#e74c3c']\n",
                "\n",
                "for i, (metric, t_val, r_val) in enumerate(zip(metrics, trained_vals, random_vals)):\n",
                "    bars = axes[i].bar(['Trained', 'Random'], [t_val, r_val], color=colors)\n",
                "    axes[i].set_title(metric, fontsize=12, fontweight='bold')\n",
                "    axes[i].grid(True, alpha=0.3)\n",
                "    for bar, val in zip(bars, [t_val, r_val]):\n",
                "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
                "                     f'{val:.1f}', ha='center', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\" + \"â•\" * 60)\n",
                "print(\"âœ… Káº¾T QUáº¢ ÄÃƒ ÄÆ¯á»¢C LÆ¯U!\")\n",
                "print(\"   ğŸ“ training_curves.png\")\n",
                "print(\"   ğŸ“ comparison.png\")\n",
                "print(\"â•\" * 60)\n",
                "\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“ TÃ“M Táº®T Káº¾T LUáº¬N\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"\\nğŸ“ Káº¾T LUáº¬N:\")\n",
                "print(f\"   âœ… DQN Agent Ä‘Ã£ há»c Ä‘Æ°á»£c chÃ­nh sÃ¡ch quáº£n lÃ½ nÄƒng lÆ°á»£ng tá»‘i Æ°u\")\n",
                "print(f\"   âœ… Cáº£i thiá»‡n reward: {reward_improvement:+.1f}% so vá»›i random\")\n",
                "print(f\"   âœ… Giáº£m chi phÃ­ Ä‘iá»‡n: {cost_improvement:+.1f}%\")\n",
                "print(f\"   âœ… Tá»· lá»‡ nÄƒng lÆ°á»£ng tÃ¡i táº¡o: {trained_results['renewable']:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“š Giáº£i ThÃ­ch Chi Tiáº¿t\n",
                "\n",
                "### ğŸ¯ 5 HÃ nh Äá»™ng Cá»§a Agent\n",
                "\n",
                "| Action | TÃªn | Ã NghÄ©a |\n",
                "|--------|-----|--------|\n",
                "| 0 | Xáº£ pin | DÃ¹ng nÄƒng lÆ°á»£ng trong pin Ä‘á»ƒ cáº¥p Ä‘iá»‡n |\n",
                "| 1 | Sáº¡c tá»« renewable | LÆ°u nÄƒng lÆ°á»£ng máº·t trá»i/giÃ³ vÃ o pin |\n",
                "| 2 | Mua tá»« lÆ°á»›i | Mua toÃ n bá»™ Ä‘iá»‡n tá»« lÆ°á»›i Ä‘iá»‡n |\n",
                "| 3 | Renewable + Xáº£ pin | Æ¯u tiÃªn dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, thiáº¿u thÃ¬ xáº£ pin |\n",
                "| 4 | Renewable + LÆ°á»›i | Æ¯u tiÃªn dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, thiáº¿u thÃ¬ mua lÆ°á»›i |\n",
                "\n",
                "### ğŸ§  DQN Há»c NhÆ° Tháº¿ NÃ o?\n",
                "\n",
                "```\n",
                "1. ğŸ² KhÃ¡m phÃ¡: Thá»­ hÃ nh Ä‘á»™ng ngáº«u nhiÃªn (epsilon cao)\n",
                "   â†“\n",
                "2. ğŸ“Š Nháº­n pháº£n há»“i: Reward (+) hoáº·c penalty (-)\n",
                "   â†“  \n",
                "3. ğŸ’¾ LÆ°u kinh nghiá»‡m: (state, action, reward, next_state)\n",
                "   â†“\n",
                "4. ğŸ§  Há»c: Cáº­p nháº­t neural network tá»« kinh nghiá»‡m\n",
                "   â†“\n",
                "5. ğŸ“ˆ Khai thÃ¡c: Giáº£m epsilon, chá»n action tá»‘t hÆ¡n\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ”§ TÃ¹y Chá»‰nh Cho BÃ i LÃ m RiÃªng\n",
                "\n",
                "Thay Ä‘á»•i cÃ¡c giÃ¡ trá»‹ trong BÆ¯á»šC 1:\n",
                "\n",
                "| Sinh viÃªn | SEED | EPISODES | LEARNING_RATE |\n",
                "|-----------|------|----------|---------------|\n",
                "| SV1 | 42 | 100 | 0.0001 |\n",
                "| SV2 | 123 | 150 | 0.0002 |\n",
                "| SV3 | 456 | 200 | 0.0005 |\n",
                "| SV4 | 789 | 250 | 0.001 |\n",
                "| SV5 | 999 | 300 | 0.0003 |"
            ]
        }
    ]
}