# BÃO CÃO: PHÆ¯Æ NG PHÃP DQN (Deep Q-Network)

# Tá»‘i Æ¯u HÃ³a PhÃ¢n Phá»‘i NÄƒng LÆ°á»£ng Trong Microgrid

---

## 1. GIá»šI THIá»†U THUáº¬T TOÃN DQN

### 1.1 DQN LÃ  GÃ¬?

**Deep Q-Network (DQN)** lÃ  thuáº­t toÃ¡n káº¿t há»£p Q-Learning truyá»n thá»‘ng vá»›i Deep Neural Network, Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Mnih et al. (2015) trong paper "Human-level control through deep reinforcement learning".

**Ã tÆ°á»Ÿng cá»‘t lÃµi**: DÃ¹ng neural network Ä‘á»ƒ xáº¥p xá»‰ hÃ m Q-value:

```
Q(s, a) â‰ˆ Q_Î¸(s, a)    (Î¸ = weights cá»§a neural network)
```

Q-value cho biáº¿t **tá»•ng reward ká»³ vá»ng** khi thá»±c hiá»‡n action `a` táº¡i state `s` vÃ  follow optimal policy sau Ä‘Ã³.

### 1.2 Táº¡i Sao Chá»n DQN Cho Microgrid?

| TiÃªu chÃ­ | LÃ½ do |
|-----------|-------|
| **State space liÃªn tá»¥c (8D)** | Neural network xá»­ lÃ½ tá»‘t continuous input |
| **Action space discrete (5)** | DQN Ä‘Æ°á»£c thiáº¿t káº¿ cho discrete actions |
| **Sample efficiency** | Experience replay giÃºp táº­n dá»¥ng má»—i transition nhiá»u láº§n |
| **Stability** | Target network ngÄƒn oscillation trong training |

### 1.3 So SÃ¡nh Vá»›i Thuáº­t ToÃ¡n KhÃ¡c

| Thuáº­t toÃ¡n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p? |
|------------|---------|------------|----------|
| Q-Learning | ÄÆ¡n giáº£n | KhÃ´ng scale vá»›i high-dim state | âŒ |
| **DQN** | Stable, sample efficient | Chá»‰ discrete actions | âœ… |
| Policy Gradient | Continuous actions | High variance | âš ï¸ |
| PPO | Linh hoáº¡t, robust | Phá»©c táº¡p hÆ¡n, on-policy | âš ï¸ |

---

## 2. KIáº¾N TRÃšC THUáº¬T TOÃN

### 2.1 CÃ¡c ThÃ nh Pháº§n ChÃ­nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DQN ARCHITECTURE                          â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Q-Network  â”‚    â”‚   Target    â”‚    â”‚  Replay Buffer   â”‚  â”‚
â”‚  â”‚  (online)   â”‚    â”‚  Network    â”‚    â”‚  (100K samples)  â”‚  â”‚
â”‚  â”‚  Î¸ â†’ update â”‚    â”‚  Î¸â»â†’ frozen â”‚    â”‚  random sample   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                    â”‚             â”‚
â”‚         â”‚    copy every    â”‚                    â”‚             â”‚
â”‚         â”‚   1000 steps     â”‚                    â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚             â”‚
â”‚                                                  â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â”‚  Îµ-greedy: random(Îµ) or argmax Q(s,a)(1-Îµ)               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Neural Network Architecture

```
Input (8)  â†’  Linear(8, 256) â†’ ReLU â†’ Dropout(0.1)
           â†’  Linear(256, 256) â†’ ReLU â†’ Dropout(0.1)
           â†’  Linear(256, 128) â†’ ReLU â†’ Dropout(0.1)
           â†’  Linear(128, 5) â†’ Q-values (no activation)

Output: Q(s, aâ‚€), Q(s, aâ‚), Q(s, aâ‚‚), Q(s, aâ‚ƒ), Q(s, aâ‚„)
```

**Giáº£i thÃ­ch:**

- **ReLU**: `f(x) = max(0, x)` â€” non-linearity, trÃ¡nh vanishing gradient
- **Dropout(0.1)**: Regularization, trÃ¡nh overfitting
- **No activation á»Ÿ output**: Q-values cÃ³ thá»ƒ Ã¢m hoáº·c dÆ°Æ¡ng
- **Xavier initialization**: Weights khá»Ÿi táº¡o cÃ¢n báº±ng

### 2.3 Experience Replay Buffer

```
Táº¡i sao cáº§n?
- Samples liÃªn tiáº¿p cÃ³ correlation cao â†’ unstable training
- Replay buffer phÃ¡ vá»¡ correlation báº±ng random sampling

Hoáº¡t Ä‘á»™ng:
1. Agent tÆ°Æ¡ng tÃ¡c vá»›i env â†’ thu (s, a, r, s', done)
2. LÆ°u vÃ o buffer (size = 100,000)
3. Random sample batch (size = 64) Ä‘á»ƒ training
4. Má»—i sample Ä‘Æ°á»£c há»c nhiá»u láº§n (off-policy)
```

### 2.4 Target Network

```
Váº¥n Ä‘á»: Q_target = r + Î³ Ã— max Q(s', a')
         â†’ Q dÃ¹ng chÃ­nh nÃ³ Ä‘á»ƒ tÃ­nh target â†’ oscillation!

Giáº£i phÃ¡p: DÃ¹ng 2 máº¡ng riÃªng biá»‡t
- Q-Network (Î¸): Update liÃªn tá»¥c má»—i step
- Target Network (Î¸â»): Copy tá»« Q-Network má»—i 1000 steps

â†’ Target á»•n Ä‘á»‹nh hÆ¡n â†’ Training stable hÆ¡n
```

### 2.5 Double DQN (Cáº£i tiáº¿n)

```
Vanilla DQN:  y = r + Î³ Ã— max_a' Q_target(s', a')
              â†’ Overestimation bias (Q-values bá»‹ inflate)

Double DQN:   a* = argmax_a' Q_online(s', a')    â† chá»n action báº±ng online
              y  = r + Î³ Ã— Q_target(s', a*)       â† Ä‘Ã¡nh giÃ¡ báº±ng target

â†’ Giáº£m overestimation â†’ Q-values chÃ­nh xÃ¡c hÆ¡n
```

---

## 3. CÃCH CHáº Y DQN

### 3.1 Files LiÃªn Quan

| File | MÃ´ táº£ |
|------|--------|
| `Microgrid_DQN_Colab.ipynb` | Notebook Ä‘áº§y Ä‘á»§ cho Google Colab |
| `Microgrid_DQN_Colab.py` | Source code Python tÆ°Æ¡ng á»©ng |
| `Microgrid_DQN_Simple.ipynb` | Notebook Ä‘Æ¡n giáº£n (3 bÆ°á»›c) |
| `run_training.py` | Script cháº¡y local |

### 3.2 Cháº¡y TrÃªn Google Colab (Khuyáº¿n nghá»‹)

**BÆ°á»›c 1**: Upload notebook lÃªn Google Colab

```
1. Má»Ÿ https://colab.research.google.com
2. File â†’ Upload notebook â†’ chá»n Microgrid_DQN_Simple.ipynb
3. Runtime â†’ Change runtime type â†’ GPU (T4)
```

**BÆ°á»›c 2**: CÃ¡ nhÃ¢n hÃ³a tham sá»‘

```python
# Thay Ä‘á»•i cÃ¡c giÃ¡ trá»‹ nÃ y cho bÃ i lÃ m riÃªng
SEED = 42              # Má»—i SV chá»n seed khÃ¡c: 42, 123, 456, 789, 999
EPISODES = 100         # Sá»‘ episodes: 50-500
LEARNING_RATE = 0.0001 # Learning rate: 0.0001-0.001
```

**BÆ°á»›c 3**: Cháº¡y 3 Ã´ theo thá»© tá»±

```
Ã” 1 (ğŸ“¦ CÃ i Äáº·t): CÃ i thÆ° viá»‡n + táº¡o env + agent    (~10 giÃ¢y)
Ã” 2 (ğŸš€ Huáº¥n Luyá»‡n): Training DQN agent               (~30 giÃ¢y)
Ã” 3 (ğŸ“Š Káº¿t Quáº£): Xem Ä‘á»“ thá»‹ + so sÃ¡nh               (~5 giÃ¢y)
```

### 3.3 Cháº¡y Local (Optional)

```bash
# CÃ i Ä‘áº·t
pip install torch numpy matplotlib

# Cháº¡y training
python run_training.py

# Output: evaluation_results/ (chá»©a biá»ƒu Ä‘á»“)
```

---

## 4. QUÃ TRÃŒNH TRAINING

### 4.1 Training Loop (Pseudocode)

```python
for episode in range(500):
    state = env.reset()
    
    for step in range(24):  # 24 giá» má»—i ngÃ y
        # 1. Chá»n action (Îµ-greedy)
        if random() < epsilon:
            action = random_action()        # Explore
        else:
            action = argmax(Q(state))        # Exploit
        
        # 2. Thá»±c hiá»‡n action
        next_state, reward, done = env.step(action)
        
        # 3. LÆ°u vÃ o replay buffer
        buffer.push(state, action, reward, next_state, done)
        
        # 4. Sample batch vÃ  update Q-network
        batch = buffer.sample(64)
        target = reward + Î³ Ã— max Q_target(next_state)
        loss = MSE(Q(state, action), target)
        optimizer.step()
        
        # 5. Copy weights sang target network (má»—i 1000 steps)
        if step_count % 1000 == 0:
            target_network â† q_network
    
    # 6. Giáº£m epsilon
    epsilon = max(0.01, epsilon Ã— 0.995)
```

### 4.2 Epsilon Decay Schedule

```
Îµ = 1.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€\
                    \
                     \          â† Îµ Ã— 0.995 má»—i episode
                      \
                       \_______ Îµ_min = 0.01
Episode:  0    100   200   300   400   500

Ã nghÄ©a:
- Îµ = 1.0: 100% random â†’ khÃ¡m phÃ¡ toÃ n bá»™ action space
- Îµ = 0.5: 50% random, 50% best action â†’ cÃ¢n báº±ng explore/exploit
- Îµ = 0.01: 1% random â†’ chá»§ yáº¿u exploit policy Ä‘Ã£ há»c
```

### 4.3 CÃ´ng Thá»©c Cáº­p Nháº­t

```
1. Q-value target:
   y = r + Î³ Ã— max_a' Q_target(s', a')       (Î³ = 0.99)

2. Loss function:
   L(Î¸) = E[(Q_Î¸(s, a) - y)Â²]                (MSE Loss)

3. Gradient descent:
   Î¸ â† Î¸ - Î± Ã— âˆ‡_Î¸ L(Î¸)                     (Î± = 0.0001)

4. Target network sync:
   Î¸â» â† Î¸     (má»—i 1000 steps)
```

---

## 5. HYPERPARAMETERS

| Parameter | Value | Ã nghÄ©a | Gá»£i Ã½ thay Ä‘á»•i |
|-----------|-------|---------|----------------|
| Learning rate | 1e-4 | Tá»‘c Ä‘á»™ há»c | 1e-4 ~ 1e-3 |
| Gamma (Î³) | 0.99 | Discount factor | 0.95 ~ 0.99 |
| Epsilon start | 1.0 | Exploration ban Ä‘áº§u | 0.9 ~ 1.0 |
| Epsilon end | 0.01 | Exploration tá»‘i thiá»ƒu | 0.01 ~ 0.05 |
| Epsilon decay | 0.995 | Tá»‘c Ä‘á»™ giáº£m Îµ | 0.990 ~ 0.998 |
| Batch size | 64 | KÃ­ch thÆ°á»›c mini-batch | 32, 64, 128 |
| Buffer size | 100,000 | Replay buffer capacity | 50K ~ 200K |
| Target update | 1000 steps | Táº§n suáº¥t sync target | 500 ~ 2000 |
| Hidden layers | [256, 256, 128] | Kiáº¿n trÃºc máº¡ng | Thay Ä‘á»•i kÃ­ch thÆ°á»›c |
| Episodes | 500 | Sá»‘ láº§n train | 100 ~ 700 |

---

## 6. Káº¾T QUáº¢ ÄÃNH GIÃ

### 6.1 Training Convergence

```
Training Progress:
Episode   10 | Reward:  -3.10 | Îµ: 0.951  â† KhÃ¡m phÃ¡ (explore)
Episode   50 | Reward:  -1.87 | Îµ: 0.778  â† Báº¯t Ä‘áº§u há»c
Episode  100 | Reward:  +2.62 | Îµ: 0.606  â† ChÃ­nh sÃ¡ch cáº£i thiá»‡n
Episode  200 | Reward:  +8.45 | Îµ: 0.367  â† Gáº§n optimal
Episode  500 | Reward: +13.37 | Îµ: 0.010  â† Converged
```

### 6.2 So SÃ¡nh Agent vs Random

| Metric | DQN Agent | Random | Improvement |
|--------|-----------|--------|-------------|
| Mean Reward | +14.75 | -3.34 | **+541%** |
| Daily Cost | $1.26 | $16.42 | **-92.3%** |
| Renewable Usage | 82.5% | 47.8% | **+34.7pp** |
| Unmet Demand | 3.4% | 16.1% | **-12.7pp** |

### 6.3 Policy Há»c ÄÆ°á»£c (24h)

| Giá» | HÃ nh vi | LÃ½ do |
|-----|---------|-------|
| 0-6 (ÄÃªm) | Renewable+Grid | GiÃ³ máº¡nh, giÃ¡ grid tháº¥p â†’ mua grid ráº» |
| 7-9 (SÃ¡ng) | Renewable+Discharge | Peak price â†’ xáº£ pin thay vÃ¬ mua grid |
| 10-14 (TrÆ°a) | Charge | Solar cao nháº¥t â†’ sáº¡c pin Ä‘áº§y |
| 15-17 (Chiá»u) | Mixed | Chuyá»ƒn tiáº¿p, duy trÃ¬ pin |
| 18-21 (Tá»‘i) | Renewable+Discharge | Peak price â†’ xáº£ pin tá»‘i Ä‘a |
| 22-23 (Khuya) | Discharge | GiÃ¡ giáº£m, dÃ¹ng ná»‘t pin dÆ° |

---

## 7. Æ¯U ÄIá»‚M VÃ€ Háº N CHáº¾ Cá»¦A DQN

### 7.1 Æ¯u Äiá»ƒm

- âœ… **Sample efficient**: Replay buffer cho phÃ©p há»c tá»« data cÅ© (off-policy)
- âœ… **Stable training**: Target network + replay buffer giáº£m oscillation
- âœ… **Proven**: ÄÃ£ Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£ trÃªn nhiá»u bÃ i toÃ¡n (Atari games, robotics)
- âœ… **ÄÆ¡n giáº£n**: Dá»… implement vÃ  debug

### 7.2 Háº¡n Cháº¿

- âŒ **Chá»‰ discrete actions**: KhÃ´ng thá»ƒ control chÃ­nh xÃ¡c kW (pháº£i dÃ¹ng DDPG/SAC)
- âŒ **Overestimation**: Q-values cÃ³ thá»ƒ bá»‹ inflate (Double DQN giáº£m nhÆ°ng khÃ´ng loáº¡i bá» hoÃ n toÃ n)
- âŒ **Memory intensive**: Replay buffer chiáº¿m nhiá»u RAM
- âŒ **KhÃ´ng song song**: KhÃ³ parallelize training

---

## 8. TÃ€I LIá»†U THAM KHáº¢O

1. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." *Nature*, 518(7540), 529-533.
2. Van Hasselt, H., et al. (2016). "Deep Reinforcement Learning with Double Q-learning." *AAAI*.
3. Lin, L.J. (1992). "Self-improving reactive agents based on RL, planning and teaching." *Machine Learning*, 8(3-4), 293-321.
4. FranÃ§ois-Lavet, V., et al. (2018). "An Introduction to Deep Reinforcement Learning." *Foundations and Trends in ML*.
