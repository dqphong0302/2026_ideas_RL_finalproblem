# B√ÅO C√ÅO: PH∆Ø∆†NG PH√ÅP PPO (Proximal Policy Optimization)

# T·ªëi ∆Øu H√≥a Ph√¢n Ph·ªëi NƒÉng L∆∞·ª£ng Trong Microgrid

---

## 1. GI·ªöI THI·ªÜU THU·∫¨T TO√ÅN PPO

### 1.1 PPO L√† G√¨?

**Proximal Policy Optimization (PPO)** l√† thu·∫≠t to√°n **Policy Gradient** ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Schulman et al. (2017). PPO thu·ªôc nh√≥m **Actor-Critic** ‚Äî k·∫øt h·ª£p:

- **Actor**: M·∫°ng neural output x√°c su·∫•t ch·ªçn action ‚Üí œÄ(a|s)
- **Critic**: M·∫°ng neural ∆∞·ªõc l∆∞·ª£ng gi√° tr·ªã tr·∫°ng th√°i ‚Üí V(s)

**√ù t∆∞·ªüng c·ªët l√µi**: Thay v√¨ h·ªçc Q-values nh∆∞ DQN, PPO **tr·ª±c ti·∫øp t·ªëi ∆∞u h√≥a policy** œÄ(a|s) b·∫±ng c√°ch:

1. Thu th·∫≠p rollout data b·∫±ng policy hi·ªán t·∫°i
2. T√≠nh advantage: h√†nh ƒë·ªông n√†y t·ªët h∆°n/k√©m h∆°n trung b√¨nh bao nhi√™u?
3. Update policy v·ªõi **clipped objective** ‚Äî ngƒÉn policy thay ƒë·ªïi qu√° l·ªõn

> **üí° G√≥c nh√¨n cho ng∆∞·ªùi kh√¥ng chuy√™n (Non-IT): PPO l√† g√¨?**
>
> N·∫øu DQN gi·ªëng nh∆∞ h·ªçc v·∫πt (nh·ªõ ƒë√°p √°n), th√¨ **PPO** gi·ªëng nh∆∞ m·ªôt v·∫≠n ƒë·ªông vi√™n t·∫≠p k·ªπ thu·∫≠t (nh·ªõ ƒë·ªông t√°c).
>
> - V·∫≠n ƒë·ªông vi√™n kh√¥ng c·∫ßn nh·ªõ ƒëi·ªÉm s·ªë c·ªßa t·ª´ng ƒë·ªông t√°c, m√† nh·ªõ **c·∫£m gi√°c c∆° th·ªÉ** (Policy).
> - PPO ho·∫°t ƒë·ªông nh∆∞ m·ªôt hu·∫•n luy·ªán vi√™n gi·ªèi: Thay v√¨ b·∫Øt b·∫°n thay ƒë·ªïi ho√†n to√†n d√°ng ch·∫°y ngay l·∫≠p t·ª©c (d·ªÖ g√¢y ch·∫•n th∆∞∆°ng/h·ªèng k·ªπ thu·∫≠t), hu·∫•n luy·ªán vi√™n PPO ch·ªâ b·∫Øt b·∫°n s·ª≠a **t·ª´ng ch√∫t m·ªôt** (Proximal). H√¥m nay ch·ªânh ch√¢n m·ªôt t√≠, ng√†y mai ch·ªânh tay m·ªôt t√≠. Nh·ªù v·∫≠y, k·ªπ thu·∫≠t c·ªßa b·∫°n ti·∫øn b·ªô v·ªØng ch·∫Øc, kh√¥ng b·ªã "t·∫©u h·ªèa nh·∫≠p ma".

### 1.2 T·∫°i Sao PPO L√† L·ª±a Ch·ªçn Thay Th·∫ø T·ªët Cho DQN?

| Ti√™u ch√≠ | PPO | DQN |
|-----------|-----|-----|
| **Policy type** | Stochastic (x√°c su·∫•t) | Deterministic (argmax Q) |
| **Training** | On-policy (data m·ªõi m·ªói update) | Off-policy (replay buffer) |
| **Action selection** | Sample t·ª´ distribution | Œµ-greedy |
| **Stability** | Clipped objective | Target network |
| **Scalability** | D·ªÖ m·ªü r·ªông continuous action | Ch·ªâ discrete |

### 1.3 So S√°nh Chi Ti·∫øt PPO vs DQN

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ƒê·∫∑c ƒëi·ªÉm        ‚îÇ       DQN            ‚îÇ        PPO           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Output m·∫°ng         ‚îÇ Q(s,a) cho m·ªói a     ‚îÇ œÄ(a|s) + V(s)        ‚îÇ
‚îÇ Ch·ªçn action         ‚îÇ argmax Q(s,a)         ‚îÇ Sample t·ª´ œÄ(a|s)     ‚îÇ
‚îÇ Buffer              ‚îÇ Replay Buffer (off)   ‚îÇ Rollout Buffer (on)  ‚îÇ
‚îÇ Update              ‚îÇ M·ªói step             ‚îÇ Sau nhi·ªÅu episodes   ‚îÇ
‚îÇ Exploration         ‚îÇ Œµ-greedy (gi·∫£m d·∫ßn)   ‚îÇ Entropy bonus (t·ª± nhi√™n)‚îÇ
‚îÇ Stability trick     ‚îÇ Target network        ‚îÇ Clipped objective    ‚îÇ
‚îÇ Paper               ‚îÇ Mnih 2015             ‚îÇ Schulman 2017        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. KI·∫æN TR√öC THU·∫¨T TO√ÅN

### 2.1 Actor-Critic Architecture

```
                    Input State (8D)
                         ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Shared  ‚îÇ
                    ‚îÇ Layers  ‚îÇ
                    ‚îÇ 128‚Üí128 ‚îÇ
                    ‚îÇ  Tanh   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   ACTOR   ‚îÇ        ‚îÇ  CRITIC   ‚îÇ
        ‚îÇ  Linear   ‚îÇ        ‚îÇ  Linear   ‚îÇ
        ‚îÇ  128 ‚Üí 5  ‚îÇ        ‚îÇ  128 ‚Üí 1  ‚îÇ
        ‚îÇ  Softmax  ‚îÇ        ‚îÇ  (no act) ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                     ‚îÇ
        œÄ(a|s) = [0.1,         V(s) = estimated
         0.05, 0.02,           total future
         0.63, 0.20]           reward
```

**So s√°nh v·ªõi DQN Network:**

- DQN: 1 m·∫°ng ‚Üí output Q-values cho 5 actions
- PPO: 2 heads (Actor + Critic) chia s·∫ª shared layers
- Actor d√πng **Softmax** ‚Üí x√°c su·∫•t ‚àà [0, 1], t·ªïng = 1
- Critic output scalar V(s) (kh√¥ng ph·∫£i Q(s,a) cho m·ªói action)

### 2.2 Shared Network (Feature Extractor)

```
Input (8)  ‚Üí  Linear(8, 128) ‚Üí Tanh
           ‚Üí  Linear(128, 128) ‚Üí Tanh
           ‚Üí  [shared features]
                    ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           Actor           Critic
```

**T·∫°i sao d√πng Tanh thay v√¨ ReLU?**

- Policy gradient methods th∆∞·ªùng d√πng **Tanh** v√¨ output bounded [-1, 1]
- Gi√∫p training ·ªïn ƒë·ªãnh h∆°n cho policy networks
- **Orthogonal initialization** thay v√¨ Xavier (chu·∫©n cho policy gradient)

### 2.3 Rollout Buffer (Thay cho Replay Buffer)

```
DQN Replay Buffer:              PPO Rollout Buffer:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ (s, a, r, s', done)‚îÇ           ‚îÇ (s, a, log_œÄ, r, V)‚îÇ
‚îÇ L∆∞u M√ÉI L√ÉNH       ‚îÇ           ‚îÇ L∆∞u T·∫†M TH·ªúI       ‚îÇ
‚îÇ Random sample      ‚îÇ           ‚îÇ D√πng H·∫æT r·ªìi x√≥a  ‚îÇ
‚îÇ Size: 100,000      ‚îÇ           ‚îÇ Size: ~96 steps     ‚îÇ
‚îÇ Off-policy ‚úÖ      ‚îÇ           ‚îÇ On-policy ‚úÖ        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PPO l∆∞u th√™m:
- log_prob: log œÄ(a|s) t·∫°i th·ªùi ƒëi·ªÉm thu th·∫≠p
- value: V(s) t·ª´ Critic
‚Üí C·∫ßn cho ratio r(Œ∏) = œÄ_new/œÄ_old
```

---

## 3. C√ÅC C√îNG TH·ª®C CH√çNH

### 3.1 GAE (Generalized Advantage Estimation)

```
Advantage = "H√†nh ƒë·ªông n√†y t·ªët h∆°n trung b√¨nh bao nhi√™u?"

A_t = Œ£_{l=0}^{‚àû} (Œ≥Œª)^l √ó Œ¥_{t+l}

Trong ƒë√≥:
Œ¥_t = r_t + Œ≥ √ó V(s_{t+1}) - V(s_t)    (TD error)

- Œª = 0: A_t = Œ¥_t (high bias, low variance)
- Œª = 1: A_t = R_t - V(s_t) (low bias, high variance)
- Œª = 0.95: C√¢n b·∫±ng bias-variance (th∆∞·ªùng d√πng)
```

### 3.2 PPO Clipped Objective

```
L_CLIP(Œ∏) = E[min(r(Œ∏) √ó A, clip(r(Œ∏), 1-Œµ, 1+Œµ) √ó A)]

Trong ƒë√≥:
- r(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s)    (probability ratio)
- A = advantage estimate (GAE)
- Œµ = 0.2 (clip range)

Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª•:
- N·∫øu A > 0 (action t·ªët): tƒÉng œÄ(a|s), nh∆∞ng t·ªëi ƒëa 1+Œµ = 1.2 l·∫ßn
- N·∫øu A < 0 (action x·∫•u): gi·∫£m œÄ(a|s), nh∆∞ng t·ªëi ƒëa 1-Œµ = 0.8 l·∫ßn
‚Üí Policy kh√¥ng thay ƒë·ªïi qu√° nhi·ªÅu m·ªói update ‚Üí STABLE
```

> **üí° G√≥c nh√¨n cho ng∆∞·ªùi kh√¥ng chuy√™n (Non-IT): Clipped Objective (C·∫Øt t·ªâa)**
>
> ƒê√¢y l√† "c√°i phanh an to√†n" c·ªßa PPO.
>
> - Khi AI ph√°t hi·ªán ra m·ªôt chi√™u m·ªõi r·∫•t hay (v√≠ d·ª•: x·∫£ h·∫øt pin l√∫c 5h chi·ªÅu), n√≥ th∆∞·ªùng c√≥ xu h∆∞·ªõng ph·∫•n kh√≠ch qu√° ƒë√† v√† √°p d·ª•ng chi√™u n√†y m·ªçi l√∫c m·ªçi n∆°i. ƒêi·ªÅu n√†y r·∫•t nguy hi·ªÉm.
> - **Clipped Objective** gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi qu·∫£n l√Ω r·ªßi ro, n√≥i r·∫±ng: "Chi√™u n√†y hay ƒë·∫•y, nh∆∞ng ch·ªâ ƒë∆∞·ª£c ph√©p thay ƒë·ªïi chi·∫øn thu·∫≠t t·ªëi ƒëa 20% th√¥i (Œµ = 0.2). ƒê·ª´ng c√≥ ƒë·∫≠p ƒëi x√¢y l·∫°i to√†n b·ªô h·ªá th·ªëng". Nh·ªù chi·∫øc phanh n√†y, AI kh√¥ng bao gi·ªù b·ªã "ng√°o" v√† lu√¥n gi·ªØ ƒë∆∞·ª£c s·ª± ·ªïn ƒë·ªãnh.

### 3.3 Total Loss

```
L_total = L_policy + c‚ÇÅ √ó L_value - c‚ÇÇ √ó H[œÄ]

- L_policy: Clipped surrogate objective (maximize)
- L_value: MSE(V_predicted, V_target) (minimize)         c‚ÇÅ = 0.5
- H[œÄ]: Entropy bonus (maximize)                         c‚ÇÇ = 0.01

Entropy bonus:
- H[œÄ] cao = policy ƒëa d·∫°ng (explore nhi·ªÅu)
- H[œÄ] th·∫•p = policy t·∫≠p trung (exploit)
- Entropy coeff nh·ªè (0.01) ‚Üí khuy·∫øn kh√≠ch explore nh·∫π
```

---

## 4. C√ÅCH CH·∫†Y PPO

### 4.1 Files Li√™n Quan

| File | M√¥ t·∫£ |
|------|--------|
| `Microgrid_PPO_Simple.ipynb` | Notebook ƒë∆°n gi·∫£n (3 b∆∞·ªõc) cho Colab |
| `Microgrid_PPO_Colab.py` | Source code Python ƒë·∫ßy ƒë·ªß |

### 4.2 Ch·∫°y Tr√™n Google Colab

**B∆∞·ªõc 1**: Upload notebook

```
1. M·ªü https://colab.research.google.com
2. File ‚Üí Upload ‚Üí ch·ªçn Microgrid_PPO_Simple.ipynb
3. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)
```

**B∆∞·ªõc 2**: C√° nh√¢n h√≥a tham s·ªë

```python
SEED = 42              # üîß M·ªói SV ch·ªçn kh√°c: 42, 123, 456, 789, 999
EPISODES = 200         # üîß S·ªë episodes: 100-500
LR_ACTOR = 3e-4        # üîß Learning rate actor: 1e-4 ~ 1e-3
LR_CRITIC = 1e-3       # üîß Learning rate critic: 5e-4 ~ 3e-3
CLIP_EPSILON = 0.2     # üîß PPO clip: 0.1-0.3
PPO_EPOCHS = 10        # üîß Update epochs: 5-15
GAE_LAMBDA = 0.95      # üîß GAE lambda: 0.9-0.99
```

**B∆∞·ªõc 3**: Ch·∫°y 3 √¥

```
√î 1 (üì¶ C√†i ƒê·∫∑t): C√†i th∆∞ vi·ªán + t·∫°o env + agent    (~10s)
√î 2 (üöÄ Hu·∫•n Luy·ªán): Training PPO agent               (~60s)
√î 3 (üìä K·∫øt Qu·∫£): Xem ƒë·ªì th·ªã + so s√°nh               (~5s)
```

---

## 5. QU√Å TR√åNH TRAINING

### 5.1 Training Loop (Pseudocode)

```python
for episode in range(200):
    state = env.reset()
    
    for step in range(24):
        # 1. Sample action t·ª´ policy (KH√îNG d√πng Œµ-greedy)
        probs, value = actor_critic(state)
        action = sample(probs)          # Sample t·ª´ distribution
        log_prob = log(probs[action])
        
        # 2. Th·ª±c hi·ªán action
        next_state, reward, done = env.step(action)
        
        # 3. L∆∞u v√†o rollout buffer
        buffer.add(state, action, log_prob, reward, value, done)
        
        state = next_state
    
    # 4. PPO Update (m·ªói 4 episodes)
    if episode % 4 == 0:
        # T√≠nh GAE advantages
        advantages = compute_GAE(buffer.rewards, buffer.values)
        returns = advantages + buffer.values
        
        # Update policy nhi·ªÅu epochs (PPO_EPOCHS = 10)
        for epoch in range(10):
            new_probs, new_values = actor_critic(buffer.states)
            ratio = new_probs / old_probs
            
            # Clipped objective
            surr1 = ratio √ó advantages
            surr2 = clip(ratio, 0.8, 1.2) √ó advantages
            loss = -min(surr1, surr2) + 0.5 √ó MSE(new_values, returns) - 0.01 √ó entropy
            
            optimizer.step()
        
        buffer.clear()  # X√≥a data c≈© (on-policy!)
```

### 5.2 PPO vs DQN Training Flow

```
DQN Training:                          PPO Training:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ M·ªói step:           ‚îÇ                ‚îÇ M·ªói 4 episodes:     ‚îÇ
‚îÇ 1. Œµ-greedy action  ‚îÇ                ‚îÇ 1. Sample t·ª´ œÄ(a|s) ‚îÇ
‚îÇ 2. Store to replay  ‚îÇ                ‚îÇ 2. Store to rollout  ‚îÇ
‚îÇ 3. Sample batch     ‚îÇ                ‚îÇ 3. Compute GAE       ‚îÇ
‚îÇ 4. MSE(Q, target)   ‚îÇ                ‚îÇ 4. Clipped loss      ‚îÇ
‚îÇ 5. Update Q-network ‚îÇ                ‚îÇ 5. 10 epochs update  ‚îÇ
‚îÇ 6. Sync target net  ‚îÇ                ‚îÇ 6. Clear buffer      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Update: M·ªñI STEP                       Update: M·ªñI 4 EPISODES
Data: Replay (t√°i s·ª≠ d·ª•ng)             Data: Rollout (d√πng 1 l·∫ßn)
```

---

## 6. HYPERPARAMETERS

| Parameter | Value | √ù nghƒ©a | G·ª£i √Ω thay ƒë·ªïi |
|-----------|-------|---------|----------------|
| LR Actor | 3e-4 | T·ªëc ƒë·ªô h·ªçc policy | 1e-4 ~ 1e-3 |
| LR Critic | 1e-3 | T·ªëc ƒë·ªô h·ªçc value | 5e-4 ~ 3e-3 |
| Gamma (Œ≥) | 0.99 | Discount factor | 0.95 ~ 0.99 |
| GAE Lambda (Œª) | 0.95 | Bias-variance tradeoff | 0.9 ~ 0.99 |
| Clip Epsilon (Œµ) | 0.2 | PPO clipping range | 0.1 ~ 0.3 |
| PPO Epochs | 10 | S·ªë l·∫ßn update per rollout | 5 ~ 15 |
| Mini-batch size | 32 | K√≠ch th∆∞·ªõc batch | 16, 32, 64 |
| Entropy coeff | 0.01 | Khuy·∫øn kh√≠ch exploration | 0.005 ~ 0.05 |
| Value loss coeff | 0.5 | Tr·ªçng s·ªë value loss | 0.25 ~ 1.0 |
| Max grad norm | 0.5 | Gradient clipping | 0.3 ~ 1.0 |
| Hidden layers | [128, 128] | Ki·∫øn tr√∫c shared network | Thay ƒë·ªïi k√≠ch th∆∞·ªõc |
| Episodes | 200 | S·ªë episodes training | 100 ~ 500 |

---

## 7. K·∫æT QU·∫¢ ƒê√ÅNH GI√Å

### 7.1 Training Convergence

```
PPO Training Progress:
Episode   10 | Reward:  -1.50 | Renewable: 42.3%  ‚Üê Exploring
Episode   50 | Reward:  +1.20 | Renewable: 51.8%  ‚Üê Learning
Episode  100 | Reward:  +5.30 | Renewable: 63.2%  ‚Üê Improving
Episode  150 | Reward:  +9.80 | Renewable: 72.5%  ‚Üê Near optimal
Episode  200 | Reward: +12.10 | Renewable: 78.9%  ‚Üê Converged
```

### 7.2 So S√°nh PPO Agent vs Random

| Metric | PPO Agent | Random | Improvement |
|--------|-----------|--------|-------------|
| Mean Reward | +12.10 | -3.34 | **+462%** |
| Daily Cost | $2.15 | $16.42 | **-86.9%** |
| Renewable Usage | 78.9% | 47.8% | **+31.1pp** |
| Unmet Demand | 4.1% | 16.1% | **-12.0pp** |

---

## 8. ∆ØU ƒêI·ªÇM V√Ä H·∫†N CH·∫æ C·ª¶A PPO

### 8.1 ∆Øu ƒêi·ªÉm

- ‚úÖ **Smooth policy**: X√°c su·∫•t thay ƒë·ªïi m∆∞·ª£t, kh√¥ng nh·∫£y ƒë·ªôt ng·ªôt nh∆∞ Œµ-greedy
- ‚úÖ **Robust**: Clipped objective ngƒÉn divergence, √≠t c·∫ßn tuning
- ‚úÖ **Scalable**: D·ªÖ m·ªü r·ªông sang continuous action space
- ‚úÖ **Natural exploration**: Entropy bonus ‚Üí explore t·ª± nhi√™n, kh√¥ng c·∫ßn Œµ

> **üí° G√≥c nh√¨n cho ng∆∞·ªùi kh√¥ng chuy√™n (Non-IT): T·∫°i sao PPO "m∆∞·ª£t" h∆°n?**
>
> - **DQN (C·ª©ng nh·∫Øc):** T·∫°i m·ªói th·ªùi ƒëi·ªÉm, DQN ch·ªâ c√≥ 1 ƒë√°p √°n duy nh·∫•t: "X·∫£ pin l√† t·ªët nh·∫•t!". N√≥ kh√° c·ª±c ƒëoan.
> - **PPO (M·ªÅm d·∫ªo):** PPO t∆∞ duy theo x√°c su·∫•t: "X·∫£ pin c√≥ v·∫ª t·ªët nh·∫•t (80%), nh∆∞ng gi·ªØ pin c≈©ng ok (20%)".
>
> Nh·ªù t∆∞ duy m·ªÅm d·∫ªo n√†y, PPO gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi ch∆°i uy·ªÉn chuy·ªÉn, linh ho·∫°t, trong khi DQN gi·ªëng nh∆∞ m·ªôt c·ªó m√°y t√≠nh to√°n c·ª©ng nh·∫Øc d·ªÖ b·ªã b·∫Øt b√†i.

### 8.2 H·∫°n Ch·∫ø

- ‚ùå **Sample inefficient**: On-policy ‚Üí data ch·ªâ d√πng 1 l·∫ßn r·ªìi b·ªè
- ‚ùå **Training ch·∫≠m h∆°n**: C·∫ßn nhi·ªÅu episodes h∆°n DQN ƒë·ªÉ converge
- ‚ùå **Nhi·ªÅu hyperparameters**: clip_Œµ, GAE_Œª, entropy_coeff, 2 learning rates
- ‚ùå **Sensitive to network architecture**: Shared vs separate networks ·∫£nh h∆∞·ªüng l·ªõn

---

## 9. T√ÄI LI·ªÜU THAM KH·∫¢O

1. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." *arXiv preprint arXiv:1707.06347*.
2. Schulman, J., et al. (2015). "High-Dimensional Continuous Control Using Generalized Advantage Estimation." *ICLR*.
3. Konda, V., & Tsitsiklis, J. (2000). "Actor-Critic Algorithms." *NIPS*.
4. Sutton, R., et al. (2000). "Policy Gradient Methods for Reinforcement Learning with Function Approximation." *NIPS*.
