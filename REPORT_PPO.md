# BÃO CÃO: PHÆ¯Æ NG PHÃP PPO (Proximal Policy Optimization)

# Tá»‘i Æ¯u HÃ³a PhÃ¢n Phá»‘i NÄƒng LÆ°á»£ng Trong Microgrid

---

## 1. GIá»šI THIá»†U THUáº¬T TOÃN PPO

### 1.1 PPO LÃ  GÃ¬?

**Proximal Policy Optimization (PPO)** lÃ  thuáº­t toÃ¡n **Policy Gradient** Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Schulman et al. (2017). PPO thuá»™c nhÃ³m **Actor-Critic** â€” káº¿t há»£p:

- **Actor**: Máº¡ng neural output xÃ¡c suáº¥t chá»n action â†’ Ï€(a|s)
- **Critic**: Máº¡ng neural Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ tráº¡ng thÃ¡i â†’ V(s)

**Ã tÆ°á»Ÿng cá»‘t lÃµi**: Thay vÃ¬ há»c Q-values nhÆ° DQN, PPO **trá»±c tiáº¿p tá»‘i Æ°u hÃ³a policy** Ï€(a|s) báº±ng cÃ¡ch:

1. Thu tháº­p rollout data báº±ng policy hiá»‡n táº¡i
2. TÃ­nh advantage: hÃ nh Ä‘á»™ng nÃ y tá»‘t hÆ¡n/kÃ©m hÆ¡n trung bÃ¬nh bao nhiÃªu?
3. Update policy vá»›i **clipped objective** â€” ngÄƒn policy thay Ä‘á»•i quÃ¡ lá»›n

### 1.2 Táº¡i Sao PPO LÃ  Lá»±a Chá»n Thay Tháº¿ Tá»‘t Cho DQN?

| TiÃªu chÃ­ | PPO | DQN |
|-----------|-----|-----|
| **Policy type** | Stochastic (xÃ¡c suáº¥t) | Deterministic (argmax Q) |
| **Training** | On-policy (data má»›i má»—i update) | Off-policy (replay buffer) |
| **Action selection** | Sample tá»« distribution | Îµ-greedy |
| **Stability** | Clipped objective | Target network |
| **Scalability** | Dá»… má»Ÿ rá»™ng continuous action | Chá»‰ discrete |

### 1.3 So SÃ¡nh Chi Tiáº¿t PPO vs DQN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Äáº·c Ä‘iá»ƒm        â”‚       DQN            â”‚        PPO           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output máº¡ng         â”‚ Q(s,a) cho má»—i a     â”‚ Ï€(a|s) + V(s)        â”‚
â”‚ Chá»n action         â”‚ argmax Q(s,a)         â”‚ Sample tá»« Ï€(a|s)     â”‚
â”‚ Buffer              â”‚ Replay Buffer (off)   â”‚ Rollout Buffer (on)  â”‚
â”‚ Update              â”‚ Má»—i step             â”‚ Sau nhiá»u episodes   â”‚
â”‚ Exploration         â”‚ Îµ-greedy (giáº£m dáº§n)   â”‚ Entropy bonus (tá»± nhiÃªn)â”‚
â”‚ Stability trick     â”‚ Target network        â”‚ Clipped objective    â”‚
â”‚ Paper               â”‚ Mnih 2015             â”‚ Schulman 2017        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. KIáº¾N TRÃšC THUáº¬T TOÃN

### 2.1 Actor-Critic Architecture

```
                    Input State (8D)
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚ Shared  â”‚
                    â”‚ Layers  â”‚
                    â”‚ 128â†’128 â”‚
                    â”‚  Tanh   â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
        â”‚   ACTOR   â”‚        â”‚  CRITIC   â”‚
        â”‚  Linear   â”‚        â”‚  Linear   â”‚
        â”‚  128 â†’ 5  â”‚        â”‚  128 â†’ 1  â”‚
        â”‚  Softmax  â”‚        â”‚  (no act) â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                     â”‚
        Ï€(a|s) = [0.1,         V(s) = estimated
         0.05, 0.02,           total future
         0.63, 0.20]           reward
```

**So sÃ¡nh vá»›i DQN Network:**

- DQN: 1 máº¡ng â†’ output Q-values cho 5 actions
- PPO: 2 heads (Actor + Critic) chia sáº» shared layers
- Actor dÃ¹ng **Softmax** â†’ xÃ¡c suáº¥t âˆˆ [0, 1], tá»•ng = 1
- Critic output scalar V(s) (khÃ´ng pháº£i Q(s,a) cho má»—i action)

### 2.2 Shared Network (Feature Extractor)

```
Input (8)  â†’  Linear(8, 128) â†’ Tanh
           â†’  Linear(128, 128) â†’ Tanh
           â†’  [shared features]
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
           Actor           Critic
```

**Táº¡i sao dÃ¹ng Tanh thay vÃ¬ ReLU?**

- Policy gradient methods thÆ°á»ng dÃ¹ng **Tanh** vÃ¬ output bounded [-1, 1]
- GiÃºp training á»•n Ä‘á»‹nh hÆ¡n cho policy networks
- **Orthogonal initialization** thay vÃ¬ Xavier (chuáº©n cho policy gradient)

### 2.3 Rollout Buffer (Thay cho Replay Buffer)

```
DQN Replay Buffer:              PPO Rollout Buffer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (s, a, r, s', done)â”‚           â”‚ (s, a, log_Ï€, r, V)â”‚
â”‚ LÆ°u MÃƒI LÃƒNH       â”‚           â”‚ LÆ°u Táº M THá»œI       â”‚
â”‚ Random sample      â”‚           â”‚ DÃ¹ng Háº¾T rá»“i xÃ³a  â”‚
â”‚ Size: 100,000      â”‚           â”‚ Size: ~96 steps     â”‚
â”‚ Off-policy âœ…      â”‚           â”‚ On-policy âœ…        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PPO lÆ°u thÃªm:
- log_prob: log Ï€(a|s) táº¡i thá»i Ä‘iá»ƒm thu tháº­p
- value: V(s) tá»« Critic
â†’ Cáº§n cho ratio r(Î¸) = Ï€_new/Ï€_old
```

---

## 3. CÃC CÃ”NG THá»¨C CHÃNH

### 3.1 GAE (Generalized Advantage Estimation)

```
Advantage = "HÃ nh Ä‘á»™ng nÃ y tá»‘t hÆ¡n trung bÃ¬nh bao nhiÃªu?"

A_t = Î£_{l=0}^{âˆž} (Î³Î»)^l Ã— Î´_{t+l}

Trong Ä‘Ã³:
Î´_t = r_t + Î³ Ã— V(s_{t+1}) - V(s_t)    (TD error)

- Î» = 0: A_t = Î´_t (high bias, low variance)
- Î» = 1: A_t = R_t - V(s_t) (low bias, high variance)
- Î» = 0.95: CÃ¢n báº±ng bias-variance (thÆ°á»ng dÃ¹ng)
```

### 3.2 PPO Clipped Objective

```
L_CLIP(Î¸) = E[min(r(Î¸) Ã— A, clip(r(Î¸), 1-Îµ, 1+Îµ) Ã— A)]

Trong Ä‘Ã³:
- r(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)    (probability ratio)
- A = advantage estimate (GAE)
- Îµ = 0.2 (clip range)

Giáº£i thÃ­ch báº±ng vÃ­ dá»¥:
- Náº¿u A > 0 (action tá»‘t): tÄƒng Ï€(a|s), nhÆ°ng tá»‘i Ä‘a 1+Îµ = 1.2 láº§n
- Náº¿u A < 0 (action xáº¥u): giáº£m Ï€(a|s), nhÆ°ng tá»‘i Ä‘a 1-Îµ = 0.8 láº§n
â†’ Policy khÃ´ng thay Ä‘á»•i quÃ¡ nhiá»u má»—i update â†’ STABLE
```

### 3.3 Total Loss

```
L_total = L_policy + câ‚ Ã— L_value - câ‚‚ Ã— H[Ï€]

- L_policy: Clipped surrogate objective (maximize)
- L_value: MSE(V_predicted, V_target) (minimize)         câ‚ = 0.5
- H[Ï€]: Entropy bonus (maximize)                         câ‚‚ = 0.01

Entropy bonus:
- H[Ï€] cao = policy Ä‘a dáº¡ng (explore nhiá»u)
- H[Ï€] tháº¥p = policy táº­p trung (exploit)
- Entropy coeff nhá» (0.01) â†’ khuyáº¿n khÃ­ch explore nháº¹
```

---

## 4. CÃCH CHáº Y PPO

### 4.1 Files LiÃªn Quan

| File | MÃ´ táº£ |
|------|--------|
| `Microgrid_PPO_Simple.ipynb` | Notebook Ä‘Æ¡n giáº£n (3 bÆ°á»›c) cho Colab |
| `Microgrid_PPO_Colab.py` | Source code Python Ä‘áº§y Ä‘á»§ |

### 4.2 Cháº¡y TrÃªn Google Colab

**BÆ°á»›c 1**: Upload notebook

```
1. Má»Ÿ https://colab.research.google.com
2. File â†’ Upload â†’ chá»n Microgrid_PPO_Simple.ipynb
3. Runtime â†’ Change runtime type â†’ GPU (T4)
```

**BÆ°á»›c 2**: CÃ¡ nhÃ¢n hÃ³a tham sá»‘

```python
SEED = 42              # ðŸ”§ Má»—i SV chá»n khÃ¡c: 42, 123, 456, 789, 999
EPISODES = 200         # ðŸ”§ Sá»‘ episodes: 100-500
LR_ACTOR = 3e-4        # ðŸ”§ Learning rate actor: 1e-4 ~ 1e-3
LR_CRITIC = 1e-3       # ðŸ”§ Learning rate critic: 5e-4 ~ 3e-3
CLIP_EPSILON = 0.2     # ðŸ”§ PPO clip: 0.1-0.3
PPO_EPOCHS = 10        # ðŸ”§ Update epochs: 5-15
GAE_LAMBDA = 0.95      # ðŸ”§ GAE lambda: 0.9-0.99
```

**BÆ°á»›c 3**: Cháº¡y 3 Ã´

```
Ã” 1 (ðŸ“¦ CÃ i Äáº·t): CÃ i thÆ° viá»‡n + táº¡o env + agent    (~10s)
Ã” 2 (ðŸš€ Huáº¥n Luyá»‡n): Training PPO agent               (~60s)
Ã” 3 (ðŸ“Š Káº¿t Quáº£): Xem Ä‘á»“ thá»‹ + so sÃ¡nh               (~5s)
```

---

## 5. QUÃ TRÃŒNH TRAINING

### 5.1 Training Loop (Pseudocode)

```python
for episode in range(200):
    state = env.reset()
    
    for step in range(24):
        # 1. Sample action tá»« policy (KHÃ”NG dÃ¹ng Îµ-greedy)
        probs, value = actor_critic(state)
        action = sample(probs)          # Sample tá»« distribution
        log_prob = log(probs[action])
        
        # 2. Thá»±c hiá»‡n action
        next_state, reward, done = env.step(action)
        
        # 3. LÆ°u vÃ o rollout buffer
        buffer.add(state, action, log_prob, reward, value, done)
        
        state = next_state
    
    # 4. PPO Update (má»—i 4 episodes)
    if episode % 4 == 0:
        # TÃ­nh GAE advantages
        advantages = compute_GAE(buffer.rewards, buffer.values)
        returns = advantages + buffer.values
        
        # Update policy nhiá»u epochs (PPO_EPOCHS = 10)
        for epoch in range(10):
            new_probs, new_values = actor_critic(buffer.states)
            ratio = new_probs / old_probs
            
            # Clipped objective
            surr1 = ratio Ã— advantages
            surr2 = clip(ratio, 0.8, 1.2) Ã— advantages
            loss = -min(surr1, surr2) + 0.5 Ã— MSE(new_values, returns) - 0.01 Ã— entropy
            
            optimizer.step()
        
        buffer.clear()  # XÃ³a data cÅ© (on-policy!)
```

### 5.2 PPO vs DQN Training Flow

```
DQN Training:                          PPO Training:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Má»—i step:           â”‚                â”‚ Má»—i 4 episodes:     â”‚
â”‚ 1. Îµ-greedy action  â”‚                â”‚ 1. Sample tá»« Ï€(a|s) â”‚
â”‚ 2. Store to replay  â”‚                â”‚ 2. Store to rollout  â”‚
â”‚ 3. Sample batch     â”‚                â”‚ 3. Compute GAE       â”‚
â”‚ 4. MSE(Q, target)   â”‚                â”‚ 4. Clipped loss      â”‚
â”‚ 5. Update Q-network â”‚                â”‚ 5. 10 epochs update  â”‚
â”‚ 6. Sync target net  â”‚                â”‚ 6. Clear buffer      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Update: Má»–I STEP                       Update: Má»–I 4 EPISODES
Data: Replay (tÃ¡i sá»­ dá»¥ng)             Data: Rollout (dÃ¹ng 1 láº§n)
```

---

## 6. HYPERPARAMETERS

| Parameter | Value | Ã nghÄ©a | Gá»£i Ã½ thay Ä‘á»•i |
|-----------|-------|---------|----------------|
| LR Actor | 3e-4 | Tá»‘c Ä‘á»™ há»c policy | 1e-4 ~ 1e-3 |
| LR Critic | 1e-3 | Tá»‘c Ä‘á»™ há»c value | 5e-4 ~ 3e-3 |
| Gamma (Î³) | 0.99 | Discount factor | 0.95 ~ 0.99 |
| GAE Lambda (Î») | 0.95 | Bias-variance tradeoff | 0.9 ~ 0.99 |
| Clip Epsilon (Îµ) | 0.2 | PPO clipping range | 0.1 ~ 0.3 |
| PPO Epochs | 10 | Sá»‘ láº§n update per rollout | 5 ~ 15 |
| Mini-batch size | 32 | KÃ­ch thÆ°á»›c batch | 16, 32, 64 |
| Entropy coeff | 0.01 | Khuyáº¿n khÃ­ch exploration | 0.005 ~ 0.05 |
| Value loss coeff | 0.5 | Trá»ng sá»‘ value loss | 0.25 ~ 1.0 |
| Max grad norm | 0.5 | Gradient clipping | 0.3 ~ 1.0 |
| Hidden layers | [128, 128] | Kiáº¿n trÃºc shared network | Thay Ä‘á»•i kÃ­ch thÆ°á»›c |
| Episodes | 200 | Sá»‘ episodes training | 100 ~ 500 |

---

## 7. Káº¾T QUáº¢ ÄÃNH GIÃ

### 7.1 Training Convergence

```
PPO Training Progress:
Episode   10 | Reward:  -1.50 | Renewable: 42.3%  â† Exploring
Episode   50 | Reward:  +1.20 | Renewable: 51.8%  â† Learning
Episode  100 | Reward:  +5.30 | Renewable: 63.2%  â† Improving
Episode  150 | Reward:  +9.80 | Renewable: 72.5%  â† Near optimal
Episode  200 | Reward: +12.10 | Renewable: 78.9%  â† Converged
```

### 7.2 So SÃ¡nh PPO Agent vs Random

| Metric | PPO Agent | Random | Improvement |
|--------|-----------|--------|-------------|
| Mean Reward | +12.10 | -3.34 | **+462%** |
| Daily Cost | $2.15 | $16.42 | **-86.9%** |
| Renewable Usage | 78.9% | 47.8% | **+31.1pp** |
| Unmet Demand | 4.1% | 16.1% | **-12.0pp** |

---

## 8. Æ¯U ÄIá»‚M VÃ€ Háº N CHáº¾ Cá»¦A PPO

### 8.1 Æ¯u Äiá»ƒm

- âœ… **Smooth policy**: XÃ¡c suáº¥t thay Ä‘á»•i mÆ°á»£t, khÃ´ng nháº£y Ä‘á»™t ngá»™t nhÆ° Îµ-greedy
- âœ… **Robust**: Clipped objective ngÄƒn divergence, Ã­t cáº§n tuning
- âœ… **Scalable**: Dá»… má»Ÿ rá»™ng sang continuous action space
- âœ… **Natural exploration**: Entropy bonus â†’ explore tá»± nhiÃªn, khÃ´ng cáº§n Îµ

### 8.2 Háº¡n Cháº¿

- âŒ **Sample inefficient**: On-policy â†’ data chá»‰ dÃ¹ng 1 láº§n rá»“i bá»
- âŒ **Training cháº­m hÆ¡n**: Cáº§n nhiá»u episodes hÆ¡n DQN Ä‘á»ƒ converge
- âŒ **Nhiá»u hyperparameters**: clip_Îµ, GAE_Î», entropy_coeff, 2 learning rates
- âŒ **Sensitive to network architecture**: Shared vs separate networks áº£nh hÆ°á»Ÿng lá»›n

---

## 9. TÃ€I LIá»†U THAM KHáº¢O

1. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." *arXiv preprint arXiv:1707.06347*.
2. Schulman, J., et al. (2015). "High-Dimensional Continuous Control Using Generalized Advantage Estimation." *ICLR*.
3. Konda, V., & Tsitsiklis, J. (2000). "Actor-Critic Algorithms." *NIPS*.
4. Sutton, R., et al. (2000). "Policy Gradient Methods for Reinforcement Learning with Function Approximation." *NIPS*.
