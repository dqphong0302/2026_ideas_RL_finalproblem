{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-",
        "\"\"\"",
        "================================================================================",
        "\ud83d\udd0b MICROGRID ENERGY OPTIMIZATION USING DEEP REINFORCEMENT LEARNING",
        "================================================================================",
        "Google Colab Standalone Script - Run All Cells to Train & Evaluate DQN Agent",
        "",
        "This notebook contains:",
        "1. Environment setup and installations",
        "2. Microgrid Environment (Gymnasium-compatible)",
        "3. DQN Agent with Experience Replay and Target Network",
        "4. Training Loop with Logging",
        "5. Evaluation and Visualization",
        "6. Full Execution Pipeline",
        "",
        "Author: Deep RL Assignment",
        "Date: February 2026",
        "",
        "================================================================================",
        "\ud83d\udcdd H\u01af\u1edaNG D\u1eaaN CHO SINH VI\u00caN (STUDENT CUSTOMIZATION GUIDE)",
        "================================================================================",
        "File n\u00e0y l\u00e0 B\u00c0I M\u1eaaU cho 5-7 sinh vi\u00ean tham kh\u1ea3o.",
        "\u0110\u1ec3 B\u00c0I L\u00c0M c\u1ee7a m\u1ed7i ng\u01b0\u1eddi KH\u00c1C NHAU, h\u00e3y thay \u0111\u1ed5i c\u00e1c gi\u00e1 tr\u1ecb \u0111\u01b0\u1ee3c \u0111\u00e1nh d\u1ea5u:",
        "    \ud83d\udd27 [CUSTOMIZABLE] - C\u00f3 th\u1ec3 thay \u0111\u1ed5i s\u1ed1 li\u1ec7u",
        "    \u26a0\ufe0f [REQUIRED CHANGE] - B\u1eaeT BU\u1ed8C thay \u0111\u1ed5i \u0111\u1ec3 tr\u00e1nh tr\u00f9ng l\u1eb7p",
        "",
        "C\u00e1c g\u1ee3i \u00fd thay \u0111\u1ed5i:",
        "- Thay \u0111\u1ed5i hyperparameters (learning rate, batch size, hidden layers)",
        "- Thay \u0111\u1ed5i c\u1ea5u tr\u00fac m\u1ea1ng neural (s\u1ed1 layers, neurons)",
        "- Thay \u0111\u1ed5i reward weights",
        "- Thay \u0111\u1ed5i seed \u0111\u1ec3 c\u00f3 k\u1ebft qu\u1ea3 kh\u00e1c",
        "- Thay \u0111\u1ed5i s\u1ed1 episodes training",
        "================================================================================",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1\ufe0f\u20e3_Install_Dependen"
      },
      "outputs": [],
      "source": [
        "#@title 1\ufe0f\u20e3 Install Dependencies & Imports\n",
        "!pip install gymnasium torch numpy matplotlib -q\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\ud83d\udda5\ufe0f Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2\ufe0f\u20e3_Configuration_Pa"
      },
      "outputs": [],
      "source": [
        "#@title 2\ufe0f\u20e3 Configuration Parameters\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\u2699\ufe0f HYPERPARAMETERS - C\u1ea4U H\u00ccNH THAM S\u1ed0\n",
        "================================================================================\n",
        "\ud83d\udd27 [CUSTOMIZABLE] - Thay \u0111\u1ed5i c\u00e1c gi\u00e1 tr\u1ecb n\u00e0y \u0111\u1ec3 t\u1ea1o b\u00e0i l\u00e0m kh\u00e1c bi\u1ec7t!\n",
        "\n",
        "G\u1ee2I \u00dd THAY \u0110\u1ed4I CHO M\u1ed6I SINH VI\u00caN:\n",
        "- Student 1: seed=42, lr=1e-4, hidden=[256,256,128], episodes=500\n",
        "- Student 2: seed=123, lr=5e-4, hidden=[512,256], episodes=600\n",
        "- Student 3: seed=2024, lr=2e-4, hidden=[128,128,64], episodes=400\n",
        "- Student 4: seed=999, lr=1e-3, hidden=[256,128,64], episodes=550\n",
        "- Student 5: seed=7777, lr=3e-4, hidden=[384,192,96], episodes=450\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "CONFIG = {\n",
        "    # =========================================================================\n",
        "    # \ud83d\udd0b ENVIRONMENT PARAMETERS - Tham s\u1ed1 m\u00f4i tr\u01b0\u1eddng Microgrid\n",
        "    # =========================================================================\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Battery capacity in kWh\n",
        "    # G\u1ee3i \u00fd: 80-150 kWh (thay \u0111\u1ed5i \u00b120% so v\u1edbi 100)\n",
        "    \"battery_capacity\": 100.0,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Battery round-trip efficiency\n",
        "    # G\u1ee3i \u00fd: 0.90-0.98 (pin lithium th\u01b0\u1eddng 0.92-0.96)\n",
        "    \"battery_efficiency\": 0.95,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Maximum charge/discharge rate in kW\n",
        "    # G\u1ee3i \u00fd: 15-30 kW\n",
        "    \"max_charge_rate\": 20.0,\n",
        "    \"max_discharge_rate\": 20.0,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Renewable energy capacity\n",
        "    # G\u1ee3i \u00fd: Solar 40-70 kW, Wind 20-50 kW\n",
        "    \"max_solar\": 50.0,\n",
        "    \"max_wind\": 30.0,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Consumer demand parameters\n",
        "    # G\u1ee3i \u00fd: base 30-60 kW, std 5-15 kW\n",
        "    \"base_demand\": 40.0,\n",
        "    \"demand_std\": 10.0,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Grid electricity price ($/kWh)\n",
        "    # G\u1ee3i \u00fd: min 0.03-0.08, max 0.20-0.35\n",
        "    \"grid_price_min\": 0.05,\n",
        "    \"grid_price_max\": 0.25,\n",
        "    \n",
        "    # Hours per episode (1 day = 24 hours)\n",
        "    \"hours_per_episode\": 24,\n",
        "    \n",
        "    # =========================================================================\n",
        "    # \ud83e\udde0 DQN PARAMETERS - Tham s\u1ed1 thu\u1eadt to\u00e1n DQN\n",
        "    # =========================================================================\n",
        "    \n",
        "    # State and Action dimensions (KH\u00d4NG THAY \u0110\u1ed4I)\n",
        "    \"state_dim\": 8,\n",
        "    \"action_dim\": 5,\n",
        "    \n",
        "    # \u26a0\ufe0f [REQUIRED CHANGE] Neural network architecture\n",
        "    # M\u1ed7i sinh vi\u00ean PH\u1ea2I thay \u0111\u1ed5i c\u1ea5u tr\u00fac n\u00e0y!\n",
        "    # G\u1ee3i \u00fd: [128,128,64], [256,128], [512,256,128], [384,192,96]\n",
        "    \"hidden_dims\": [256, 256, 128],\n",
        "    \n",
        "    # \u26a0\ufe0f [REQUIRED CHANGE] Learning rate\n",
        "    # G\u1ee3i \u00fd: 1e-4, 2e-4, 5e-4, 1e-3, 3e-4\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Discount factor (gamma)\n",
        "    # G\u1ee3i \u00fd: 0.95-0.99\n",
        "    \"gamma\": 0.99,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Epsilon-greedy exploration\n",
        "    # G\u1ee3i \u00fd: start 0.9-1.0, end 0.01-0.05, decay 0.990-0.998\n",
        "    \"epsilon_start\": 1.0,\n",
        "    \"epsilon_end\": 0.01,\n",
        "    \"epsilon_decay\": 0.995,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Batch size for training\n",
        "    # G\u1ee3i \u00fd: 32, 64, 128, 256\n",
        "    \"batch_size\": 64,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Replay buffer size\n",
        "    # G\u1ee3i \u00fd: 50000-200000\n",
        "    \"buffer_size\": 100000,\n",
        "    \n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Target network update frequency\n",
        "    # G\u1ee3i \u00fd: 500-2000 steps\n",
        "    \"target_update_freq\": 1000,\n",
        "    \n",
        "    # =========================================================================\n",
        "    # \ud83d\udcca TRAINING PARAMETERS - Tham s\u1ed1 hu\u1ea5n luy\u1ec7n\n",
        "    # =========================================================================\n",
        "    \n",
        "    # \u26a0\ufe0f [REQUIRED CHANGE] Number of training episodes\n",
        "    # M\u1ed7i sinh vi\u00ean n\u00ean d\u00f9ng s\u1ed1 kh\u00e1c nhau: 400-700\n",
        "    \"num_episodes\": 500,\n",
        "    \n",
        "    # Steps per episode (= hours per day)\n",
        "    \"max_steps_per_episode\": 24,\n",
        "    \n",
        "    # Checkpoint save frequency\n",
        "    \"save_freq\": 100,\n",
        "    \n",
        "    # Logging frequency\n",
        "    \"log_freq\": 10,\n",
        "    \n",
        "    # \u26a0\ufe0f [REQUIRED CHANGE] Random seed - M\u1ed6I SINH VI\u00caN PH\u1ea2I KH\u00c1C!\n",
        "    # G\u1ee3i \u00fd: 42, 123, 456, 789, 999, 2024, 7777\n",
        "    \"seed\": 42,\n",
        "    \n",
        "    # =========================================================================\n",
        "    # \ud83c\udfaf REWARD SHAPING - Tr\u1ecdng s\u1ed1 ph\u1ea7n th\u01b0\u1edfng\n",
        "    # =========================================================================\n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Reward weights - Thay \u0111\u1ed5i \u0111\u1ec3 \u01b0u ti\u00ean m\u1ee5c ti\u00eau kh\u00e1c nhau\n",
        "    \n",
        "    # Positive reward for using renewable energy\n",
        "    # G\u1ee3i \u00fd: 0.5-2.0\n",
        "    \"reward_renewable\": 1.0,\n",
        "    \n",
        "    # Negative penalty for grid electricity purchase\n",
        "    # G\u1ee3i \u00fd: -1.0 \u0111\u1ebfn -3.0\n",
        "    \"reward_grid_penalty\": -2.0,\n",
        "    \n",
        "    # Heavy penalty for unmet demand\n",
        "    # G\u1ee3i \u00fd: -3.0 \u0111\u1ebfn -10.0\n",
        "    \"reward_unmet_penalty\": -5.0,\n",
        "    \n",
        "    # Small penalty for battery wear (cycling)\n",
        "    # G\u1ee3i \u00fd: -0.05 \u0111\u1ebfn -0.2\n",
        "    \"reward_battery_wear\": -0.1,\n",
        "    \n",
        "    # Bonus for avoiding grid during peak hours\n",
        "    # G\u1ee3i \u00fd: 0.3-1.0\n",
        "    \"reward_peak_bonus\": 0.5,\n",
        "    \n",
        "    # =========================================================================\n",
        "    # \ud83d\uded1 EPISODE TERMINATION CONDITIONS - \u0110i\u1ec1u ki\u1ec7n k\u1ebft th\u00fac episode\n",
        "    # =========================================================================\n",
        "    # \ud83d\udd27 [CUSTOMIZABLE] Th\u00eam \u0111i\u1ec1u ki\u1ec7n k\u1ebft th\u00fac s\u1edbm\n",
        "    \n",
        "    # Critical low battery threshold (fraction of capacity)\n",
        "    # N\u1ebfu pin < threshold, episode k\u1ebft th\u00fac. G\u1ee3i \u00fd: 0.05-0.15\n",
        "    \"battery_critical_low\": 0.05,\n",
        "    \n",
        "    # Critical high battery threshold (fraction of capacity)\n",
        "    # N\u1ebfu pin > threshold, k\u1ebft th\u00fac. G\u1ee3i \u00fd: 0.95-1.0\n",
        "    \"battery_critical_high\": 1.0,\n",
        "    \n",
        "    # Maximum cumulative unmet demand ratio before termination\n",
        "    # G\u1ee3i \u00fd: 0.15-0.30\n",
        "    \"max_unmet_ratio\": 0.20,\n",
        "}\n",
        "\n",
        "print(\"\u2705 Configuration loaded!\")\n",
        "print(f\"   Episodes: {CONFIG['num_episodes']}\")\n",
        "print(f\"   Learning Rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"   Gamma: {CONFIG['gamma']}\")\n",
        "print(f\"   Hidden Layers: {CONFIG['hidden_dims']}\")\n",
        "print(f\"   Seed: {CONFIG['seed']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3\ufe0f\u20e3_Microgrid_Enviro"
      },
      "outputs": [],
      "source": [
        "#@title 3\ufe0f\u20e3 Microgrid Environment\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\ud83d\udd0b MICROGRID ENVIRONMENT - M\u00f4i tr\u01b0\u1eddng m\u00f4 ph\u1ecfng l\u01b0\u1edbi \u0111i\u1ec7n si\u00eau nh\u1ecf\n",
        "================================================================================\n",
        "\n",
        "Gymnasium-compatible environment simulating:\n",
        "- Solar and wind renewable generation (ph\u00e1t \u0111i\u1ec7n t\u00e1i t\u1ea1o)\n",
        "- Battery storage with efficiency losses (l\u01b0u tr\u1eef pin v\u1edbi t\u1ed5n hao)\n",
        "- Grid connection with time-varying prices (k\u1ebft n\u1ed1i l\u01b0\u1edbi v\u1edbi gi\u00e1 bi\u1ebfn \u0111\u1ed5i)\n",
        "- Consumer demand with stochastic variations (nhu c\u1ea7u ng\u1eabu nhi\u00ean)\n",
        "\n",
        "State Space (8D continuous):\n",
        "    [battery_level, demand, solar, wind, grid_price, hour_sin, hour_cos, prev_action]\n",
        "\n",
        "Action Space (5 discrete):\n",
        "    0: Discharge battery (X\u1ea3 pin)\n",
        "    1: Charge from renewable (S\u1ea1c t\u1eeb n\u0103ng l\u01b0\u1ee3ng t\u00e1i t\u1ea1o)\n",
        "    2: Buy from grid (Mua t\u1eeb l\u01b0\u1edbi \u0111i\u1ec7n)\n",
        "    3: Renewable + Discharge (K\u1ebft h\u1ee3p t\u00e1i t\u1ea1o + x\u1ea3 pin)\n",
        "    4: Renewable + Grid (K\u1ebft h\u1ee3p t\u00e1i t\u1ea1o + l\u01b0\u1edbi)\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "class MicrogridEnv:\n",
        "    \"\"\"\n",
        "    Microgrid Energy Management Environment\n",
        "    \n",
        "    M\u00f4i tr\u01b0\u1eddng qu\u1ea3n l\u00fd n\u0103ng l\u01b0\u1ee3ng l\u01b0\u1edbi \u0111i\u1ec7n si\u00eau nh\u1ecf, m\u00f4 ph\u1ecfng:\n",
        "    - Ph\u00e1t \u0111i\u1ec7n m\u1eb7t tr\u1eddi (solar) theo th\u1eddi gian trong ng\u00e0y\n",
        "    - Ph\u00e1t \u0111i\u1ec7n gi\u00f3 (wind) v\u1edbi bi\u1ebfn \u0111\u1ed9ng ng\u1eabu nhi\u00ean\n",
        "    - Pin l\u01b0u tr\u1eef v\u1edbi hi\u1ec7u su\u1ea5t round-trip\n",
        "    - Nhu c\u1ea7u ti\u00eau th\u1ee5 v\u1edbi peak s\u00e1ng v\u00e0 t\u1ed1i\n",
        "    - Gi\u00e1 \u0111i\u1ec7n l\u01b0\u1edbi bi\u1ebfn \u0111\u1ed5i theo gi\u1edd\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict):\n",
        "        \"\"\"\n",
        "        Kh\u1edfi t\u1ea1o m\u00f4i tr\u01b0\u1eddng v\u1edbi c\u1ea5u h\u00ecnh.\n",
        "        \n",
        "        Args:\n",
        "            config: Dictionary ch\u1ee9a t\u1ea5t c\u1ea3 tham s\u1ed1 c\u1ea5u h\u00ecnh\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        \n",
        "        # \ud83d\udd0b Battery parameters - Tham s\u1ed1 pin\n",
        "        self.battery_capacity = config[\"battery_capacity\"]  # kWh\n",
        "        self.battery_efficiency = config[\"battery_efficiency\"]  # Hi\u1ec7u su\u1ea5t\n",
        "        self.max_charge = config[\"max_charge_rate\"]  # kW - t\u1ed1c \u0111\u1ed9 s\u1ea1c t\u1ed1i \u0111a\n",
        "        self.max_discharge = config[\"max_discharge_rate\"]  # kW - t\u1ed1c \u0111\u1ed9 x\u1ea3 t\u1ed1i \u0111a\n",
        "        \n",
        "        # \u2600\ufe0f Renewable energy parameters - Tham s\u1ed1 n\u0103ng l\u01b0\u1ee3ng t\u00e1i t\u1ea1o\n",
        "        self.max_solar = config[\"max_solar\"]  # kW peak solar\n",
        "        self.max_wind = config[\"max_wind\"]  # kW peak wind\n",
        "        \n",
        "        # \ud83c\udfe0 Demand parameters - Tham s\u1ed1 nhu c\u1ea7u ti\u00eau th\u1ee5\n",
        "        self.base_demand = config[\"base_demand\"]  # kW trung b\u00ecnh\n",
        "        self.demand_std = config[\"demand_std\"]  # \u0110\u1ed9 l\u1ec7ch chu\u1ea9n\n",
        "        \n",
        "        # \ud83d\udcb0 Grid price parameters - Tham s\u1ed1 gi\u00e1 \u0111i\u1ec7n l\u01b0\u1edbi\n",
        "        self.grid_price_min = config[\"grid_price_min\"]  # $/kWh off-peak\n",
        "        self.grid_price_max = config[\"grid_price_max\"]  # $/kWh peak\n",
        "        \n",
        "        # \u23f0 Time parameters\n",
        "        self.hours_per_episode = config[\"hours_per_episode\"]\n",
        "        \n",
        "        # \ud83c\udfaf Reward weights - Tr\u1ecdng s\u1ed1 ph\u1ea7n th\u01b0\u1edfng\n",
        "        self.r_renewable = config[\"reward_renewable\"]\n",
        "        self.r_grid = config[\"reward_grid_penalty\"]\n",
        "        self.r_unmet = config[\"reward_unmet_penalty\"]\n",
        "        self.r_wear = config[\"reward_battery_wear\"]\n",
        "        self.r_bonus = config[\"reward_peak_bonus\"]\n",
        "        \n",
        "        # \ud83d\uded1 Termination conditions - \u0110i\u1ec1u ki\u1ec7n k\u1ebft th\u00fac\n",
        "        self.battery_critical_low = config.get(\"battery_critical_low\", 0.05)\n",
        "        self.battery_critical_high = config.get(\"battery_critical_high\", 1.0)\n",
        "        self.max_unmet_ratio = config.get(\"max_unmet_ratio\", 0.20)\n",
        "        \n",
        "        # State tracking\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self, seed: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Reset environment to initial state.\n",
        "        \u0110\u1eb7t l\u1ea1i m\u00f4i tr\u01b0\u1eddng v\u1ec1 tr\u1ea1ng th\u00e1i ban \u0111\u1ea7u.\n",
        "        \n",
        "        Args:\n",
        "            seed: Random seed for reproducibility\n",
        "            \n",
        "        Returns:\n",
        "            Initial observation vector (8D)\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Initial battery level\n",
        "        # G\u1ee3i \u00fd: Thay \u0111\u1ed5i m\u1ee9c pin ban \u0111\u1ea7u (0.3-0.7 c\u1ee7a capacity)\n",
        "        self.battery_level = self.battery_capacity * 0.5  # Start at 50%\n",
        "        self.current_hour = 0\n",
        "        self.prev_action = 0\n",
        "        \n",
        "        # Episode tracking - Theo d\u00f5i episode\n",
        "        self.total_demand = 0.0\n",
        "        self.total_renewable_used = 0.0\n",
        "        self.total_grid_cost = 0.0\n",
        "        self.total_unmet = 0.0\n",
        "        self.episode_history = []\n",
        "        \n",
        "        return self._get_obs()\n",
        "    \n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate normalized observation vector.\n",
        "        T\u1ea1o vector quan s\u00e1t \u0111\u00e3 chu\u1ea9n h\u00f3a.\n",
        "        \n",
        "        Observation vector g\u1ed3m 8 th\u00e0nh ph\u1ea7n:\n",
        "        - battery_level: M\u1ee9c pin hi\u1ec7n t\u1ea1i [0,1]\n",
        "        - demand: Nhu c\u1ea7u ti\u00eau th\u1ee5 [0,1]\n",
        "        - solar: S\u1ea3n l\u01b0\u1ee3ng \u0111i\u1ec7n m\u1eb7t tr\u1eddi [0,1]\n",
        "        - wind: S\u1ea3n l\u01b0\u1ee3ng \u0111i\u1ec7n gi\u00f3 [0,1]\n",
        "        - grid_price: Gi\u00e1 \u0111i\u1ec7n l\u01b0\u1edbi [0,1]\n",
        "        - hour_sin: Sin c\u1ee7a gi\u1edd (cyclic encoding)\n",
        "        - hour_cos: Cos c\u1ee7a gi\u1edd (cyclic encoding)\n",
        "        - prev_action: H\u00e0nh \u0111\u1ed9ng tr\u01b0\u1edbc \u0111\u00f3 [0,1]\n",
        "        \n",
        "        Returns:\n",
        "            Normalized observation array (8D)\n",
        "        \"\"\"\n",
        "        demand = self._get_demand(self.current_hour)\n",
        "        solar = self._get_solar(self.current_hour)\n",
        "        wind = self._get_wind(self.current_hour)\n",
        "        price = self._get_price(self.current_hour)\n",
        "        \n",
        "        # Normalize all features to [0, 1]\n",
        "        obs = np.array([\n",
        "            self.battery_level / self.battery_capacity,\n",
        "            demand / (self.base_demand * 2),\n",
        "            solar / self.max_solar,\n",
        "            wind / self.max_wind,\n",
        "            (price - self.grid_price_min) / (self.grid_price_max - self.grid_price_min),\n",
        "            (np.sin(2 * np.pi * self.current_hour / 24) + 1) / 2,\n",
        "            (np.cos(2 * np.pi * self.current_hour / 24) + 1) / 2,\n",
        "            self.prev_action / 4.0,\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        return np.clip(obs, 0.0, 1.0)\n",
        "    \n",
        "    def _get_demand(self, hour: int) -> float:\n",
        "        \"\"\"\n",
        "        Generate stochastic demand with morning and evening peaks.\n",
        "        T\u1ea1o nhu c\u1ea7u ng\u1eabu nhi\u00ean v\u1edbi \u0111\u1ec9nh s\u00e1ng v\u00e0 t\u1ed1i.\n",
        "        \n",
        "        \ud83d\udd27 [CUSTOMIZABLE] C\u00f3 th\u1ec3 thay \u0111\u1ed5i:\n",
        "        - Gi\u1edd peak s\u00e1ng (m\u1eb7c \u0111\u1ecbnh: 8h)\n",
        "        - Gi\u1edd peak t\u1ed1i (m\u1eb7c \u0111\u1ecbnh: 19h)\n",
        "        - C\u01b0\u1eddng \u0111\u1ed9 peak (0.3 v\u00e0 0.4)\n",
        "        \n",
        "        Args:\n",
        "            hour: Current hour (0-23)\n",
        "            \n",
        "        Returns:\n",
        "            Demand in kW\n",
        "        \"\"\"\n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Peak hours - Thay \u0111\u1ed5i gi\u1edd cao \u0111i\u1ec3m\n",
        "        morning_peak_hour = 8  # G\u1ee3i \u00fd: 7-9\n",
        "        evening_peak_hour = 19  # G\u1ee3i \u00fd: 18-21\n",
        "        \n",
        "        morning_peak = np.exp(-((hour - morning_peak_hour) ** 2) / 8)\n",
        "        evening_peak = np.exp(-((hour - evening_peak_hour) ** 2) / 8)\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Peak intensity - Thay \u0111\u1ed5i c\u01b0\u1eddng \u0111\u1ed9 peak\n",
        "        base = self.base_demand * (0.5 + 0.3 * morning_peak + 0.4 * evening_peak)\n",
        "        noise = np.random.normal(0, self.demand_std * 0.3)\n",
        "        \n",
        "        return max(0, base + noise)\n",
        "    \n",
        "    def _get_solar(self, hour: int) -> float:\n",
        "        \"\"\"\n",
        "        Generate solar power based on time of day.\n",
        "        T\u1ea1o s\u1ea3n l\u01b0\u1ee3ng \u0111i\u1ec7n m\u1eb7t tr\u1eddi theo th\u1eddi gian trong ng\u00e0y.\n",
        "        \n",
        "        \ud83d\udd27 [CUSTOMIZABLE] C\u00f3 th\u1ec3 thay \u0111\u1ed5i:\n",
        "        - Gi\u1edd m\u1eb7t tr\u1eddi m\u1ecdc/l\u1eb7n (m\u1eb7c \u0111\u1ecbnh: 6-18h)\n",
        "        - Bi\u1ebfn \u0111\u1ed9ng th\u1eddi ti\u1ebft (0.8-1.2)\n",
        "        \n",
        "        Args:\n",
        "            hour: Current hour (0-23)\n",
        "            \n",
        "        Returns:\n",
        "            Solar power in kW\n",
        "        \"\"\"\n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Sunrise/sunset hours\n",
        "        sunrise = 6  # G\u1ee3i \u00fd: 5-7\n",
        "        sunset = 18  # G\u1ee3i \u00fd: 17-19\n",
        "        \n",
        "        if sunrise <= hour <= sunset:\n",
        "            base = self.max_solar * np.sin(np.pi * (hour - sunrise) / (sunset - sunrise))\n",
        "            # \ud83d\udd27 [CUSTOMIZABLE] Weather variability\n",
        "            noise = np.random.uniform(0.8, 1.2)  # G\u1ee3i \u00fd: 0.7-1.3\n",
        "            return max(0, base * noise)\n",
        "        return 0.0\n",
        "    \n",
        "    def _get_wind(self, hour: int) -> float:\n",
        "        \"\"\"\n",
        "        Generate stochastic wind power.\n",
        "        T\u1ea1o s\u1ea3n l\u01b0\u1ee3ng \u0111i\u1ec7n gi\u00f3 ng\u1eabu nhi\u00ean.\n",
        "        \n",
        "        \ud83d\udd27 [CUSTOMIZABLE] C\u00f3 th\u1ec3 thay \u0111\u1ed5i:\n",
        "        - Base wind level (0.5)\n",
        "        - Variation amplitude\n",
        "        - Noise range (0.5-1.5)\n",
        "        \n",
        "        Args:\n",
        "            hour: Current hour (0-23)\n",
        "            \n",
        "        Returns:\n",
        "            Wind power in kW\n",
        "        \"\"\"\n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Wind pattern\n",
        "        base_level = 0.5  # G\u1ee3i \u00fd: 0.3-0.6\n",
        "        base = self.max_wind * base_level\n",
        "        variation = self.max_wind * (1 - base_level) * np.sin(np.pi * hour / 12)\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Wind variability\n",
        "        noise = np.random.uniform(0.5, 1.5)  # G\u1ee3i \u00fd: 0.4-1.6\n",
        "        \n",
        "        return max(0, (base + variation) * noise)\n",
        "    \n",
        "    def _get_price(self, hour: int) -> float:\n",
        "        \"\"\"\n",
        "        Generate time-varying grid electricity price.\n",
        "        T\u1ea1o gi\u00e1 \u0111i\u1ec7n l\u01b0\u1edbi bi\u1ebfn \u0111\u1ed5i theo th\u1eddi gian.\n",
        "        \n",
        "        \ud83d\udd27 [CUSTOMIZABLE] C\u00f3 th\u1ec3 thay \u0111\u1ed5i:\n",
        "        - Peak hours (7-9, 18-21)\n",
        "        - Off-peak hours (22-6)\n",
        "        - Price variation (0.9-1.1)\n",
        "        \n",
        "        Args:\n",
        "            hour: Current hour (0-23)\n",
        "            \n",
        "        Returns:\n",
        "            Price in $/kWh\n",
        "        \"\"\"\n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Peak/off-peak hours\n",
        "        if 7 <= hour <= 9 or 18 <= hour <= 21:  # Peak hours\n",
        "            base = self.grid_price_max\n",
        "        elif 22 <= hour or hour <= 6:  # Off-peak\n",
        "            base = self.grid_price_min\n",
        "        else:  # Mid-peak\n",
        "            base = (self.grid_price_min + self.grid_price_max) / 2\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Price variability\n",
        "        noise = np.random.uniform(0.9, 1.1)  # G\u1ee3i \u00fd: 0.85-1.15\n",
        "        \n",
        "        return base * noise\n",
        "    \n",
        "    def _check_termination(self) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Check if episode should terminate early.\n",
        "        Ki\u1ec3m tra \u0111i\u1ec1u ki\u1ec7n k\u1ebft th\u00fac s\u1edbm episode.\n",
        "        \n",
        "        Returns:\n",
        "            (should_terminate, reason)\n",
        "        \"\"\"\n",
        "        # Check end of day\n",
        "        if self.current_hour >= self.hours_per_episode:\n",
        "            return True, \"end_of_day\"\n",
        "        \n",
        "        # Check critical battery level\n",
        "        battery_ratio = self.battery_level / self.battery_capacity\n",
        "        if battery_ratio < self.battery_critical_low:\n",
        "            return True, \"battery_critical_low\"\n",
        "        if battery_ratio > self.battery_critical_high:\n",
        "            return True, \"battery_critical_high\"\n",
        "        \n",
        "        # Check cumulative unmet demand ratio\n",
        "        if self.total_demand > 0:\n",
        "            unmet_ratio = self.total_unmet / self.total_demand\n",
        "            if unmet_ratio > self.max_unmet_ratio:\n",
        "                return True, \"max_unmet_exceeded\"\n",
        "        \n",
        "        return False, \"\"\n",
        "    \n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
        "        \"\"\"\n",
        "        Execute one time step.\n",
        "        Th\u1ef1c hi\u1ec7n m\u1ed9t b\u01b0\u1edbc th\u1eddi gian (1 gi\u1edd).\n",
        "        \n",
        "        Args:\n",
        "            action: Integer action (0-4)\n",
        "            \n",
        "        Returns:\n",
        "            (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        demand = self._get_demand(self.current_hour)\n",
        "        solar = self._get_solar(self.current_hour)\n",
        "        wind = self._get_wind(self.current_hour)\n",
        "        price = self._get_price(self.current_hour)\n",
        "        renewable = solar + wind\n",
        "        \n",
        "        # Initialize energy flows\n",
        "        renewable_used = 0.0\n",
        "        grid_purchased = 0.0\n",
        "        battery_charge = 0.0\n",
        "        battery_discharge = 0.0\n",
        "        unmet_demand = 0.0\n",
        "        \n",
        "        # Process action - X\u1eed l\u00fd h\u00e0nh \u0111\u1ed9ng\n",
        "        if action == 0:  # Discharge battery - X\u1ea3 pin\n",
        "            discharge = min(self.battery_level, self.max_discharge, demand)\n",
        "            battery_discharge = discharge\n",
        "            self.battery_level -= discharge\n",
        "            remaining = demand - discharge * self.battery_efficiency\n",
        "            if remaining > 0:\n",
        "                unmet_demand = remaining\n",
        "        \n",
        "        elif action == 1:  # Charge from renewable - S\u1ea1c t\u1eeb t\u00e1i t\u1ea1o\n",
        "            supply = min(renewable, demand)\n",
        "            renewable_used = supply\n",
        "            remaining = demand - supply\n",
        "            if remaining > 0:\n",
        "                unmet_demand = remaining\n",
        "            excess = renewable - supply\n",
        "            if excess > 0:\n",
        "                charge = min(excess, self.max_charge, \n",
        "                           self.battery_capacity - self.battery_level)\n",
        "                battery_charge = charge\n",
        "                self.battery_level += charge * self.battery_efficiency\n",
        "        \n",
        "        elif action == 2:  # Buy from grid - Mua t\u1eeb l\u01b0\u1edbi\n",
        "            grid_purchased = demand\n",
        "        \n",
        "        elif action == 3:  # Renewable + Discharge - T\u00e1i t\u1ea1o + X\u1ea3 pin\n",
        "            renewable_used = min(renewable, demand)\n",
        "            remaining = demand - renewable_used\n",
        "            if remaining > 0:\n",
        "                discharge = min(self.battery_level, self.max_discharge, remaining)\n",
        "                battery_discharge = discharge\n",
        "                self.battery_level -= discharge\n",
        "                remaining -= discharge * self.battery_efficiency\n",
        "            if remaining > 0:\n",
        "                unmet_demand = remaining\n",
        "        \n",
        "        elif action == 4:  # Renewable + Grid - T\u00e1i t\u1ea1o + L\u01b0\u1edbi\n",
        "            renewable_used = min(renewable, demand)\n",
        "            remaining = demand - renewable_used\n",
        "            if remaining > 0:\n",
        "                grid_purchased = remaining\n",
        "        \n",
        "        # Calculate reward - T\u00ednh ph\u1ea7n th\u01b0\u1edfng\n",
        "        normalized_price = (price - self.grid_price_min) / (self.grid_price_max - self.grid_price_min)\n",
        "        is_peak = 18 <= self.current_hour <= 21\n",
        "        \n",
        "        reward = (\n",
        "            self.r_renewable * (renewable_used / self.base_demand) +\n",
        "            self.r_grid * (grid_purchased / self.base_demand) * normalized_price +\n",
        "            self.r_unmet * (unmet_demand / self.base_demand) +\n",
        "            self.r_wear * ((battery_charge + battery_discharge) / self.max_charge)\n",
        "        )\n",
        "        \n",
        "        # Bonus for avoiding grid during peak\n",
        "        if is_peak and grid_purchased == 0:\n",
        "            reward += self.r_bonus\n",
        "        \n",
        "        # Update tracking\n",
        "        self.total_demand += demand\n",
        "        self.total_renewable_used += renewable_used\n",
        "        self.total_grid_cost += grid_purchased * price\n",
        "        self.total_unmet += unmet_demand\n",
        "        \n",
        "        # Store history\n",
        "        self.episode_history.append({\n",
        "            \"hour\": self.current_hour,\n",
        "            \"demand\": demand,\n",
        "            \"solar\": solar,\n",
        "            \"wind\": wind,\n",
        "            \"price\": price,\n",
        "            \"action\": action,\n",
        "            \"renewable_used\": renewable_used,\n",
        "            \"grid_purchased\": grid_purchased,\n",
        "            \"battery_level\": self.battery_level,\n",
        "            \"reward\": reward,\n",
        "        })\n",
        "        \n",
        "        # Advance time\n",
        "        self.current_hour += 1\n",
        "        self.prev_action = action\n",
        "        \n",
        "        # Check termination\n",
        "        done, termination_reason = self._check_termination()\n",
        "        \n",
        "        info = {\n",
        "            \"total_cost\": self.total_grid_cost,\n",
        "            \"renewable_ratio\": self.total_renewable_used / max(1, self.total_demand),\n",
        "            \"unmet_ratio\": self.total_unmet / max(1, self.total_demand),\n",
        "            \"termination_reason\": termination_reason,\n",
        "        }\n",
        "        \n",
        "        return self._get_obs(), reward, done, info\n",
        "\n",
        "print(\"\u2705 MicrogridEnv class defined!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4\ufe0f\u20e3_Neural_Network_&"
      },
      "outputs": [],
      "source": [
        "#@title 4\ufe0f\u20e3 Neural Network & Replay Buffer\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\ud83e\udde0 DQN COMPONENTS - C\u00e1c th\u00e0nh ph\u1ea7n DQN\n",
        "================================================================================\n",
        "- Q-Network: Multi-layer perceptron v\u1edbi ReLU activations\n",
        "- Replay Buffer: Uniform sampling for experience replay\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network for value function approximation.\n",
        "    M\u1ea1ng Q s\u00e2u \u0111\u1ec3 x\u1ea5p x\u1ec9 h\u00e0m gi\u00e1 tr\u1ecb.\n",
        "    \n",
        "    \ud83d\udd27 [CUSTOMIZABLE] C\u00f3 th\u1ec3 thay \u0111\u1ed5i:\n",
        "    - S\u1ed1 layers v\u00e0 neurons (trong CONFIG['hidden_dims'])\n",
        "    - Dropout rate\n",
        "    - Activation functions\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int]):\n",
        "        \"\"\"\n",
        "        Kh\u1edfi t\u1ea1o m\u1ea1ng Q.\n",
        "        \n",
        "        Args:\n",
        "            state_dim: Dimension of state space (8)\n",
        "            action_dim: Number of actions (5)\n",
        "            hidden_dims: List of hidden layer sizes, e.g., [256, 256, 128]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = state_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            # \ud83d\udd27 [CUSTOMIZABLE] Dropout rate\n",
        "            # G\u1ee3i \u00fd: 0.0-0.3\n",
        "            layers.append(nn.Dropout(0.1))\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        layers.append(nn.Linear(prev_dim, action_dim))\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"\n",
        "        Xavier initialization for stable training.\n",
        "        Kh\u1edfi t\u1ea1o Xavier \u0111\u1ec3 hu\u1ea5n luy\u1ec7n \u1ed5n \u0111\u1ecbnh.\n",
        "        \n",
        "        \ud83d\udd27 [CUSTOMIZABLE] C\u00f3 th\u1ec3 thay \u0111\u1ed5i ph\u01b0\u01a1ng ph\u00e1p init:\n",
        "        - xavier_uniform_\n",
        "        - kaiming_uniform_\n",
        "        - orthogonal_\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass qua m\u1ea1ng.\"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience replay buffer for DQN training.\n",
        "    B\u1ed9 \u0111\u1ec7m ph\u00e1t l\u1ea1i kinh nghi\u1ec7m cho hu\u1ea5n luy\u1ec7n DQN.\n",
        "    \n",
        "    Gi\u00fap ph\u00e1 v\u1ee1 correlation gi\u1eefa c\u00e1c m\u1eabu li\u00ean ti\u1ebfp.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, capacity: int):\n",
        "        \"\"\"\n",
        "        Kh\u1edfi t\u1ea1o buffer.\n",
        "        \n",
        "        Args:\n",
        "            capacity: Maximum number of transitions to store\n",
        "        \"\"\"\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Th\u00eam transition v\u00e0o buffer.\"\"\"\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def sample(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        Sample random batch from buffer.\n",
        "        L\u1ea5y m\u1eabu ng\u1eabu nhi\u00ean t\u1eeb buffer.\n",
        "        \n",
        "        Args:\n",
        "            batch_size: Number of samples\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (states, actions, rewards, next_states, dones)\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        \n",
        "        return (\n",
        "            torch.FloatTensor(np.array(states)),\n",
        "            torch.LongTensor(actions),\n",
        "            torch.FloatTensor(rewards),\n",
        "            torch.FloatTensor(np.array(next_states)),\n",
        "            torch.FloatTensor(dones),\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return current buffer size.\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "print(\"\u2705 QNetwork and ReplayBuffer defined!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5\ufe0f\u20e3_DQN_Agent_(with_"
      },
      "outputs": [],
      "source": [
        "#@title 5\ufe0f\u20e3 DQN Agent (with Double DQN)\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\ud83e\udd16 DOUBLE DQN AGENT - Agent s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n Double DQN\n",
        "================================================================================\n",
        "\n",
        "C\u1ea3i ti\u1ebfn so v\u1edbi vanilla DQN:\n",
        "- Separate target network: M\u1ea1ng target ri\u00eang \u0111\u1ec3 \u1ed5n \u0111\u1ecbnh Q-values\n",
        "- Double DQN: Gi\u1ea3m overestimation bias\n",
        "- Epsilon-greedy exploration: Kh\u00e1m ph\u00e1 v\u1edbi epsilon decay\n",
        "\n",
        "Reference paper:\n",
        "- DQN: Mnih et al., 2015 \"Human-level control through deep RL\"\n",
        "- Double DQN: Van Hasselt et al., 2016 \"Deep RL with Double Q-learning\"\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Double DQN Agent for Microgrid Optimization.\n",
        "    Agent Double DQN cho t\u1ed1i \u01b0u h\u00f3a Microgrid.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: Dict, device: torch.device):\n",
        "        \"\"\"\n",
        "        Kh\u1edfi t\u1ea1o agent.\n",
        "        \n",
        "        Args:\n",
        "            config: Configuration dictionary\n",
        "            device: torch.device (cuda/cpu)\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        \n",
        "        self.state_dim = config[\"state_dim\"]\n",
        "        self.action_dim = config[\"action_dim\"]\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.target_update_freq = config[\"target_update_freq\"]\n",
        "        \n",
        "        # Epsilon parameters - Tham s\u1ed1 epsilon cho exploration\n",
        "        self.epsilon = config[\"epsilon_start\"]\n",
        "        self.epsilon_end = config[\"epsilon_end\"]\n",
        "        self.epsilon_decay = config[\"epsilon_decay\"]\n",
        "        \n",
        "        # Networks - M\u1ea1ng neural\n",
        "        self.q_network = QNetwork(\n",
        "            self.state_dim, \n",
        "            self.action_dim, \n",
        "            config[\"hidden_dims\"]\n",
        "        ).to(device)\n",
        "        \n",
        "        self.target_network = QNetwork(\n",
        "            self.state_dim, \n",
        "            self.action_dim, \n",
        "            config[\"hidden_dims\"]\n",
        "        ).to(device)\n",
        "        \n",
        "        # Copy weights to target network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Optimizer\n",
        "        # G\u1ee3i \u00fd thay \u0111\u1ed5i: SGD, RMSprop, AdamW\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.q_network.parameters(), \n",
        "            lr=config[\"learning_rate\"]\n",
        "        )\n",
        "        \n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(config[\"buffer_size\"])\n",
        "        \n",
        "        # Training tracking\n",
        "        self.training_step = 0\n",
        "        self.losses = []\n",
        "    \n",
        "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection.\n",
        "        Ch\u1ecdn h\u00e0nh \u0111\u1ed9ng theo chi\u1ebfn l\u01b0\u1ee3c epsilon-greedy.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state observation\n",
        "            training: Whether in training mode (use exploration)\n",
        "            \n",
        "        Returns:\n",
        "            Selected action (0-4)\n",
        "        \"\"\"\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_dim - 1)\n",
        "        \n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax(dim=1).item()\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store transition in replay buffer.\n",
        "        L\u01b0u transition v\u00e0o replay buffer.\n",
        "        \"\"\"\n",
        "        self.replay_buffer.push(state, action, reward, next_state, float(done))\n",
        "    \n",
        "    def update(self) -> Optional[float]:\n",
        "        \"\"\"\n",
        "        Perform one gradient update step.\n",
        "        Th\u1ef1c hi\u1ec7n m\u1ed9t b\u01b0\u1edbc c\u1eadp nh\u1eadt gradient.\n",
        "        \n",
        "        Returns:\n",
        "            Loss value or None if buffer too small\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        # Sample batch\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        \n",
        "        # Current Q values\n",
        "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        # Double DQN: use online network to select action, target network to evaluate\n",
        "        # Double DQN: d\u00f9ng m\u1ea1ng online ch\u1ecdn action, m\u1ea1ng target \u0111\u00e1nh gi\u00e1\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.q_network(next_states).argmax(dim=1, keepdim=True)\n",
        "            next_q = self.target_network(next_states).gather(1, next_actions).squeeze(1)\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Loss function\n",
        "        # G\u1ee3i \u00fd thay \u0111\u1ed5i: MSELoss, SmoothL1Loss (Huber)\n",
        "        loss = nn.SmoothL1Loss()(current_q, target_q)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # \ud83d\udd27 [CUSTOMIZABLE] Gradient clipping\n",
        "        # G\u1ee3i \u00fd: 1.0-20.0\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network periodically\n",
        "        self.training_step += 1\n",
        "        if self.training_step % self.target_update_freq == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        loss_value = loss.item()\n",
        "        self.losses.append(loss_value)\n",
        "        return loss_value\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decay exploration rate.\n",
        "        Gi\u1ea3m t\u1ef7 l\u1ec7 kh\u00e1m ph\u00e1.\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"\n",
        "        Save model weights.\n",
        "        L\u01b0u tr\u1ecdng s\u1ed1 model.\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            \"q_network\": self.q_network.state_dict(),\n",
        "            \"target_network\": self.target_network.state_dict(),\n",
        "            \"optimizer\": self.optimizer.state_dict(),\n",
        "            \"epsilon\": self.epsilon,\n",
        "            \"training_step\": self.training_step,\n",
        "        }, path)\n",
        "    \n",
        "    def load(self, path: str):\n",
        "        \"\"\"\n",
        "        Load model weights.\n",
        "        T\u1ea3i tr\u1ecdng s\u1ed1 model.\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.q_network.load_state_dict(checkpoint[\"q_network\"])\n",
        "        self.target_network.load_state_dict(checkpoint[\"target_network\"])\n",
        "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        self.epsilon = checkpoint[\"epsilon\"]\n",
        "        self.training_step = checkpoint[\"training_step\"]\n",
        "\n",
        "print(\"\u2705 DQNAgent class defined!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6\ufe0f\u20e3_Training_Functio"
      },
      "outputs": [],
      "source": [
        "#@title 6\ufe0f\u20e3 Training Function\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\ud83d\ude80 TRAINING LOOP - V\u00f2ng l\u1eb7p hu\u1ea5n luy\u1ec7n\n",
        "================================================================================\n",
        "\n",
        "Implements the main training procedure with:\n",
        "- Episode-based learning\n",
        "- Periodic logging\n",
        "- Best model checkpointing\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "def train(config: Dict, device: torch.device) -> Dict[str, List]:\n",
        "    \"\"\"\n",
        "    Train the DQN agent.\n",
        "    Hu\u1ea5n luy\u1ec7n agent DQN.\n",
        "    \n",
        "    Args:\n",
        "        config: Configuration dictionary\n",
        "        device: torch.device\n",
        "        \n",
        "    Returns:\n",
        "        Training history dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"\ud83d\ude80 STARTING TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Set random seeds for reproducibility\n",
        "    seed = config[\"seed\"]\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    # Initialize environment and agent\n",
        "    env = MicrogridEnv(config)\n",
        "    agent = DQNAgent(config, device)\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        \"rewards\": [],\n",
        "        \"costs\": [],\n",
        "        \"renewable_ratios\": [],\n",
        "        \"epsilons\": [],\n",
        "        \"losses\": [],\n",
        "    }\n",
        "    \n",
        "    best_reward = float(\"-inf\")\n",
        "    \n",
        "    # Training loop\n",
        "    for episode in range(config[\"num_episodes\"]):\n",
        "        state = env.reset(seed=seed + episode)\n",
        "        episode_reward = 0\n",
        "        episode_losses = []\n",
        "        \n",
        "        for step in range(config[\"max_steps_per_episode\"]):\n",
        "            # Select and execute action\n",
        "            action = agent.select_action(state, training=True)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            # Store and learn\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            loss = agent.update()\n",
        "            \n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Decay epsilon\n",
        "        agent.decay_epsilon()\n",
        "        \n",
        "        # Store history\n",
        "        history[\"rewards\"].append(episode_reward)\n",
        "        history[\"costs\"].append(info[\"total_cost\"])\n",
        "        history[\"renewable_ratios\"].append(info[\"renewable_ratio\"])\n",
        "        history[\"epsilons\"].append(agent.epsilon)\n",
        "        if episode_losses:\n",
        "            history[\"losses\"].append(np.mean(episode_losses))\n",
        "        \n",
        "        # Track best\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            agent.save(\"best_model.pt\")\n",
        "        \n",
        "        # Logging\n",
        "        if (episode + 1) % config[\"log_freq\"] == 0:\n",
        "            print(f\"Episode {episode+1:4d} | \"\n",
        "                  f\"Reward: {episode_reward:7.2f} | \"\n",
        "                  f\"Cost: ${info['total_cost']:6.2f} | \"\n",
        "                  f\"Renewable: {info['renewable_ratio']*100:5.1f}% | \"\n",
        "                  f\"\u03b5: {agent.epsilon:.3f}\")\n",
        "    \n",
        "    # Save final model\n",
        "    agent.save(\"final_model.pt\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\u2705 Training complete! Best reward: {best_reward:.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return history, agent, env\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7\ufe0f\u20e3_Evaluation_Funct"
      },
      "outputs": [],
      "source": [
        "#@title 7\ufe0f\u20e3 Evaluation Functions\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\ud83d\udcca EVALUATION - \u0110\u00e1nh gi\u00e1 model\n",
        "================================================================================\n",
        "\n",
        "Functions for:\n",
        "- Testing trained agent\n",
        "- Comparing with random baseline\n",
        "- Generating visualizations\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_agent(agent: DQNAgent, env: MicrogridEnv, \n",
        "                   num_episodes: int = 20, seed: int = 42) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate trained agent.\n",
        "    \u0110\u00e1nh gi\u00e1 agent \u0111\u00e3 hu\u1ea5n luy\u1ec7n.\n",
        "    \"\"\"\n",
        "    \n",
        "    rewards = []\n",
        "    costs = []\n",
        "    renewable_ratios = []\n",
        "    unmet_ratios = []\n",
        "    \n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset(seed=seed + ep)\n",
        "        episode_reward = 0\n",
        "        \n",
        "        while True:\n",
        "            action = agent.select_action(state, training=False)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        costs.append(info[\"total_cost\"])\n",
        "        renewable_ratios.append(info[\"renewable_ratio\"])\n",
        "        unmet_ratios.append(info[\"unmet_ratio\"])\n",
        "    \n",
        "    return {\n",
        "        \"mean_reward\": np.mean(rewards),\n",
        "        \"std_reward\": np.std(rewards),\n",
        "        \"mean_cost\": np.mean(costs),\n",
        "        \"mean_renewable\": np.mean(renewable_ratios),\n",
        "        \"mean_unmet\": np.mean(unmet_ratios),\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_random(env: MicrogridEnv, num_episodes: int = 20, seed: int = 42) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate random baseline.\n",
        "    \u0110\u00e1nh gi\u00e1 baseline ng\u1eabu nhi\u00ean.\n",
        "    \"\"\"\n",
        "    \n",
        "    rewards = []\n",
        "    costs = []\n",
        "    renewable_ratios = []\n",
        "    unmet_ratios = []\n",
        "    \n",
        "    for ep in range(num_episodes):\n",
        "        state = env.reset(seed=seed + ep)\n",
        "        episode_reward = 0\n",
        "        \n",
        "        while True:\n",
        "            action = random.randint(0, 4)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        costs.append(info[\"total_cost\"])\n",
        "        renewable_ratios.append(info[\"renewable_ratio\"])\n",
        "        unmet_ratios.append(info[\"unmet_ratio\"])\n",
        "    \n",
        "    return {\n",
        "        \"mean_reward\": np.mean(rewards),\n",
        "        \"std_reward\": np.std(rewards),\n",
        "        \"mean_cost\": np.mean(costs),\n",
        "        \"mean_renewable\": np.mean(renewable_ratios),\n",
        "        \"mean_unmet\": np.mean(unmet_ratios),\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_results(history: Dict):\n",
        "    \"\"\"\n",
        "    Generate training visualization plots.\n",
        "    T\u1ea1o bi\u1ec3u \u0111\u1ed3 tr\u1ef1c quan h\u00f3a training.\n",
        "    \"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle(\"\ud83d\udd0b DQN Training Results for Microgrid Optimization\", fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Reward curve\n",
        "    ax = axes[0, 0]\n",
        "    rewards = history[\"rewards\"]\n",
        "    ax.plot(rewards, alpha=0.4, label=\"Raw\")\n",
        "    if len(rewards) >= 20:\n",
        "        smoothed = np.convolve(rewards, np.ones(20)/20, mode='valid')\n",
        "        ax.plot(range(19, len(rewards)), smoothed, linewidth=2, label=\"Smoothed (MA20)\")\n",
        "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Episode Reward\")\n",
        "    ax.set_title(\"Reward Curve\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Cost curve\n",
        "    ax = axes[0, 1]\n",
        "    costs = history[\"costs\"]\n",
        "    ax.plot(costs, alpha=0.4, color='orange', label=\"Raw\")\n",
        "    if len(costs) >= 20:\n",
        "        smoothed = np.convolve(costs, np.ones(20)/20, mode='valid')\n",
        "        ax.plot(range(19, len(costs)), smoothed, linewidth=2, color='red', label=\"Smoothed\")\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Daily Grid Cost ($)\")\n",
        "    ax.set_title(\"Grid Electricity Cost\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Renewable ratio\n",
        "    ax = axes[1, 0]\n",
        "    ratios = [r * 100 for r in history[\"renewable_ratios\"]]\n",
        "    ax.plot(ratios, alpha=0.4, color='green', label=\"Raw\")\n",
        "    if len(ratios) >= 20:\n",
        "        smoothed = np.convolve(ratios, np.ones(20)/20, mode='valid')\n",
        "        ax.plot(range(19, len(ratios)), smoothed, linewidth=2, color='darkgreen', label=\"Smoothed\")\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Renewable Usage (%)\")\n",
        "    ax.set_title(\"Renewable Energy Utilization\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Epsilon decay\n",
        "    ax = axes[1, 1]\n",
        "    ax.plot(history[\"epsilons\"], color='purple', linewidth=2)\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(\"Epsilon (\u03b5)\")\n",
        "    ax.set_title(\"Exploration Rate Decay\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\ud83d\udcca Saved: training_curves.png\")\n",
        "\n",
        "\n",
        "def plot_episode_analysis(env: MicrogridEnv):\n",
        "    \"\"\"\n",
        "    Analyze single episode behavior.\n",
        "    Ph\u00e2n t\u00edch h\u00e0nh vi trong m\u1ed9t episode.\n",
        "    \"\"\"\n",
        "    \n",
        "    if not env.episode_history:\n",
        "        print(\"No episode history to plot!\")\n",
        "        return\n",
        "    \n",
        "    history = env.episode_history\n",
        "    hours = [h[\"hour\"] for h in history]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle(\"\ud83d\udd0b 24-Hour Episode Analysis\", fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Energy balance\n",
        "    ax = axes[0, 0]\n",
        "    ax.bar(hours, [h[\"demand\"] for h in history], alpha=0.5, label=\"Demand\", color='red')\n",
        "    ax.bar(hours, [h[\"solar\"] for h in history], alpha=0.7, label=\"Solar\", color='gold')\n",
        "    ax.bar(hours, [h[\"wind\"] for h in history], alpha=0.7, bottom=[h[\"solar\"] for h in history], \n",
        "           label=\"Wind\", color='skyblue')\n",
        "    ax.set_xlabel(\"Hour\")\n",
        "    ax.set_ylabel(\"Power (kW)\")\n",
        "    ax.set_title(\"Energy Supply & Demand\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Battery level\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(hours, [h[\"battery_level\"] for h in history], 'b-', linewidth=2, marker='o', markersize=4)\n",
        "    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5, label=\"Max Capacity\")\n",
        "    ax.set_xlabel(\"Hour\")\n",
        "    ax.set_ylabel(\"Battery Level (kWh)\")\n",
        "    ax.set_title(\"Battery State of Charge\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim(0, 110)\n",
        "    \n",
        "    # Actions taken\n",
        "    ax = axes[1, 0]\n",
        "    action_names = [\"Discharge\", \"Charge\", \"Grid\", \"Renew+Disch\", \"Renew+Grid\"]\n",
        "    actions = [h[\"action\"] for h in history]\n",
        "    colors = ['red', 'green', 'orange', 'blue', 'purple']\n",
        "    for i, h in enumerate(history):\n",
        "        ax.bar(h[\"hour\"], 1, bottom=0, color=colors[h[\"action\"]], alpha=0.7)\n",
        "    ax.set_xlabel(\"Hour\")\n",
        "    ax.set_ylabel(\"Action\")\n",
        "    ax.set_title(\"Actions Taken by Agent\")\n",
        "    ax.set_yticks([])\n",
        "    # Legend\n",
        "    handles = [plt.Rectangle((0,0),1,1, color=c, alpha=0.7) for c in colors]\n",
        "    ax.legend(handles, action_names, loc='upper right', fontsize=8)\n",
        "    \n",
        "    # Cumulative reward\n",
        "    ax = axes[1, 1]\n",
        "    cumulative = np.cumsum([h[\"reward\"] for h in history])\n",
        "    ax.plot(hours, cumulative, 'g-', linewidth=2, marker='o', markersize=4)\n",
        "    ax.fill_between(hours, cumulative, alpha=0.3, color='green')\n",
        "    ax.set_xlabel(\"Hour\")\n",
        "    ax.set_ylabel(\"Cumulative Reward\")\n",
        "    ax.set_title(\"Reward Accumulation\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"episode_analysis.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\ud83d\udcca Saved: episode_analysis.png\")\n",
        "\n",
        "print(\"\u2705 Evaluation functions defined!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8\ufe0f\u20e3_\ud83d\ude80_RUN_TRAINING_&"
      },
      "outputs": [],
      "source": [
        "#@title 8\ufe0f\u20e3 \ud83d\ude80 RUN TRAINING & EVALUATION\n",
        "\"\"\"\n",
        "================================================================================\n",
        "\ud83c\udfaf MAIN EXECUTION - Th\u1ef1c thi ch\u00ednh\n",
        "================================================================================\n",
        "\n",
        "Run this cell to train and evaluate the DQN agent!\n",
        "Ch\u1ea1y cell n\u00e0y \u0111\u1ec3 hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1 agent DQN!\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Train the agent\n",
        "history, agent, env = train(CONFIG, device)\n",
        "\n",
        "# Plot training results\n",
        "plot_results(history)\n",
        "\n",
        "# Evaluate trained agent\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\ud83d\udcca EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "agent_metrics = evaluate_agent(agent, env, num_episodes=20)\n",
        "random_metrics = evaluate_random(env, num_episodes=20)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'Trained Agent':>15} {'Random Baseline':>15} {'Improvement':>12}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Mean Reward':<25} {agent_metrics['mean_reward']:>15.2f} {random_metrics['mean_reward']:>15.2f} \"\n",
        "      f\"{(agent_metrics['mean_reward'] - random_metrics['mean_reward'])/abs(random_metrics['mean_reward'])*100:>+10.1f}%\")\n",
        "print(f\"{'Daily Cost ($)':<25} {agent_metrics['mean_cost']:>15.2f} {random_metrics['mean_cost']:>15.2f} \"\n",
        "      f\"{(random_metrics['mean_cost'] - agent_metrics['mean_cost'])/random_metrics['mean_cost']*100:>+10.1f}%\")\n",
        "print(f\"{'Renewable Usage (%)':<25} {agent_metrics['mean_renewable']*100:>14.1f}% {random_metrics['mean_renewable']*100:>14.1f}% \"\n",
        "      f\"{(agent_metrics['mean_renewable'] - random_metrics['mean_renewable'])*100:>+10.1f}pp\")\n",
        "print(f\"{'Unmet Demand (%)':<25} {agent_metrics['mean_unmet']*100:>14.1f}% {random_metrics['mean_unmet']*100:>14.1f}% \"\n",
        "      f\"{(random_metrics['mean_unmet'] - agent_metrics['mean_unmet'])*100:>+10.1f}pp\")\n",
        "\n",
        "# Run and analyze a demo episode\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\ud83c\udfae DEMO EPISODE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "state = env.reset(seed=999)\n",
        "total_reward = 0\n",
        "\n",
        "print(f\"\\n{'Hour':>5} | {'Action':<15} | {'Demand':>8} | {'Solar':>8} | {'Wind':>8} | {'Battery':>8} | {'Reward':>8}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "action_names = [\"Discharge\", \"Charge\", \"Grid\", \"Renew+Disch\", \"Renew+Grid\"]\n",
        "for step in range(24):\n",
        "    action = agent.select_action(state, training=False)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    h = env.episode_history[-1]\n",
        "    print(f\"{h['hour']:>5} | {action_names[action]:<15} | {h['demand']:>8.1f} | {h['solar']:>8.1f} | \"\n",
        "          f\"{h['wind']:>8.1f} | {h['battery_level']:>8.1f} | {reward:>+8.2f}\")\n",
        "    \n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'TOTAL':>5} | {'':15} | {'':>8} | {'':>8} | {'':>8} | {'':>8} | {total_reward:>+8.2f}\")\n",
        "print(f\"\\n\ud83d\udcc8 Total Reward: {total_reward:.2f}\")\n",
        "print(f\"\ud83d\udcb0 Grid Cost: ${info['total_cost']:.2f}\")\n",
        "print(f\"\ud83c\udf31 Renewable Usage: {info['renewable_ratio']*100:.1f}%\")\n",
        "\n",
        "# Plot episode analysis\n",
        "plot_episode_analysis(env)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\u2705 ALL DONE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"  \ud83d\udcc1 best_model.pt - Best trained model weights\")\n",
        "print(\"  \ud83d\udcc1 final_model.pt - Final trained model weights\")\n",
        "print(\"  \ud83d\udcc1 training_curves.png - Training visualization\")\n",
        "print(\"  \ud83d\udcc1 episode_analysis.png - 24-hour episode analysis\")\n",
        "\n"
      ]
    }
  ]
}