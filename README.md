# ğŸ”‹ Microgrid Energy Optimization using Deep Reinforcement Learning

## Tá»‘i Æ¯u HÃ³a NÄƒng LÆ°á»£ng Microgrid Sá»­ Dá»¥ng Deep Reinforcement Learning (DQN & PPO)

### ğŸ“ Files

| File | MÃ´ Táº£ | Äá»‘i TÆ°á»£ng |
|------|-------|-----------|
| **`Microgrid_DQN_Simple.ipynb`** | â­ DQN Ä‘Æ¡n giáº£n, CHá»ˆ 3 BÆ¯á»šC | NgÆ°á»i má»›i báº¯t Ä‘áº§u |
| **`Microgrid_PPO_Simple.ipynb`** | â­ PPO Ä‘Æ¡n giáº£n, CHá»ˆ 3 BÆ¯á»šC | NgÆ°á»i má»›i báº¯t Ä‘áº§u |
| `Microgrid_DQN_Colab.py` | DQN phiÃªn báº£n Ä‘áº§y Ä‘á»§, chi tiáº¿t | Sinh viÃªn nÃ¢ng cao |
| `Microgrid_PPO_Colab.py` | PPO phiÃªn báº£n Ä‘áº§y Ä‘á»§, chi tiáº¿t | Sinh viÃªn nÃ¢ng cao |
| `REPORT_DQN.md` | BÃ¡o cÃ¡o chi tiáº¿t phÆ°Æ¡ng phÃ¡p DQN | Táº¥t cáº£ |
| `REPORT_PPO.md` | BÃ¡o cÃ¡o chi tiáº¿t phÆ°Æ¡ng phÃ¡p PPO | Táº¥t cáº£ |
| `REPORT.md` | BÃ¡o cÃ¡o tá»•ng há»£p Ä‘á»“ Ã¡n | Táº¥t cáº£ |

---

## ğŸ”€ SO SÃNH DQN vs PPO â€” Hai PhÆ°Æ¡ng PhÃ¡p RL KhÃ¡c Nhau

### Tá»•ng Quan

Dá»± Ã¡n nÃ y triá»ƒn khai **hai nhÃ³m thuáº­t toÃ¡n RL** cho cÃ¹ng bÃ i toÃ¡n microgrid:

| | **DQN** (Deep Q-Network) | **PPO** (Proximal Policy Optimization) |
|---|---|---|
| **NhÃ³m** | Value-based | Policy-based (Actor-Critic) |
| **Paper** | Mnih et al., 2015 (Nature) | Schulman et al., 2017 (OpenAI) |
| **Ã tÆ°á»Ÿng** | Há»c giÃ¡ trá»‹ Q(s,a) â†’ chá»n action cÃ³ Q cao nháº¥t | Há»c trá»±c tiáº¿p policy Ï€(a\|s) â†’ sample action tá»« distribution |

### Kiáº¿n TrÃºc Máº¡ng

```
DQN:    State(8) â†’ [256â†’256â†’128] â†’ Q-values(5)      â† 1 head, output Q cho má»—i action
PPO:    State(8) â†’ [128â†’128] â†’ Actor: Ï€(a|s)(5)      â† 2 heads: policy + value
                             â†’ Critic: V(s)(1)
```

| Äáº·c Ä‘iá»ƒm | DQN | PPO |
|-----------|-----|-----|
| **Output** | Q(s, aâ‚€)...Q(s, aâ‚„) | Ï€(a\|s) + V(s) |
| **Activation** | ReLU | Tanh |
| **Initialization** | Xavier | Orthogonal |
| **Regularization** | Dropout 0.1 | Entropy bonus |

### CÃ¡ch Chá»n Action

```
DQN (Îµ-greedy):                    PPO (stochastic policy):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ if random() < Îµ:     â”‚           â”‚ probs = Actor(state) â”‚
â”‚   action = random()  â”‚           â”‚ action = sample(probs)â”‚
â”‚ else:                â”‚           â”‚                      â”‚
â”‚   action = argmax Q  â”‚           â”‚ â†’ Tá»° NHIÃŠN explore   â”‚
â”‚                      â”‚           â”‚   (entropy cao =     â”‚
â”‚ â†’ Cáº¦N Îµ-greedy      â”‚           â”‚    Ä‘a dáº¡ng action)   â”‚
â”‚   Ä‘á»ƒ explore         â”‚           â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡ch Training

```
DQN (Off-policy):                  PPO (On-policy):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TÆ°Æ¡ng tÃ¡c env     â”‚           â”‚ 1. Thu tháº­p rollout  â”‚
â”‚ 2. LÆ°u replay buffer â”‚           â”‚    (nhiá»u episodes)  â”‚
â”‚ 3. Random sample     â”‚           â”‚ 2. TÃ­nh GAE advantageâ”‚
â”‚    batch (64)        â”‚           â”‚ 3. Clipped update    â”‚
â”‚ 4. MSE loss cáº­p nháº­t â”‚           â”‚    (10 epochs)       â”‚
â”‚ 5. Sync target net   â”‚           â”‚ 4. Clear buffer      â”‚
â”‚    (má»—i 1000 steps)  â”‚           â”‚    (dÃ¹ng 1 láº§n!)     â”‚
â”‚                      â”‚           â”‚                      â”‚
â”‚ Update: Má»–I STEP     â”‚           â”‚ Update: Má»–I 4 EP     â”‚
â”‚ Data: TÃI Sá»¬ Dá»¤NG   â”‚           â”‚ Data: DÃ™NG 1 Láº¦N     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stability Trick

| Váº¥n Ä‘á» | DQN | PPO |
|--------|-----|-----|
| **Moving target** | Target Network (copy má»—i 1000 steps) | â€” |
| **Policy collapse** | â€” | Clipped objective (ratio âˆˆ [0.8, 1.2]) |
| **Gradient explosion** | Gradient clipping | Gradient clipping |
| **Overestimation** | Double DQN | KhÃ´ng cÃ³ váº¥n Ä‘á» nÃ y |

### CÃ´ng Thá»©c ChÃ­nh

```
DQN Loss:   L = (Q(s,a) - [r + Î³ Ã— max Q_target(s',a')])Â²

PPO Loss:   L = -min(ratio Ã— A, clip(ratio, 1-Îµ, 1+Îµ) Ã— A)
            + 0.5 Ã— MSE(V_pred, V_target)
            - 0.01 Ã— Entropy(Ï€)

            ratio = Ï€_new(a|s) / Ï€_old(a|s)
            A = GAE advantage
```

### Khi NÃ o DÃ¹ng CÃ¡i NÃ o?

| TÃ¬nh huá»‘ng | Chá»n | LÃ½ do |
|------------|------|-------|
| Action rá»i ráº¡c, Ã­t action | **DQN** | DQN tá»‘i Æ°u cho discrete |
| Cáº§n sample efficient | **DQN** | Replay buffer tÃ¡i sá»­ dá»¥ng data |
| Muá»‘n Ä‘Æ¡n giáº£n, dá»… debug | **DQN** | Ãt hyperparameters hÆ¡n |
| Muá»‘n policy smooth | **PPO** | XÃ¡c suáº¥t thay Ä‘á»•i mÆ°á»£t mÃ  |
| CÃ³ thá»ƒ má»Ÿ rá»™ng continuous | **PPO** | Dá»… scale sang continuous action |
| Cáº§n robust, Ã­t tuning | **PPO** | Clipped objective tá»± á»•n Ä‘á»‹nh |

---

## ğŸ“Š Káº¿t Quáº£ Training

### Performance Metrics

| Metric | Trained Agent | Random Baseline | Improvement |
|--------|---------------|-----------------|-------------|
| **Mean Episode Reward** | +14.77 | +10.72 | **+37.8%** |
| **Mean Daily Cost** | $64.78 | $77.02 | **-15.9%** |
| **Renewable Usage Ratio** | 66.9% | 58.0% | +8.9% |

### ğŸ¯ Random Baseline lÃ  gÃ¬?

**Random Baseline** lÃ  má»™t agent **khÃ´ng há»c gÃ¬ cáº£**, chá»‰ Ä‘Æ¡n giáº£n chá»n hÃ nh Ä‘á»™ng ngáº«u nhiÃªn á»Ÿ má»—i bÆ°á»›c:

```python
def select_action(self, state):
    return random.randint(0, 4)  # Ngáº«u nhiÃªn 1 trong 5 hÃ nh Ä‘á»™ng
```

**Táº¡i sao dÃ¹ng Random Baseline?**

- ÄÃ¡nh giÃ¡ hiá»‡u quáº£ há»c: Xem agent cÃ³ thá»±c sá»± "há»c" Ä‘Æ°á»£c khÃ´ng
- Äo lÆ°á»ng cáº£i thiá»‡n: TÃ­nh % improvement so vá»›i baseline
- TiÃªu chuáº©n cÆ¡ báº£n: Báº¥t ká»³ thuáº­t toÃ¡n nÃ o cÅ©ng pháº£i tá»‘t hÆ¡n random

**Random agent** chá»n hÃ nh Ä‘á»™ng khÃ´ng theo logic â†’ thÆ°á»ng xáº£ pin khi giÃ¡ Ä‘iá»‡n tháº¥p (lÃ£ng phÃ­), mua Ä‘iá»‡n lÆ°á»›i khi giÃ¡ cao (tá»‘n tiá»n).

**Trained DQN agent** Ä‘Ã£ há»c Ä‘Æ°á»£c â†’ sáº¡c pin khi nÄƒng lÆ°á»£ng tÃ¡i táº¡o dá»“i dÃ o, xáº£ pin khi giÃ¡ Ä‘iá»‡n cao, tá»‘i Ä‘a hÃ³a dÃ¹ng solar/wind.

### Training Curves

![Training Curves](evaluation_results/training_curves.png)

### Agent vs Random Comparison

![Agent vs Random](evaluation_results/agent_vs_random.png)

### 24-Hour Episode Analysis

![Episode Analysis](evaluation_results/episode_analysis.png)

### Key Conclusions

âœ… **DQN agent Ä‘Ã£ há»c thÃ nh cÃ´ng chÃ­nh sÃ¡ch phÃ¢n phá»‘i nÄƒng lÆ°á»£ng:**

- Tá»‘i Ä‘a hÃ³a sá»­ dá»¥ng nguá»“n nÄƒng lÆ°á»£ng tÃ¡i táº¡o (66.9%)
- Giáº£m chi phÃ­ mua Ä‘iá»‡n tá»« lÆ°á»›i tá»›i 15.9%
- Cáº£i thiá»‡n reward so vá»›i random baseline: +37.8%
- Duy trÃ¬ má»©c pin há»£p lÃ½ cho giá» cao Ä‘iá»ƒm

---

## âš–ï¸ Ethical, Practical & Future Considerations

### 1. Váº¥n Äá» Äáº¡o Äá»©c (Ethical Concerns)

| Váº¥n Ä‘á» | MÃ´ táº£ | Giáº£i phÃ¡p Ä‘á» xuáº¥t |
|--------|-------|-------------------|
| **CÃ´ng báº±ng nÄƒng lÆ°á»£ng** | AI cÃ³ thá»ƒ Æ°u tiÃªn tá»‘i Æ°u chi phÃ­ thay vÃ¬ Ä‘áº£m báº£o Ä‘iá»‡n cho táº¥t cáº£ | ThÃªm constraint Ä‘áº£m báº£o demand satisfaction tá»‘i thiá»ƒu |
| **Transparency** | Black-box decision lÃ m ngÆ°á»i dÃ¹ng khÃ³ hiá»ƒu | Explainable AI (XAI), visualization cá»§a policy |
| **Bias trong data** | Model trained trÃªn data mÃ¹a hÃ¨ cÃ³ thá»ƒ fail vÃ o mÃ¹a Ä‘Ã´ng | Äa dáº¡ng hÃ³a training data, continuous learning |
| **Privacy** | Thu tháº­p dá»¯ liá»‡u tiÃªu thá»¥ cÃ³ thá»ƒ xÃ¢m pháº¡m quyá»n riÃªng tÆ° | Federated Learning, data anonymization |

### 2. ThÃ¡ch Thá»©c Triá»ƒn Khai Thá»±c Táº¿ (Practical Deployment)

**Challenges:**

- ğŸ”§ **Hardware**: Cáº§n edge device Ä‘á»§ máº¡nh Ä‘á»ƒ cháº¡y inference real-time
- ğŸ“¡ **Connectivity**: Máº¥t káº¿t ná»‘i internet â†’ cáº§n fallback policy
- ğŸ”‹ **Battery degradation**: Model cáº§n cáº­p nháº­t khi pin xuá»‘ng cáº¥p
- âš¡ **Safety**: Cáº§n mechanism override manual khi AI quyáº¿t Ä‘á»‹nh sai
- ğŸ“Š **Monitoring**: Cáº§n dashboard giÃ¡m sÃ¡t hiá»‡u suáº¥t Agent

**Recommendations:**

```
1. Hybrid approach: RL + Rule-based fallback
2. Edge deployment vá»›i periodic cloud sync
3. Human-in-the-loop cho critical decisions
4. A/B testing trÆ°á»›c khi full deployment
```

### 3. HÆ°á»›ng PhÃ¡t Triá»ƒn TÆ°Æ¡ng Lai (Future Enhancements)

| Cáº£i tiáº¿n | MÃ´ táº£ | Lá»£i Ã­ch tiá»m nÄƒng |
|----------|-------|-------------------|
| **Multi-Agent RL** | Nhiá»u agent quáº£n lÃ½ cÃ¡c zone khÃ¡c nhau | Scalable cho grid lá»›n |
| **PPO/A2C** âœ… | Thuáº­t toÃ¡n Policy Gradient thay DQN | ÄÃ£ triá»ƒn khai trong project |
| **Continuous Actions** | DÃ¹ng DDPG/SAC cho action liÃªn tá»¥c | Äiá»u khiá»ƒn chÃ­nh xÃ¡c hÆ¡n |
| **Demand Forecasting** | Káº¿t há»£p LSTM dá»± Ä‘oÃ¡n demand | Proactive planning |
| **Multi-objective RL** | Tá»‘i Æ°u Ä‘á»“ng thá»i cost, reliability, emissions | CÃ¢n báº±ng nhiá»u má»¥c tiÃªu |
| **Transfer Learning** | Train 1 láº§n, deploy nhiá»u microgrid | Giáº£m thá»i gian training |
| **Digital Twin** | MÃ´ phá»ng trÆ°á»›c khi deploy tháº­t | An toÃ n, test edge cases |

### 4. Káº¿t Luáº­n Tá»•ng Quan

> Dá»± Ã¡n Ä‘Ã£ chá»©ng minh **Deep Reinforcement Learning (DQN)** cÃ³ thá»ƒ:
>
> - âœ… Há»c Ä‘Æ°á»£c chÃ­nh sÃ¡ch tá»‘i Æ°u phÃ¢n phá»‘i nÄƒng lÆ°á»£ng
> - âœ… Cáº£i thiá»‡n **+37.8%** reward so vá»›i random baseline
> - âœ… Giáº£m **15.9%** chi phÃ­ mua Ä‘iá»‡n tá»« lÆ°á»›i
> - âœ… Äáº¡t **66.9%** tá»· lá»‡ sá»­ dá»¥ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o
>
> Tuy nhiÃªn, viá»‡c triá»ƒn khai thá»±c táº¿ cáº§n xem xÃ©t Ä‘áº§y Ä‘á»§ cÃ¡c yáº¿u tá»‘ Ä‘áº¡o Ä‘á»©c, an toÃ n vÃ  kháº£ nÄƒng má»Ÿ rá»™ng cá»§a há»‡ thá»‘ng.

---

## ğŸ“ˆ Data Generation (Simulated Environment)

Theo yÃªu cáº§u Ä‘á» bÃ i, dá»¯ liá»‡u Ä‘Æ°á»£c **mÃ´ phá»ng ngáº«u nhiÃªn theo mÃ´ hÃ¬nh xÃ¡c suáº¥t**, khÃ´ng sá»­ dá»¥ng dataset tá»« nguá»“n bÃªn ngoÃ i:

| ThÃ nh pháº§n | CÃ´ng thá»©c mÃ´ phá»ng | Ã nghÄ©a |
|------------|---------------------|---------|
| **Solar** | `sin((hour-6) Ã— Ï€/12) Ã— 50 Ã— random(0.8, 1.2)` | Cao nháº¥t lÃºc 12h, cÃ³ nhiá»…u |
| **Wind** | `30 Ã— random()` | HoÃ n toÃ n ngáº«u nhiÃªn 0-30 kW |
| **Demand** | `40 + 20 Ã— sin((hour-6) Ã— Ï€/12) + random(0, 10)` | Cao nháº¥t buá»•i chiá»u, cÃ³ nhiá»…u |
| **Price** | `0.15 + 0.1 Ã— (17â‰¤hourâ‰¤21)` | GiÃ¡ cao giá» cao Ä‘iá»ƒm 17h-21h |

```python
# VÃ­ dá»¥ code sinh dá»¯ liá»‡u
solar = max(0, np.sin((hour - 6) * np.pi / 12)) * 50 * (0.8 + 0.4 * np.random.random())
wind = 30 * np.random.random()
demand = 40 + 20 * np.sin((hour - 6) * np.pi / 12) + 10 * np.random.random()
```

> **LÆ°u Ã½:** ÄÃ¢y lÃ  **Stochastic Simulation** theo Ä‘Ãºng yÃªu cáº§u cá»§a Ä‘á» bÃ i, khÃ´ng pháº£i dataset thá»±c táº¿.

---

## ğŸ§  Deep Learning & Reinforcement Learning Pipeline

### Pipeline Tá»•ng Quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MICROGRID RL PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ ENVIRONMENT â”‚ â†’  â”‚ RL AGENT    â”‚ â†’  â”‚ NEURAL NET  â”‚         â”‚
â”‚  â”‚ (Microgrid) â”‚    â”‚ (Decision)  â”‚    â”‚ (DQN)       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚        â†“                   â†“                  â†“                 â”‚
â”‚   State (8D)         Action (5)         Q-values               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RL á» Giai Äoáº¡n NÃ o? TÃ¡c Äá»™ng Ra Sao?

| Giai Äoáº¡n | CÃ´ng Nghá»‡ | Vai TrÃ² | TÃ¡c Äá»™ng |
|-----------|-----------|---------|----------|
| **Perception** | State Vector | Thu tháº­p 8 features tá»« mÃ´i trÆ°á»ng | Hiá»ƒu tÃ¬nh huá»‘ng hiá»‡n táº¡i |
| **Decision** | **RL Agent** | Chá»n 1 trong 5 hÃ nh Ä‘á»™ng | Ra quyáº¿t Ä‘á»‹nh thÃ´ng minh |
| **Learning** | **Deep Learning** | Neural network há»c Q-function | Cáº£i thiá»‡n theo thá»i gian |
| **Action** | Environment | Thá»±c thi hÃ nh Ä‘á»™ng, nháº­n reward | Thay Ä‘á»•i tráº¡ng thÃ¡i há»‡ thá»‘ng |

### DQN (Deep Q-Network) LÃ  GÃ¬?

**DQN = Q-Learning + Deep Neural Network**

- **Q-Learning**: Thuáº­t toÃ¡n RL há»c giÃ¡ trá»‹ cá»§a má»—i hÃ nh Ä‘á»™ng
- **Deep Neural Network**: Xáº¥p xá»‰ Q-function cho state space liÃªn tá»¥c

```
DQN Architecture:
   State (8D)  â”€â”€â–º  [256 â†’ 256 â†’ 128]  â”€â”€â–º  Q-values (5 actions)
                     ReLU    ReLU   ReLU
   
   Q(s,a) = Expected cumulative reward náº¿u chá»n action a á»Ÿ state s
   Agent chá»n: a* = argmax Q(s,a)  (action cÃ³ Q-value cao nháº¥t)
```

### VÃ²ng Láº·p Training

```
Repeat for each episode:
   1. Observe state s (battery, demand, solar, wind, price, time)
   2. Îµ-greedy selection:
      - Random action (vá»›i xÃ¡c suáº¥t Îµ) â†’ Exploration
      - argmax Q(s,a) (vá»›i xÃ¡c suáº¥t 1-Îµ) â†’ Exploitation
   3. Execute action a â†’ receive reward r, observe s'
   4. Store (s, a, r, s', done) in Replay Buffer
   5. Sample random batch from buffer
   6. Compute target: y = r + Î³ Ã— max Q(s',a')  [Bellman equation]
   7. Update network: minimize loss = (Q(s,a) - y)Â²
   8. Repeat until episode ends (24 hours simulated)
```

### Táº¡i Sao DÃ¹ng DQN Thay VÃ¬ Q-Learning ThÃ´ng ThÆ°á»ng?

| Q-Learning | DQN |
|------------|-----|
| Báº£ng Q-table | Neural Network |
| State rá»i ráº¡c | State liÃªn tá»¥c (8D) |
| KhÃ´ng scale | Scale Ä‘Æ°á»£c vá»›i state phá»©c táº¡p |
| KhÃ´ng generalize | Generalize cho state chÆ°a gáº·p |

---

# ğŸ“˜ HÆ¯á»šNG DáºªN CHI TIáº¾T Äá»€ BÃ€I

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Tá»•ng Quan Äá» BÃ i](#1-tá»•ng-quan-Ä‘á»-bÃ i)
2. [Bá»‘i Cáº£nh BÃ i ToÃ¡n](#2-bá»‘i-cáº£nh-bÃ i-toÃ¡n)
3. [Pháº§n 1: MÃ´ Táº£ BÃ i ToÃ¡n (15%)](#3-pháº§n-1-mÃ´-táº£-bÃ i-toÃ¡n-15)
4. [Pháº§n 2: MÃ´ HÃ¬nh HÃ³a MDP (20%)](#4-pháº§n-2-mÃ´-hÃ¬nh-hÃ³a-mdp-20)
5. [Pháº§n 3: Thuáº­t ToÃ¡n RL (25%)](#5-pháº§n-3-thuáº­t-toÃ¡n-rl-25)
6. [Pháº§n 4: PhÃ¢n TÃ­ch Tá»‘i Æ¯u HÃ³a (15%)](#6-pháº§n-4-phÃ¢n-tÃ­ch-tá»‘i-Æ°u-hÃ³a-15)
7. [Pháº§n 5: Káº¿t Quáº£ & ÄÃ¡nh GiÃ¡ (15%)](#7-pháº§n-5-káº¿t-quáº£--Ä‘Ã¡nh-giÃ¡-15)
8. [Pháº§n 6: Äáº¡o Äá»©c & TÆ°Æ¡ng Lai (10%)](#8-pháº§n-6-Ä‘áº¡o-Ä‘á»©c--tÆ°Æ¡ng-lai-10)
9. [YÃªu Cáº§u Ká»¹ Thuáº­t](#9-yÃªu-cáº§u-ká»¹-thuáº­t)
10. [LÆ°u Ã & TÃ i Liá»‡u Tham Kháº£o](#10-lÆ°u-Ã½--tÃ i-liá»‡u-tham-kháº£o)

---

## 1. Tá»•ng Quan Äá» BÃ i

### ğŸ¯ Má»¥c TiÃªu ChÃ­nh

XÃ¢y dá»±ng má»™t **tÃ¡c tá»­ Deep Reinforcement Learning (DQN hoáº·c Policy Gradient)** Ä‘á»ƒ tá»‘i Æ°u hÃ³a viá»‡c phÃ¢n phá»‘i nÄƒng lÆ°á»£ng trong há»‡ thá»‘ng microgrid.

### ğŸ“Œ BÃ i ToÃ¡n Cáº§n Giáº£i Quyáº¿t

PhÃ¢n phá»‘i nÄƒng lÆ°á»£ng trong microgrid lÃ  bÃ i toÃ¡n tá»‘i Æ°u hÃ³a phá»©c táº¡p bao gá»“m:

- Nhu cáº§u nÄƒng lÆ°á»£ng Ä‘á»™ng (thay Ä‘á»•i theo thá»i gian)
- Nguá»“n nÄƒng lÆ°á»£ng tÃ¡i táº¡o biáº¿n thiÃªn (máº·t trá»i, giÃ³)
- RÃ ng buá»™c vá» lÆ°u trá»¯ (dung lÆ°á»£ng pin há»¯u háº¡n)
- Tá»‘i thiá»ƒu hÃ³a chi phÃ­

### ğŸ† Ba Má»¥c TiÃªu Tá»‘i Æ¯u ChÃ­nh

1. **Tiáº¿t kiá»‡m chi phÃ­**: Giáº£m thiá»ƒu chi phÃ­ mua Ä‘iá»‡n tá»« lÆ°á»›i chÃ­nh
2. **Äá»™ tin cáº­y**: Äáº£m báº£o luÃ´n Ä‘Ã¡p á»©ng Ä‘á»§ nhu cáº§u tiÃªu thá»¥ Ä‘iá»‡n
3. **NÄƒng lÆ°á»£ng xanh**: Tá»‘i Ä‘a hÃ³a viá»‡c sá»­ dá»¥ng nÄƒng lÆ°á»£ng máº·t trá»i vÃ  giÃ³

### ğŸ“Š Ká»¹ NÄƒng ÄÆ°á»£c ÄÃ¡nh GiÃ¡

- MÃ´ hÃ¬nh hÃ³a bÃ i toÃ¡n thá»±c táº¿ thÃ nh Markov Decision Process (MDP)
- CÃ i Ä‘áº·t thuáº­t toÃ¡n RL tiÃªn tiáº¿n
- PhÃ¢n tÃ­ch hiá»‡u suáº¥t tá»‘i Æ°u hÃ³a vÃ  cÃ¡c háº¡n cháº¿
- ÄÃ¡nh giÃ¡ phÃª phÃ¡n vá» triá»ƒn khai, Ä‘áº¡o Ä‘á»©c vÃ  cáº£i tiáº¿n tÆ°Æ¡ng lai

---

## 2. Bá»‘i Cáº£nh BÃ i ToÃ¡n

### ğŸ”‹ Há»‡ Thá»‘ng Microgrid LÃ  GÃ¬?

**Microgrid** lÃ  má»™t máº¡ng lÆ°á»›i Ä‘iá»‡n quy mÃ´ nhá», cá»¥c bá»™, tÃ­ch há»£p:

- CÃ¡c nguá»“n nÄƒng lÆ°á»£ng phÃ¢n tÃ¡n
- Há»‡ thá»‘ng lÆ°u trá»¯
- Táº£i Ä‘iá»‡n cÃ³ thá»ƒ Ä‘iá»u khiá»ƒn

Microgrid cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng **káº¿t ná»‘i vá»›i lÆ°á»›i Ä‘iá»‡n chÃ­nh** hoáº·c **Ä‘á»™c láº­p**.

### ğŸ—ï¸ CÃ¡c ThÃ nh Pháº§n Cá»§a Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Há»† THá»NG MICROGRID                              â”‚
â”‚                                                                         â”‚
â”‚   â˜€ï¸ NÄ‚NG LÆ¯á»¢NG       ğŸŒ¬ï¸ NÄ‚NG LÆ¯á»¢NG       ğŸ”‹ PIN LÆ¯U        âš¡ LÆ¯á»šI    â”‚
â”‚      Máº¶T TRá»œI            GIÃ“               TRá»®              ÄIá»†N       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  PV  â”‚           â”‚ Wind â”‚           â”‚ BESS â”‚         â”‚Utilityâ”‚     â”‚
â”‚   â”‚Panelsâ”‚           â”‚Turbineâ”‚          â”‚100kWhâ”‚         â”‚ Grid â”‚      â”‚
â”‚   â””â”€â”€â”¬â”€â”€â”€â”˜           â””â”€â”€â”¬â”€â”€â”€â”˜           â””â”€â”€â”¬â”€â”€â”€â”˜         â””â”€â”€â”¬â”€â”€â”€â”˜      â”‚
â”‚      â”‚                  â”‚                   â”‚                â”‚          â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                   â”‚                                     â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                          â”‚   Há»† THá»NG      â”‚                            â”‚
â”‚                          â”‚  QUáº¢N LÃ        â”‚  â—„â”€â”€ ğŸ¤– DQN Agent         â”‚
â”‚                          â”‚  NÄ‚NG LÆ¯á»¢NG     â”‚                            â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                   â”‚                                     â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                     â”‚                           â”‚                       â”‚
â”‚               ğŸ  Táº£i Há»™               ğŸ­ Táº£i CÃ´ng                       â”‚
â”‚                 Gia ÄÃ¬nh                  Nghiá»‡p                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| ThÃ nh pháº§n | MÃ´ táº£ chi tiáº¿t |
|------------|----------------|
| **Pin máº·t trá»i (Solar PV)** | PhÃ¡t Ä‘iá»‡n tá»« Ã¡nh sÃ¡ng máº·t trá»i, sáº£n lÆ°á»£ng cao nháº¥t vÃ o buá»•i trÆ°a, khÃ´ng hoáº¡t Ä‘á»™ng ban Ä‘Ãªm |
| **Turbine giÃ³ (Wind)** | PhÃ¡t Ä‘iá»‡n tá»« giÃ³, sáº£n lÆ°á»£ng phá»¥ thuá»™c vÃ o tá»‘c Ä‘á»™ giÃ³, cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng cáº£ ngÃ y láº«n Ä‘Ãªm |
| **Pin lÆ°u trá»¯ (BESS)** | Battery Energy Storage System, dung lÆ°á»£ng 100kWh, lÆ°u nÄƒng lÆ°á»£ng dÆ° thá»«a Ä‘á»ƒ dÃ¹ng khi cáº§n |
| **LÆ°á»›i Ä‘iá»‡n (Utility Grid)** | Nguá»“n Ä‘iá»‡n dá»± phÃ²ng, cÃ³ thá»ƒ mua khi thiáº¿u Ä‘iá»‡n, giÃ¡ biáº¿n Ä‘á»™ng theo giá» |
| **Táº£i tiÃªu thá»¥** | Nhu cáº§u Ä‘iá»‡n cá»§a há»™ gia Ä‘Ã¬nh vÃ  cÃ´ng nghiá»‡p, thay Ä‘á»•i theo thá»i gian trong ngÃ y |

### â“ Táº¡i Sao LÃ  BÃ i ToÃ¡n Ra Quyáº¿t Äá»‹nh Tuáº§n Tá»±?

#### Äáº·c Ä‘iá»ƒm 1: LiÃªn Káº¿t Thá»i Gian (Temporal Coupling)

- Tráº¡ng thÃ¡i pin táº¡i thá»i Ä‘iá»ƒm **t+1** phá»¥ thuá»™c vÃ o hÃ nh Ä‘á»™ng táº¡i thá»i Ä‘iá»ƒm **t**
- VÃ­ dá»¥: Náº¿u sáº¡c pin lÃºc 12h trÆ°a â†’ pin Ä‘áº§y lÃºc 6h tá»‘i Ä‘á»ƒ dÃ¹ng

#### Äáº·c Ä‘iá»ƒm 2: Háº­u Quáº£ TrÃ¬ HoÃ£n (Delayed Consequences)

- TÃ¡c Ä‘á»™ng cá»§a quyáº¿t Ä‘á»‹nh cÃ³ thá»ƒ khÃ´ng tháº¥y ngay láº­p tá»©c
- VÃ­ dá»¥: Xáº£ pin háº¿t lÃºc 5h chiá»u â†’ khÃ´ng cÃ³ pin dÃ¹ng lÃºc 7h tá»‘i giÃ¡ cao

#### Äáº·c Ä‘iá»ƒm 3: ÄÃ¡nh Äá»•i Ngáº¯n Háº¡n vs DÃ i Háº¡n

- Tá»‘i Ä‘a hÃ³a lá»£i Ã­ch ngay cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng xáº¥u Ä‘áº¿n hiá»‡u suáº¥t tÆ°Æ¡ng lai
- VÃ­ dá»¥: BÃ¡n Ä‘iá»‡n lÃºc trÆ°a (giÃ¡ trung bÃ¬nh) vs giá»¯ láº¡i dÃ¹ng lÃºc tá»‘i (giÃ¡ cao)

### ğŸ”„ Háº¡n Cháº¿ Cá»§a PhÆ°Æ¡ng PhÃ¡p Truyá»n Thá»‘ng

| PhÆ°Æ¡ng phÃ¡p | CÃ¡ch hoáº¡t Ä‘á»™ng | Háº¡n cháº¿ chi tiáº¿t |
|-------------|----------------|------------------|
| **Rule-based** | Quy táº¯c if-then cá»‘ Ä‘á»‹nh | KhÃ´ng thá»ƒ thÃ­ch á»©ng vá»›i thay Ä‘á»•i; bá» lá»¡ cÆ¡ há»™i tá»‘i Æ°u |
| **Linear Programming** | Tá»‘i Æ°u hÃ³a toÃ¡n há»c | Cáº§n dá»± bÃ¡o hoÃ n háº£o vá» tÆ°Æ¡ng lai; giáº£ Ä‘á»‹nh quan há»‡ tuyáº¿n tÃ­nh khÃ´ng thá»±c táº¿ |
| **Model Predictive Control** | Tá»‘i Æ°u hÃ³a theo horizon cuá»™n | Chi phÃ­ tÃ­nh toÃ¡n ráº¥t cao; cáº§n mÃ´ hÃ¬nh chÃ­nh xÃ¡c cá»§a há»‡ thá»‘ng |
| **Heuristics (GA, PSO)** | Thuáº­t toÃ¡n di truyá»n, báº§y Ä‘Ã n | KhÃ´ng Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c lá»i giáº£i tá»‘i Æ°u toÃ n cá»¥c; phá»¥ thuá»™c bÃ i toÃ¡n cá»¥ thá»ƒ |

### âœ… Æ¯u Äiá»ƒm Cá»§a Reinforcement Learning

1. **Há»c khÃ´ng cáº§n mÃ´ hÃ¬nh (Model-free)**: KhÃ´ng cáº§n mÃ´ hÃ¬nh toÃ¡n há»c tÆ°á»ng minh cá»§a há»‡ thá»‘ng
2. **ThÃ­ch á»©ng liÃªn tá»¥c**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh theo Ä‘iá»u kiá»‡n thay Ä‘á»•i
3. **Tá»‘i Æ°u dÃ i háº¡n**: Tá»± nhiÃªn cÃ¢n báº±ng giá»¯a pháº§n thÆ°á»Ÿng ngay vÃ  tÆ°Æ¡ng lai
4. **Xá»­ lÃ½ báº¥t Ä‘á»‹nh**: PhÃ¡t triá»ƒn chÃ­nh sÃ¡ch robust vá»›i nhiá»u ká»‹ch báº£n

---

## 3. Pháº§n 1: MÃ´ Táº£ BÃ i ToÃ¡n (15%)

### ğŸ“ YÃªu Cáº§u Chi Tiáº¿t

Pháº§n nÃ y yÃªu cáº§u báº¡n **giáº£i thÃ­ch** há»‡ thá»‘ng microgrid vÃ  **láº­p luáº­n** táº¡i sao Reinforcement Learning phÃ¹ há»£p hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c.

### ğŸ“¦ Sáº£n Pháº©m Cáº§n Ná»™p

1. **Giáº£i thÃ­ch há»‡ thá»‘ng Microgrid**
2. **LÃ½ do phÃ¢n phá»‘i nÄƒng lÆ°á»£ng lÃ  bÃ i toÃ¡n ra quyáº¿t Ä‘á»‹nh tuáº§n tá»±**
3. **Háº¡n cháº¿ cá»§a phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u thÃ´ng thÆ°á»ng**
4. **Ã nghÄ©a thá»±c tiá»…n vÃ  tÃ¡c Ä‘á»™ng**

### ğŸ“– HÆ°á»›ng Dáº«n Viáº¿t Chi Tiáº¿t

#### 3.1 Giá»›i Thiá»‡u Há»‡ Thá»‘ng Microgrid

**Ná»™i dung cáº§n viáº¿t:**

- Äá»‹nh nghÄ©a microgrid lÃ  gÃ¬
- Sá»± khÃ¡c biá»‡t vá»›i há»‡ thá»‘ng Ä‘iá»‡n táº­p trung truyá»n thá»‘ng
- Æ¯u Ä‘iá»ƒm: Ä‘á»™ tin cáº­y cao, giáº£m tá»•n tháº¥t truyá»n táº£i, tÃ­ch há»£p nÄƒng lÆ°á»£ng tÃ¡i táº¡o tá»‘t hÆ¡n

**MÃ´ táº£ tá»«ng thÃ nh pháº§n:**

```markdown
1. NGUá»’N NÄ‚NG LÆ¯á»¢NG TÃI Táº O
   - Pin máº·t trá»i: Sáº£n lÆ°á»£ng thay Ä‘á»•i theo thá»i tiáº¿t vÃ  thá»i gian trong ngÃ y
   - Turbine giÃ³: Sáº£n lÆ°á»£ng phá»¥ thuá»™c vÃ o tá»‘c Ä‘á»™ giÃ³, cÃ³ thá»ƒ dá»± Ä‘oÃ¡n má»™t pháº§n

2. Há»† THá»NG LÆ¯U TRá»® NÄ‚NG LÆ¯á»¢NG (BESS)
   - Dung lÆ°á»£ng há»¯u háº¡n (100kWh trong Ä‘á» bÃ i)
   - LÆ°u nÄƒng lÆ°á»£ng dÆ° thá»«a khi phÃ¡t nhiá»u
   - Giáº£i phÃ³ng nÄƒng lÆ°á»£ng khi nhu cáº§u cao hoáº·c phÃ¡t Ã­t

3. Káº¾T Ná»I LÆ¯á»šI ÄIá»†N
   - Mua Ä‘iá»‡n khi nguá»“n tÃ¡i táº¡o vÃ  pin khÃ´ng Ä‘á»§
   - GiÃ¡ Ä‘iá»‡n biáº¿n Ä‘á»™ng theo thá»i gian (peak/off-peak)

4. Táº¢I TIÃŠU THá»¤
   - Nhu cáº§u Ä‘iá»‡n tá»•ng há»£p tá»« nhiá»u há»™ gia Ä‘Ã¬nh vÃ  doanh nghiá»‡p
   - CÃ³ pattern theo thá»i gian trong ngÃ y (cao Ä‘iá»ƒm sÃ¡ng, trÆ°a, tá»‘i)
```

#### 3.2 Táº¡i Sao LÃ  BÃ i ToÃ¡n Ra Quyáº¿t Äá»‹nh Tuáº§n Tá»±

**VÃ­ dá»¥ minh há»a:**
> LÃºc 12h trÆ°a, pin Ä‘áº§y vÃ  nÄƒng lÆ°á»£ng máº·t trá»i Ä‘ang á»Ÿ Ä‘á»‰nh. NgÆ°á»i váº­n hÃ nh pháº£i quyáº¿t Ä‘á»‹nh:
>
> - BÃ¡n Ä‘iá»‡n dÆ° cho lÆ°á»›i vá»›i giÃ¡ hiá»‡n táº¡i?
> - Hay giá»¯ láº¡i Ä‘á»ƒ dÃ¹ng vÃ o buá»•i tá»‘i khi giÃ¡ cao hÆ¡n?
>
> Quyáº¿t Ä‘á»‹nh nÃ y phá»¥ thuá»™c khÃ´ng chá»‰ vÃ o Ä‘iá»u kiá»‡n hiá»‡n táº¡i mÃ  cÃ²n dá»± Ä‘oÃ¡n vá» nhu cáº§u tÆ°Æ¡ng lai, sáº£n lÆ°á»£ng phÃ¡t Ä‘iá»‡n, vÃ  giÃ¡ Ä‘iá»‡n.

**Ba Ä‘áº·c Ä‘iá»ƒm cáº§n giáº£i thÃ­ch:**

1. LiÃªn káº¿t thá»i gian
2. Háº­u quáº£ trÃ¬ hoÃ£n
3. Trade-off ngáº¯n háº¡n vÃ  dÃ i háº¡n

#### 3.3 Háº¡n Cháº¿ Cá»§a PhÆ°Æ¡ng PhÃ¡p Truyá»n Thá»‘ng

**Báº£ng so sÃ¡nh chi tiáº¿t:**

| PhÆ°Æ¡ng phÃ¡p | MÃ´ táº£ | Æ¯u Ä‘iá»ƒm | Háº¡n cháº¿ |
|-------------|-------|---------|---------|
| Rule-based | Quy táº¯c if-then cá»‘ Ä‘á»‹nh | ÄÆ¡n giáº£n, dá»… hiá»ƒu | KhÃ´ng thÃ­ch á»©ng, bá» lá»¡ cÆ¡ há»™i |
| Linear Programming | Tá»‘i Æ°u hÃ³a toÃ¡n há»c | CÃ³ lÃ½ thuyáº¿t chá»©ng minh | Cáº§n dá»± bÃ¡o hoÃ n háº£o, giáº£ Ä‘á»‹nh tuyáº¿n tÃ­nh |
| MPC | Tá»‘i Æ°u theo horizon cuá»™n | Xá»­ lÃ½ rÃ ng buá»™c tá»‘t | Chi phÃ­ tÃ­nh toÃ¡n cao, cáº§n mÃ´ hÃ¬nh chÃ­nh xÃ¡c |
| GA/PSO | TÃ¬m kiáº¿m heuristic | Xá»­ lÃ½ khÃ´ng gian phá»©c táº¡p | KhÃ´ng Ä‘áº£m báº£o tá»‘i Æ°u, cháº­m há»™i tá»¥ |

#### 3.4 Ã NghÄ©a Thá»±c Tiá»…n

**CÃ¡c Ä‘iá»ƒm cáº§n Ä‘á» cáº­p:**

- Giáº£m carbon footprint báº±ng viá»‡c tá»‘i Ä‘a hÃ³a nÄƒng lÆ°á»£ng tÃ¡i táº¡o
- Tiáº¿t kiá»‡m chi phÃ­ Ä‘iá»‡n cho há»™ gia Ä‘Ã¬nh vÃ  doanh nghiá»‡p
- TÄƒng Ä‘á»™ á»•n Ä‘á»‹nh lÆ°á»›i Ä‘iá»‡n
- Há»— trá»£ chuyá»ƒn Ä‘á»•i nÄƒng lÆ°á»£ng xanh

---

## 4. Pháº§n 2: MÃ´ HÃ¬nh HÃ³a MDP (20%)

### ğŸ“ YÃªu Cáº§u Chi Tiáº¿t

Chuyá»ƒn Ä‘á»•i bÃ i toÃ¡n microgrid thÃ nh **Markov Decision Process** formal vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c thÃ nh pháº§n.

### ğŸ“¦ Sáº£n Pháº©m Cáº§n Ná»™p

1. **SÆ¡ Ä‘á»“ MDP** (diagram)
2. **Giáº£i thÃ­ch chi tiáº¿t**: States, Actions, Rewards, Transitions
3. **LÃ½ giáº£i** cho cÃ¡c lá»±a chá»n thiáº¿t káº¿

### ğŸ”² MDP Framework

MDP Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi tuple **(S, A, P, R, Î³)**:

- **S**: KhÃ´ng gian tráº¡ng thÃ¡i (State space)
- **A**: KhÃ´ng gian hÃ nh Ä‘á»™ng (Action space)
- **P**: HÃ m chuyá»ƒn Ä‘á»•i (Transition function)
- **R**: HÃ m pháº§n thÆ°á»Ÿng (Reward function)
- **Î³**: Há»‡ sá»‘ chiáº¿t kháº¥u (Discount factor)

### ğŸ“Š KhÃ´ng Gian Tráº¡ng ThÃ¡i (State Space) - 8 Chiá»u

| # | TÃªn biáº¿n | Pháº¡m vi | Ã nghÄ©a | LÃ½ do Ä‘Æ°a vÃ o |
|---|----------|---------|---------|---------------|
| 1 | `battery_level` | [0, 1] | Má»©c nÄƒng lÆ°á»£ng pin Ä‘Ã£ chuáº©n hÃ³a (0=trá»‘ng, 1=Ä‘áº§y) | Quyáº¿t Ä‘á»‹nh cÃ³ thá»ƒ sáº¡c/xáº£ khÃ´ng |
| 2 | `demand` | [0, 1] | Nhu cáº§u tiÃªu thá»¥ Ä‘iá»‡n hiá»‡n táº¡i | Biáº¿t cáº§n bao nhiÃªu Ä‘iá»‡n Ä‘á»ƒ Ä‘Ã¡p á»©ng |
| 3 | `solar_generation` | [0, 1] | CÃ´ng suáº¥t phÃ¡t tá»« pin máº·t trá»i | Biáº¿t cÃ³ bao nhiÃªu Ä‘iá»‡n miá»…n phÃ­ |
| 4 | `wind_generation` | [0, 1] | CÃ´ng suáº¥t phÃ¡t tá»« turbine giÃ³ | Biáº¿t cÃ³ bao nhiÃªu Ä‘iá»‡n miá»…n phÃ­ |
| 5 | `grid_price` | [0, 1] | GiÃ¡ Ä‘iá»‡n lÆ°á»›i hiá»‡n táº¡i (chuáº©n hÃ³a) | Quyáº¿t Ä‘á»‹nh cÃ³ nÃªn mua tá»« lÆ°á»›i |
| 6 | `hour_sin` | [-1, 1] | sin(2Ï€ Ã— hour / 24) | MÃ£ hÃ³a thá»i gian tuáº§n hoÃ n |
| 7 | `hour_cos` | [-1, 1] | cos(2Ï€ Ã— hour / 24) | MÃ£ hÃ³a thá»i gian tuáº§n hoÃ n |
| 8 | `prev_action` | [0, 1] | HÃ nh Ä‘á»™ng á»Ÿ bÆ°á»›c trÆ°á»›c (chuáº©n hÃ³a) | Táº¡o tÃ­nh nháº¥t quÃ¡n trong chÃ­nh sÃ¡ch |

#### ğŸ’¡ Giáº£i ThÃ­ch MÃ£ HÃ³a Thá»i Gian Tuáº§n HoÃ n

**Váº¥n Ä‘á»:** Náº¿u dÃ¹ng hour = 0, 1, 2, ..., 23 trá»±c tiáº¿p:

- Model nghÄ© 23h vÃ  0h ráº¥t xa nhau (khoáº£ng cÃ¡ch = 23)
- NhÆ°ng thá»±c táº¿ chÃºng chá»‰ cÃ¡ch nhau 1 giá»!

**Giáº£i phÃ¡p:** Sá»­ dá»¥ng mÃ£ hÃ³a sin/cos:

```python
hour_sin = sin(2Ï€ Ã— hour / 24)
hour_cos = cos(2Ï€ Ã— hour / 24)
```

**VÃ­ dá»¥:**

- 0h: sin=0, cos=1
- 6h: sin=1, cos=0
- 12h: sin=0, cos=-1
- 18h: sin=-1, cos=0
- 23h: sinâ‰ˆ0, cosâ‰ˆ1 (gáº§n vá»›i 0h!)

### ğŸ® KhÃ´ng Gian HÃ nh Äá»™ng (Action Space) - 5 HÃ nh Äá»™ng Rá»i Ráº¡c

| HÃ nh Ä‘á»™ng | TÃªn | MÃ´ táº£ chi tiáº¿t | Khi nÃ o nÃªn dÃ¹ng |
|-----------|-----|----------------|------------------|
| **0** | Xáº£ pin (Discharge) | DÃ¹ng nÄƒng lÆ°á»£ng trong pin Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u | GiÃ¡ Ä‘iá»‡n lÆ°á»›i cao, pin cÃ²n Ä‘á»§ |
| **1** | Sáº¡c tá»« nÄƒng lÆ°á»£ng tÃ¡i táº¡o | LÆ°u nÄƒng lÆ°á»£ng dÆ° thá»«a vÃ o pin | NÄƒng lÆ°á»£ng tÃ¡i táº¡o > nhu cáº§u, pin chÆ°a Ä‘áº§y |
| **2** | Mua tá»« lÆ°á»›i | Mua toÃ n bá»™ Ä‘iá»‡n cáº§n thiáº¿t tá»« lÆ°á»›i | KhÃ´ng Ä‘á»§ nÄƒng lÆ°á»£ng tÃ¡i táº¡o vÃ  pin trá»‘ng |
| **3** | TÃ¡i táº¡o + Xáº£ pin | Æ¯u tiÃªn dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, thiáº¿u thÃ¬ bá»• sung tá»« pin | CÃ³ nÄƒng lÆ°á»£ng tÃ¡i táº¡o nhÆ°ng khÃ´ng Ä‘á»§ |
| **4** | TÃ¡i táº¡o + LÆ°á»›i | Æ¯u tiÃªn dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o, thiáº¿u thÃ¬ mua tá»« lÆ°á»›i | Pin tháº¥p, cáº§n tiáº¿t kiá»‡m pin cho peak |

**Táº¡i sao dÃ¹ng action rá»i ráº¡c thay vÃ¬ liÃªn tá»¥c?**

- DQN hoáº¡t Ä‘á»™ng tá»‘t nháº¥t vá»›i action rá»i ráº¡c
- Dá»… há»c vÃ  há»™i tá»¥ hÆ¡n
- PhÃ¹ há»£p vá»›i quyáº¿t Ä‘á»‹nh thá»±c táº¿ (chá»n cháº¿ Ä‘á»™ váº­n hÃ nh)

### ğŸ… HÃ m Pháº§n ThÆ°á»Ÿng (Reward Function)

```
R(s, a, s') = R_renewable + R_grid + R_unmet + R_battery + R_bonus
```

#### Chi Tiáº¿t Tá»«ng ThÃ nh Pháº§n

| ThÃ nh pháº§n | CÃ´ng thá»©c | Giáº£i thÃ­ch | TÃ¡c dá»¥ng |
|------------|-----------|------------|----------|
| **R_renewable** | +1.0 Ã— (renewable_used / base_demand) | ThÆ°á»Ÿng khi dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o | Khuyáº¿n khÃ­ch nÄƒng lÆ°á»£ng sáº¡ch |
| **R_grid** | âˆ’2.0 Ã— (grid_purchased / base_demand) Ã— price | Pháº¡t khi mua tá»« lÆ°á»›i, nhÃ¢n vá»›i giÃ¡ | Giáº£m chi phÃ­ Ä‘iá»‡n |
| **R_unmet** | âˆ’5.0 Ã— (unmet_demand / base_demand) | Pháº¡t náº·ng khi khÃ´ng Ä‘á»§ Ä‘iá»‡n | Äáº£m báº£o Ä‘á»™ tin cáº­y |
| **R_battery** | âˆ’0.1 Ã— (charge + discharge) | Pháº¡t nháº¹ cho má»—i láº§n sáº¡c/xáº£ | MÃ´ hÃ¬nh hao mÃ²n pin |
| **R_bonus** | +0.5 náº¿u khÃ´ng dÃ¹ng lÆ°á»›i trong giá» cao Ä‘iá»ƒm | ThÆ°á»Ÿng khi trÃ¡nh Ä‘Æ°á»£c peak hours | Tá»‘i Æ°u thá»i Ä‘iá»ƒm chiáº¿n lÆ°á»£c |

#### ğŸ¯ Thiáº¿t Káº¿ Reward Function

**NguyÃªn táº¯c:**

1. **ThÆ°á»Ÿng > 0** cho hÃ nh vi mong muá»‘n (dÃ¹ng nÄƒng lÆ°á»£ng tÃ¡i táº¡o)
2. **Pháº¡t < 0** cho hÃ nh vi khÃ´ng mong muá»‘n (mua tá»« lÆ°á»›i, thiáº¿u Ä‘iá»‡n)
3. **Trá»ng sá»‘** pháº£n Ã¡nh má»©c Ä‘á»™ quan trá»ng (thiáº¿u Ä‘iá»‡n pháº¡t náº·ng nháº¥t: -5.0)
4. **Chuáº©n hÃ³a** theo base_demand Ä‘á»ƒ reward á»•n Ä‘á»‹nh

### ğŸ”„ Äá»™ng Lá»±c Chuyá»ƒn Äá»•i (Transition Dynamics)

```python
# Cáº­p nháº­t tráº¡ng thÃ¡i pin
new_battery = old_battery + charge_amount - discharge_amount
new_battery = clip(new_battery, 0, max_capacity)  # RÃ ng buá»™c váº­t lÃ½

# Sinh nÄƒng lÆ°á»£ng tÃ¡i táº¡o (stochastic)
solar_gen = solar_pattern(hour) + random_noise()
wind_gen = wind_pattern(hour) + random_noise()

# Nhu cáº§u tiÃªu thá»¥ (probabilistic)
demand = base_demand(hour) + random_variation()
```

**Äáº·c Ä‘iá»ƒm:**

- **Deterministic**: Cáº­p nháº­t pin theo cÃ´ng thá»©c váº­t lÃ½
- **Stochastic**: NÄƒng lÆ°á»£ng tÃ¡i táº¡o cÃ³ nhiá»…u (thá»i tiáº¿t)
- **Probabilistic**: Nhu cáº§u cÃ³ biáº¿n Ä‘á»™ng ngáº«u nhiÃªn

### ğŸ›‘ Äiá»u Kiá»‡n Káº¿t ThÃºc Episode

| Äiá»u kiá»‡n | MÃ´ táº£ | Reward khi káº¿t thÃºc |
|-----------|-------|---------------------|
| Háº¿t ngÃ y/tuáº§n | HoÃ n thÃ nh horizon thá»i gian | Tá»•ng reward tÃ­ch lÅ©y |
| Pin gáº·p sá»± cá»‘ | Battery level = 0 vÃ  demand > supply | Pháº¡t náº·ng |
| VÆ°á»£t ngÆ°á»¡ng unmet | QuÃ¡ nhiá»u láº§n khÃ´ng Ä‘Ã¡p á»©ng Ä‘Æ°á»£c nhu cáº§u | Pháº¡t náº·ng |

---

## 5. Pháº§n 3: Thuáº­t ToÃ¡n RL (25%)

### ğŸ“ YÃªu Cáº§u Chi Tiáº¿t

Chá»n vÃ  cÃ i Ä‘áº·t thuáº­t toÃ¡n RL phÃ¹ há»£p vá»›i bÃ i toÃ¡n.

### ğŸ“¦ Sáº£n Pháº©m Cáº§n Ná»™p

1. **Lá»±a chá»n thuáº­t toÃ¡n** vá»›i lÃ½ giáº£i
2. **Code Python** sá»­ dá»¥ng PyTorch hoáº·c TensorFlow, cÃ³ comment Ä‘áº§y Ä‘á»§
3. **Giáº£i thÃ­ch kiáº¿n trÃºc máº¡ng** vÃ  hyperparameters

### ğŸ¤– So SÃ¡nh CÃ¡c Thuáº­t ToÃ¡n RL

| Thuáº­t toÃ¡n | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p |
|------------|---------|------------|---------|
| **Tabular Q-Learning** | ÄÆ¡n giáº£n, Ä‘áº£m báº£o há»™i tá»¥ | KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c state liÃªn tá»¥c | âŒ |
| **DQN** | Xá»­ lÃ½ state liÃªn tá»¥c, sample efficient | Chá»‰ vá»›i action rá»i ráº¡c | âœ… Ráº¥t phÃ¹ há»£p |
| **Policy Gradient** | Xá»­ lÃ½ action liÃªn tá»¥c | Variance cao, sample inefficient | âš ï¸ CÃ³ thá»ƒ dÃ¹ng |
| **Actor-Critic** | Variance tháº¥p hÆ¡n PG | Phá»©c táº¡p hÆ¡n cáº§n thiáº¿t | âš ï¸ Overcomplicated |

**Khuyáº¿n nghá»‹: Double DQN** - Giáº£i quyáº¿t váº¥n Ä‘á» overestimation cá»§a DQN gá»‘c.

### ğŸ§  Kiáº¿n TrÃºc Neural Network

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         INPUT LAYER (8 neurons)         â”‚
â”‚    [battery, demand, solar, wind,       â”‚
â”‚     price, hour_sin, hour_cos, prev]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HIDDEN LAYER 1: 256 neurons           â”‚
â”‚   Activation: ReLU                      â”‚
â”‚   Dropout: 0.1 (regularization)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HIDDEN LAYER 2: 256 neurons           â”‚
â”‚   Activation: ReLU                      â”‚
â”‚   Dropout: 0.1                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HIDDEN LAYER 3: 128 neurons           â”‚
â”‚   Activation: ReLU                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        OUTPUT LAYER (5 neurons)         â”‚
â”‚   Q(s, aâ‚€), Q(s, aâ‚), Q(s, aâ‚‚),        â”‚
â”‚   Q(s, aâ‚ƒ), Q(s, aâ‚„)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tá»•ng tham sá»‘: ~104,000
Khá»Ÿi táº¡o: Xavier/Glorot
```

### ğŸ”§ CÃ¡c ThÃ nh Pháº§n Quan Trá»ng Cá»§a DQN

#### 1. Experience Replay Buffer

```python
class ReplayBuffer:
    """
    LÆ°u trá»¯ cÃ¡c transition (s, a, r, s', done) Ä‘á»ƒ train offline.
    
    Lá»£i Ã­ch:
    - PhÃ¡ vá»¡ tÆ°Æ¡ng quan thá»i gian giá»¯a cÃ¡c samples
    - TÃ¡i sá»­ dá»¥ng kinh nghiá»‡m nhiá»u láº§n
    - á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh há»c
    """
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size=64):
        # Random sampling Ä‘á»ƒ phÃ¡ correlation
        return random.sample(self.buffer, batch_size)
```

#### 2. Target Network

```python
class DQNAgent:
    def __init__(self):
        self.q_network = QNetwork()        # Máº¡ng chÃ­nh, update liÃªn tá»¥c
        self.target_network = QNetwork()   # Máº¡ng target, update cháº­m
        
    def update_target(self):
        """Copy weights tá»« q_network sang target_network má»—i 1000 steps"""
        self.target_network.load_state_dict(self.q_network.state_dict())
```

**Táº¡i sao cáº§n Target Network?**

- Q-learning update: Q(s,a) â† r + Î³ Ã— max Q(s', a')
- Náº¿u dÃ¹ng cÃ¹ng 1 network Ä‘á»ƒ tÃ­nh cáº£ 2 váº¿ â†’ "moving target" â†’ khÃ´ng á»•n Ä‘á»‹nh
- Target network cung cáº¥p target á»•n Ä‘á»‹nh trong 1000 steps

#### 3. Epsilon-Greedy Exploration

```python
def select_action(self, state, epsilon):
    if random.random() < epsilon:
        return random.randint(0, 4)  # Exploration: random action
    else:
        with torch.no_grad():
            q_values = self.q_network(state)
            return q_values.argmax().item()  # Exploitation: best action

# Epsilon decay schedule
epsilon_start = 1.0    # Ban Ä‘áº§u: 100% random
epsilon_end = 0.01     # Cuá»‘i: 1% random
epsilon_decay = 0.995  # Giáº£m 0.5% má»—i episode
```

### âš™ï¸ Hyperparameters Chi Tiáº¿t

| Tham sá»‘ | GiÃ¡ trá»‹ | Giáº£i thÃ­ch chi tiáº¿t |
|---------|---------|---------------------|
| **Learning rate (Î±)** | 1Ã—10â»â´ | Nhá» Ä‘á»ƒ há»™i tá»¥ á»•n Ä‘á»‹nh vá»›i state liÃªn tá»¥c |
| **Discount factor (Î³)** | 0.99 | Cao (gáº§n 1) vÃ¬ cáº§n planning dÃ i háº¡n (24h+) |
| **Batch size** | 64 | Äá»§ lá»›n Ä‘á»ƒ giáº£m variance, Ä‘á»§ nhá» Ä‘á»ƒ train nhanh |
| **Buffer size** | 100,000 | LÆ°u ~4,000 episodes kinh nghiá»‡m |
| **Target update** | 1,000 steps | Update Ä‘á»§ thÆ°á»ng xuyÃªn nhÆ°ng khÃ´ng quÃ¡ nhanh |
| **Gradient clipping** | max_norm=10 | NgÄƒn exploding gradients |
| **Optimizer** | Adam | Tá»‘t cho deep learning, adaptive learning rate |
| **Hidden layers** | [256, 256, 128] | Äá»§ capacity cho bÃ i toÃ¡n 8D state |

### ğŸ’» Code Template

```python
import torch
import torch.nn as nn
import torch.optim as optim

class QNetwork(nn.Module):
    def __init__(self, state_dim=8, action_dim=5):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        return self.layers(x)

class DQNAgent:
    def __init__(self, lr=1e-4, gamma=0.99):
        self.q_net = QNetwork()
        self.target_net = QNetwork()
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.gamma = gamma
        self.buffer = ReplayBuffer()
        
    def train_step(self, batch_size=64):
        if len(self.buffer) < batch_size:
            return
        
        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)
        
        # Current Q values
        current_q = self.q_net(states).gather(1, actions)
        
        # Double DQN: action selection tá»« q_net, evaluation tá»« target_net
        next_actions = self.q_net(next_states).argmax(1, keepdim=True)
        next_q = self.target_net(next_states).gather(1, next_actions)
        target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # Loss vÃ  update
        loss = nn.MSELoss()(current_q, target_q.detach())
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.q_net.parameters(), max_norm=10)
        self.optimizer.step()
```

---

## 6. Pháº§n 4: PhÃ¢n TÃ­ch Tá»‘i Æ¯u HÃ³a (15%)

### ğŸ“ YÃªu Cáº§u Chi Tiáº¿t

Tháº£o luáº­n vá» cÃ¡ch RL agent tá»‘i Æ°u hÃ³a viá»‡c phÃ¢n phá»‘i nÄƒng lÆ°á»£ng. PhÃ¢n tÃ­ch xu hÆ°á»›ng reward, há»™i tá»¥ há»c, vÃ  hiá»‡u quáº£ chÃ­nh sÃ¡ch.

### ğŸ“¦ Sáº£n Pháº©m Cáº§n Ná»™p

1. **PhÃ¢n tÃ­ch phÃª phÃ¡n** vá» hiá»‡u suáº¥t tá»‘i Æ°u hÃ³a
2. **Äá»“ thá»‹** thá»ƒ hiá»‡n quÃ¡ trÃ¬nh há»c
3. **Giáº£i thÃ­ch** cÃ¡c pattern hÃ nh vi Ä‘Ã£ há»c

### ğŸ“ˆ Máº«u HÃ nh Vi ÄÃ£ Há»c Theo Thá»i Gian

| Thá»i gian | Giá» | HÃ nh vi Agent | LÃ½ do chi tiáº¿t |
|-----------|-----|---------------|----------------|
| **ÄÃªm khuya** | 0:00-6:00 | TÃ¡i táº¡o + LÆ°á»›i | GiÃ¡ Ä‘iá»‡n tháº¥p nháº¥t trong ngÃ y, nhu cáº§u tháº¥p, báº£o toÃ n pin cho peak |
| **Cao Ä‘iá»ƒm sÃ¡ng** | 7:00-9:00 | TÃ¡i táº¡o + Xáº£ pin | Báº¯t Ä‘áº§u cÃ³ máº·t trá»i, nhu cáº§u tÄƒng, trÃ¡nh mua lÆ°á»›i giÃ¡ cao |
| **TrÆ°a** | 10:00-14:00 | Sáº¡c pin | NÄƒng lÆ°á»£ng máº·t trá»i Ä‘á»‰nh Ä‘iá»ƒm, nhu cáº§u trung bÃ¬nh, tÃ­ch trá»¯ cho tá»‘i |
| **Chiá»u** | 15:00-17:00 | Há»—n há»£p | Máº·t trá»i giáº£m, chuáº©n bá»‹ cho peak tá»‘i, duy trÃ¬ má»©c pin |
| **Cao Ä‘iá»ƒm tá»‘i** | 18:00-21:00 | TÃ¡i táº¡o + Xáº£ pin | GiÃ¡ Ä‘iá»‡n cao nháº¥t, nhu cáº§u cao nháº¥t, táº­n dá»¥ng pin Ä‘Ã£ sáº¡c |
| **Khuya** | 22:00-23:00 | TÃ¡i táº¡o + Xáº£ pin | DÃ¹ng ná»‘t pin cÃ²n láº¡i, chuáº©n bá»‹ cho ngÃ y má»›i |

### ğŸ“Š PhÃ¢n TÃ­ch Há»™i Tá»¥ Huáº¥n Luyá»‡n

**VÃ­ dá»¥ log huáº¥n luyá»‡n:**

```
Episode   100 | Reward:  -1.94 | Îµ: 0.606 â†’ Giai Ä‘oáº¡n exploration
Episode   200 | Reward:  +7.89 | Îµ: 0.367 â†’ Báº¯t Ä‘áº§u há»™i tá»¥
Episode   300 | Reward:  +8.39 | Îµ: 0.222 â†’ Tinh chá»‰nh
Episode   400 | Reward: +12.17 | Îµ: 0.135 â†’ Gáº§n tá»‘i Æ°u
Episode   500 | Reward: +13.34 | Îµ: 0.082 â†’ ÄÃ£ há»™i tá»¥

Best Episode: 18.00 | Training Time: ~9 seconds
```

**CÃ¡c Ä‘á»“ thá»‹ cáº§n váº½:**

1. **Reward vs Episode**: Thá»ƒ hiá»‡n quÃ¡ trÃ¬nh há»c
2. **Epsilon vs Episode**: Thá»ƒ hiá»‡n exploration decay
3. **Loss vs Training Step**: Thá»ƒ hiá»‡n sá»± á»•n Ä‘á»‹nh cá»§a há»c
4. **Action Distribution vs Hour**: Thá»ƒ hiá»‡n pattern Ä‘Ã£ há»c

### ğŸ” PhÃ¢n TÃ­ch Cáº§n Thá»±c Hiá»‡n

1. **Tá»‘c Ä‘á»™ há»™i tá»¥**: Agent há»c nhanh hay cháº­m? Táº¡i sao?
2. **á»”n Ä‘á»‹nh**: Reward cÃ³ dao Ä‘á»™ng máº¡nh khÃ´ng? Variance cao hay tháº¥p?
3. **ChÃ­nh sÃ¡ch cuá»‘i**: Agent Ä‘Ã£ há»c Ä‘Æ°á»£c gÃ¬? Pattern cÃ³ há»£p lÃ½ khÃ´ng?
4. **So sÃ¡nh baseline**: Tá»‘t hÆ¡n random policy bao nhiÃªu %?

---

## 7. Pháº§n 5: Káº¿t Quáº£ & ÄÃ¡nh GiÃ¡ (15%)

### ğŸ“ YÃªu Cáº§u Chi Tiáº¿t

TrÃ¬nh bÃ y káº¿t quáº£ báº±ng Ä‘á»“ thá»‹, báº£ng biá»ƒu. So sÃ¡nh vá»›i ká»³ vá»ng.

### ğŸ“¦ Sáº£n Pháº©m Cáº§n Ná»™p

1. **Äá»“ thá»‹ trá»±c quan** cho cÃ¡c metrics
2. **Báº£ng sá»‘ liá»‡u** so sÃ¡nh
3. **PhÃ¢n tÃ­ch** káº¿t quáº£

### ğŸ“Š CÃ¡c Metrics Quan Trá»ng

| Metric | Äá»‹nh nghÄ©a | Má»¥c tiÃªu |
|--------|------------|----------|
| **Cumulative Reward** | Tá»•ng pháº§n thÆ°á»Ÿng qua táº¥t cáº£ steps | CÃ ng cao cÃ ng tá»‘t |
| **Daily Cost Savings** | Tiá»n tiáº¿t kiá»‡m so vá»›i chá»‰ dÃ¹ng lÆ°á»›i | CÃ ng cao cÃ ng tá»‘t |
| **Renewable Usage Ratio** | % nÄƒng lÆ°á»£ng tá»« nguá»“n tÃ¡i táº¡o | CÃ ng cao cÃ ng tá»‘t |
| **Unmet Demand Frequency** | Sá»‘ láº§n khÃ´ng Ä‘Ã¡p á»©ng Ä‘Æ°á»£c nhu cáº§u | CÃ ng tháº¥p cÃ ng tá»‘t |
| **Grid Dependency** | % nÄƒng lÆ°á»£ng mua tá»« lÆ°á»›i | CÃ ng tháº¥p cÃ ng tá»‘t |
| **Battery Efficiency** | Hiá»‡u suáº¥t sá»­ dá»¥ng pin | Tá»‘i Æ°u (khÃ´ng quÃ¡ cao/tháº¥p) |

### ğŸ“ˆ Máº«u Äá»“ Thá»‹ Cáº§n CÃ³

```python
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. Training Curve
axes[0,0].plot(rewards_per_episode)
axes[0,0].set_title('Cumulative Reward per Episode')
axes[0,0].set_xlabel('Episode')
axes[0,0].set_ylabel('Total Reward')

# 2. Epsilon Decay
axes[0,1].plot(epsilon_values)
axes[0,1].set_title('Epsilon Decay')

# 3. Renewable Usage over Time
axes[0,2].bar(hours, renewable_usage_by_hour)
axes[0,2].set_title('Renewable Usage by Hour')

# 4. Action Distribution
axes[1,0].bar(action_names, action_counts)
axes[1,0].set_title('Action Distribution')

# 5. Cost Comparison
axes[1,1].bar(['RL Agent', 'Rule-based', 'Random'], costs)
axes[1,1].set_title('Daily Cost Comparison')

# 6. Energy Flow Sankey Diagram
# ... (optional advanced visualization)
```

### ğŸ“‹ Báº£ng So SÃ¡nh Máº«u

| Metric | RL Agent | Rule-based | Random | Improvement |
|--------|----------|------------|--------|-------------|
| Daily Cost ($) | 45.2 | 62.8 | 89.3 | -28% vs Rule |
| Renewable Usage | 78.5% | 52.1% | 33.3% | +50% |
| Unmet Demand | 0.2% | 3.5% | 15.2% | -94% |
| Grid Dependency | 21.5% | 47.9% | 66.7% | -55% |

---

## 8. Pháº§n 6: Äáº¡o Äá»©c & TÆ°Æ¡ng Lai (10%)

### ğŸ“ YÃªu Cáº§u Chi Tiáº¿t

Tháº£o luáº­n vá» cÃ¡c váº¥n Ä‘á» Ä‘áº¡o Ä‘á»©c, thá»±c tiá»…n triá»ƒn khai, vÃ  hÆ°á»›ng cáº£i tiáº¿n.

### ğŸ“¦ Sáº£n Pháº©m Cáº§n Ná»™p

1. **Tháº£o luáº­n Ä‘áº¡o Ä‘á»©c** vá» há»‡ thá»‘ng AI tá»± Ä‘á»™ng
2. **Váº¥n Ä‘á» triá»ƒn khai** thá»±c táº¿
3. **Äá» xuáº¥t cáº£i tiáº¿n** cho tÆ°Æ¡ng lai

### âš–ï¸ Váº¥n Äá» Äáº¡o Äá»©c

#### 1. CÃ´ng Báº±ng Trong PhÃ¢n Phá»‘i NÄƒng LÆ°á»£ng

- AI cÃ³ thá»ƒ Æ°u tiÃªn má»™t sá»‘ há»™ gia Ä‘Ã¬nh hÆ¡n cÃ¡c há»™ khÃ¡c khÃ´ng?
- LÃ m sao Ä‘áº£m báº£o má»i ngÆ°á»i Ä‘á»u Ä‘Æ°á»£c tiáº¿p cáº­n Ä‘iá»‡n cÃ´ng báº±ng?
- Khi thiáº¿u Ä‘iá»‡n, ai Ä‘Æ°á»£c Æ°u tiÃªn?

#### 2. Báº£o Máº­t Dá»¯ Liá»‡u

- Dá»¯ liá»‡u tiÃªu thá»¥ Ä‘iá»‡n tiáº¿t lá»™ nhiá»u vá» lá»‘i sá»‘ng
- Cáº§n báº£o vá»‡ quyá»n riÃªng tÆ° cá»§a ngÆ°á»i dÃ¹ng
- GDPR vÃ  cÃ¡c quy Ä‘á»‹nh vá» dá»¯ liá»‡u

#### 3. TrÃ¡ch Nhiá»‡m Khi Tháº¥t Báº¡i

- Náº¿u AI quyáº¿t Ä‘á»‹nh sai â†’ thiáº¿u Ä‘iá»‡n â†’ thiá»‡t háº¡i
- Ai chá»‹u trÃ¡ch nhiá»‡m? Developer? Operator? AI?
- Cáº§n cÃ³ human-in-the-loop khÃ´ng?

#### 4. Äá»™ Tin Cáº­y

- AI cÃ³ thá»ƒ bá»‹ attack/manipulate khÃ´ng?
- LÃ m sao Ä‘áº£m báº£o há»‡ thá»‘ng hoáº¡t Ä‘á»™ng Ä‘Ãºng 24/7?
- Backup plan khi AI fail?

### ğŸ—ï¸ Váº¥n Äá» Triá»ƒn Khai Thá»±c Táº¿

| ThÃ¡ch thá»©c | MÃ´ táº£ | Giáº£i phÃ¡p |
|------------|-------|-----------|
| **Chi phÃ­ triá»ƒn khai** | Cáº§n Ä‘áº§u tÆ° ban Ä‘áº§u lá»›n | ROI analysis, phased rollout |
| **TÃ­ch há»£p há»‡ thá»‘ng** | Káº¿t ná»‘i vá»›i thiáº¿t bá»‹ hiá»‡n táº¡i | Standard APIs, protocols |
| **Maintenance** | Cáº§n update, monitor liÃªn tá»¥c | MLOps pipeline |
| **Scalability** | Má»Ÿ rá»™ng cho nhiá»u microgrid | Distributed training |
| **Regulatory** | TuÃ¢n thá»§ quy Ä‘á»‹nh Ä‘iá»‡n lá»±c | Work vá»›i regulators |

### ğŸš€ HÆ°á»›ng Cáº£i Tiáº¿n TÆ°Æ¡ng Lai

1. **Multi-Agent Systems**
   - Nhiá»u microgrid há»£p tÃ¡c vá»›i nhau
   - Chia sáº» nÄƒng lÆ°á»£ng dÆ° thá»«a
   - Game theory cho optimal pricing

2. **Transfer Learning**
   - Train trÃªn 1 microgrid, deploy cho nhiá»u nÆ¡i
   - Giáº£m thá»i gian vÃ  chi phÃ­ training
   - Domain adaptation techniques

3. **Hybrid Methods**
   - Káº¿t há»£p RL vá»›i forecasting models
   - Dá»± bÃ¡o thá»i tiáº¿t â†’ dá»± Ä‘oÃ¡n sinh nÄƒng lÆ°á»£ng
   - Dá»± bÃ¡o nhu cáº§u â†’ plan trÆ°á»›c

4. **Continuous Action Space**
   - Chuyá»ƒn sang DDPG, SAC, TD3
   - Fine-grained control
   - Smoother policy

5. **Safe RL**
   - Constrained optimization
   - Äáº£m báº£o khÃ´ng vi pháº¡m rÃ ng buá»™c an toÃ n
   - Constraint satisfaction

---

## 9. YÃªu Cáº§u Ká»¹ Thuáº­t

### ğŸ› ï¸ CÃ´ng Nghá»‡ Báº¯t Buá»™c

| CÃ´ng nghá»‡ | PhiÃªn báº£n | Má»¥c Ä‘Ã­ch |
|-----------|-----------|----------|
| Python | 3.8+ | NgÃ´n ngá»¯ láº­p trÃ¬nh chÃ­nh |
| PyTorch hoáº·c TensorFlow | Latest | Deep Learning framework |
| NumPy | 1.20+ | Array operations |
| Pandas | 1.3+ | Data manipulation |
| Matplotlib/Seaborn | Latest | Visualization |
| Gymnasium (OpenAI Gym) | 0.26+ | RL Environment |

### ğŸ“ Cáº¥u TrÃºc File: Google Colab Notebook (.ipynb)

**LÆ°u Ã½ quan trá»ng:** BÃ i ná»™p lÃ  má»™t file **Google Colab Notebook (.ipynb)** duy nháº¥t, KHÃ”NG pháº£i cáº¥u trÃºc project Python nhiá»u file.

#### Cáº¥u TrÃºc Notebook Gá»£i Ã

```
Microgrid_DQN_Colab.ipynb
â”‚
â”œâ”€â”€ ğŸ“Œ Section 1: Giá»›i Thiá»‡u & CÃ i Äáº·t
â”‚   â”œâ”€â”€ Markdown: MÃ´ táº£ bÃ i toÃ¡n
â”‚   â”œâ”€â”€ Code: !pip install cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
â”‚   â””â”€â”€ Code: Import libraries
â”‚
â”œâ”€â”€ ğŸ“Œ Section 2: Cáº¥u HÃ¬nh & Hyperparameters
â”‚   â”œâ”€â”€ Markdown: Giáº£i thÃ­ch cÃ¡c tham sá»‘
â”‚   â””â”€â”€ Code: Config dictionary/class
â”‚
â”œâ”€â”€ ğŸ“Œ Section 3: Environment (MÃ´i TrÆ°á»ng MDP)
â”‚   â”œâ”€â”€ Markdown: Giáº£i thÃ­ch State, Action, Reward
â”‚   â””â”€â”€ Code: class MicrogridEnv(gym.Env)
â”‚
â”œâ”€â”€ ğŸ“Œ Section 4: Neural Network & Agent
â”‚   â”œâ”€â”€ Markdown: Giáº£i thÃ­ch kiáº¿n trÃºc
â”‚   â”œâ”€â”€ Code: class QNetwork(nn.Module)
â”‚   â”œâ”€â”€ Code: class ReplayBuffer
â”‚   â””â”€â”€ Code: class DQNAgent
â”‚
â”œâ”€â”€ ğŸ“Œ Section 5: Training
â”‚   â”œâ”€â”€ Markdown: Giáº£i thÃ­ch quy trÃ¬nh training
â”‚   â””â”€â”€ Code: Training loop
â”‚
â”œâ”€â”€ ğŸ“Œ Section 6: Evaluation & Visualization
â”‚   â”œâ”€â”€ Markdown: Giáº£i thÃ­ch cÃ¡c metrics
â”‚   â”œâ”€â”€ Code: Evaluation functions
â”‚   â””â”€â”€ Code: Plotting (matplotlib)
â”‚
â”œâ”€â”€ ğŸ“Œ Section 7: Káº¿t Quáº£ & PhÃ¢n TÃ­ch
â”‚   â”œâ”€â”€ Markdown: PhÃ¢n tÃ­ch káº¿t quáº£
â”‚   â””â”€â”€ Code: Display results, charts
â”‚
â””â”€â”€ ğŸ“Œ Section 8: Káº¿t Luáº­n & Tháº£o Luáº­n
    â””â”€â”€ Markdown: Äáº¡o Ä‘á»©c, tÆ°Æ¡ng lai, references
```

#### Æ¯u Äiá»ƒm Cá»§a Colab Notebook

| Æ¯u Ä‘iá»ƒm | MÃ´ táº£ |
|---------|-------|
| **GPU miá»…n phÃ­** | Sá»­ dá»¥ng GPU T4/V100 cá»§a Google Ä‘á»ƒ train nhanh hÆ¡n |
| **Táº¥t cáº£ trong 1 file** | Code + giáº£i thÃ­ch + káº¿t quáº£ + Ä‘á»“ thá»‹ trong cÃ¹ng 1 file |
| **Dá»… chia sáº»** | Chá»‰ cáº§n share link hoáº·c download .ipynb |
| **Reproducible** | NgÆ°á»i cháº¥m cÃ³ thá»ƒ cháº¡y láº¡i toÃ n bá»™ notebook |
| **Interactive** | CÃ³ thá»ƒ chá»‰nh sá»­a vÃ  xem káº¿t quáº£ ngay |

#### Máº«u Báº¯t Äáº§u Notebook

```python
# Cell 1: CÃ i Ä‘áº·t thÆ° viá»‡n
!pip install gymnasium torch numpy matplotlib seaborn pandas -q

# Cell 2: Import
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import gymnasium as gym
from collections import deque
import random

# Kiá»ƒm tra GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
```

### ğŸ“‹ YÃªu Cáº§u Code

1. **Clean Code**: Äáº·t tÃªn biáº¿n rÃµ rÃ ng, cÃ³ Ã½ nghÄ©a
2. **Documentation**: Docstrings cho táº¥t cáº£ functions/classes
3. **Comments**: Giáº£i thÃ­ch logic phá»©c táº¡p
4. **Modularity**: TÃ¡ch riÃªng cÃ¡c components
5. **Reproducibility**: Seed random, save hyperparameters

---

## 10. LÆ°u Ã & TÃ i Liá»‡u Tham Kháº£o

### âš ï¸ TÃ­nh ToÃ n Váº¹n Há»c Thuáº­t

- **KHÃ”NG Ä‘áº¡o vÄƒn**: Táº¥t cáº£ nguá»“n pháº£i Ä‘Æ°á»£c trÃ­ch dáº«n (APA hoáº·c IEEE)
- **CÃ´ng cá»¥ AI**: ÄÆ°á»£c dÃ¹ng Ä‘á»ƒ há»— trá»£ há»c, KHÃ”NG dá»±a vÃ o Ä‘á»ƒ táº¡o solution
- **Tá»± lÃ m**: Pháº£n Ã¡nh hiá»ƒu biáº¿t, phÃ¢n tÃ­ch vÃ  coding cá»§a báº£n thÃ¢n

### ğŸ“… Quy Äá»‹nh Ná»™p BÃ i

- Ná»™p muá»™n: Bá»‹ trá»« Ä‘iá»ƒm theo chÃ­nh sÃ¡ch trÆ°á»ng
- Format: PDF cho report, ZIP cho code
- Cáº¥u trÃºc: RÃµ rÃ ng, cÃ³ má»¥c lá»¥c, Ä‘Ã¡nh sá»‘ trang

### âœ… Checklist HoÃ n ThÃ nh

- [ ] Pháº§n 1: MÃ´ táº£ bÃ i toÃ¡n rÃµ rÃ ng, logic
- [ ] Pháº§n 2: MDP thiáº¿t káº¿ há»£p lÃ½, cÃ³ diagram
- [ ] Pháº§n 3: Code cháº¡y Ä‘Æ°á»£c, cÃ³ comment
- [ ] Pháº§n 4: PhÃ¢n tÃ­ch sÃ¢u sáº¯c vá» optimization
- [ ] Pháº§n 5: Äá»“ thá»‹, báº£ng biá»ƒu Ä‘áº§y Ä‘á»§
- [ ] Pháº§n 6: Tháº£o luáº­n Ä‘áº¡o Ä‘á»©c cÃ³ chiá»u sÃ¢u
- [ ] References: Äáº§y Ä‘á»§, Ä‘Ãºng format

### ğŸ“š TÃ i Liá»‡u Tham Kháº£o Gá»£i Ã

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
2. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.
3. Van Hasselt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement Learning with Double Q-learning. *AAAI*.
4. Lin, L. J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. *Machine Learning*, 8(3-4), 293-321.

---

> ğŸ’¡ **Máº¹o thÃ nh cÃ´ng**: Báº¯t Ä‘áº§u sá»›m, thá»­ nghiá»‡m nhiá»u hyperparameters, vÃ  khÃ´ng ngáº¡i cháº¡y láº¡i training náº¿u káº¿t quáº£ chÆ°a tá»‘t!

---

*TÃ i liá»‡u hÆ°á»›ng dáº«n chi tiáº¿t cho Ä‘á» bÃ i Microgrid Energy Optimization using Deep Reinforcement Learning*
*PhiÃªn báº£n: 3.0 - Cáº­p nháº­t: 07/02/2026 - ThÃªm PPO vÃ  so sÃ¡nh DQN vs PPO*
